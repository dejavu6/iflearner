{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"iFLearner - A Powerfule and Lightweight Federated Learning Framework \u00b6 iFLearner is a federated learning framework, which provides a secure computing framework based on data privacy security protection, mainly for federated modeling in deep learning scenarios. Its security bottom layer supports various encryption technologies such as homomorphic encryption, secret sharing, and differential privacy. The algorithm layer supports various deep learning network models, and supports mainstream frameworks such as Tensorflow, Mxnet, and Pytorch. Architecture \u00b6 The design of iFLearner is based on a few guiding principles: Event-driven mechanism : Use an event-driven programming paradigm to build federated learning, that is, to regard federated learning as the process of sending and receiving messages between participants, and describe the federated learning process by defining message types and the behavior of processing messages. Training framework abstraction : Abstract deep learning backend, compatible with support for multiple types of framework backends such as Tensorflow and Pytorch. High scalability: modular design , users can customize aggregation strategies, encryption modules, and support algorithms in various scenarios. Lightweight and simple : The framework is Lib level, light enough, and users can simply transform their deep learning algorithms into federated learning algorithms.","title":"Home"},{"location":"#iflearner-a-powerfule-and-lightweight-federated-learning-framework","text":"iFLearner is a federated learning framework, which provides a secure computing framework based on data privacy security protection, mainly for federated modeling in deep learning scenarios. Its security bottom layer supports various encryption technologies such as homomorphic encryption, secret sharing, and differential privacy. The algorithm layer supports various deep learning network models, and supports mainstream frameworks such as Tensorflow, Mxnet, and Pytorch.","title":"iFLearner - A Powerfule and Lightweight Federated Learning Framework"},{"location":"#architecture","text":"The design of iFLearner is based on a few guiding principles: Event-driven mechanism : Use an event-driven programming paradigm to build federated learning, that is, to regard federated learning as the process of sending and receiving messages between participants, and describe the federated learning process by defining message types and the behavior of processing messages. Training framework abstraction : Abstract deep learning backend, compatible with support for multiple types of framework backends such as Tensorflow and Pytorch. High scalability: modular design , users can customize aggregation strategies, encryption modules, and support algorithms in various scenarios. Lightweight and simple : The framework is Lib level, light enough, and users can simply transform their deep learning algorithms into federated learning algorithms.","title":"Architecture"},{"location":"api/reference/","text":"","title":"Index"},{"location":"api/reference/business/","text":"","title":"Index"},{"location":"api/reference/business/homo/","text":"","title":"Index"},{"location":"api/reference/business/homo/aggregate_server/","text":"AggregateServer ( addr , strategy , num_clients , strategy_params = {}, epochs = 0 ) \u00b6 The server processes the requests of all parties according to the usage policy. Source code in iflearner/business/homo/aggregate_server.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , addr : str , strategy : Union [ str , StrategyServer ], num_clients : int , strategy_params : Dict [ str , Any ] = {}, epochs : int = 0 , ) -> None : logger . add ( \"log/server.log\" , backtrace = True , diagnose = True ) logger . info ( f \"server address: { addr } , strategy: { strategy } , client number: { num_clients } , epochs: { epochs } , strategy params: { strategy_params } \" ) if isinstance ( strategy , str ): if strategy == message_type . STRATEGY_FEDAVG : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , False , ** strategy_params ) elif strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , True , ** strategy_params ) elif strategy == message_type . STRATEGY_FEDOPT : if strategy_params . get ( \"opt\" ) is None : raise Exception ( \"expect 'opt' when you use fedopt sever\" ) else : opt_type = strategy_params . pop ( \"opt\" ) module = import_module ( f \"iflearner.business.homo.strategy.opt. { opt_type . lower () } \" ) opt_class = getattr ( module , f \" { opt_type } \" ) opt = opt_class ( ** strategy_params ) self . _strategy_server = fedopt_server . FedoptServer ( num_clients , epochs , opt = opt , ) # type: ignore logger . info ( \" \" . join ([ f \" { k } : { v } \" for k , v in strategy_params . items ()]) ) elif strategy == message_type . STRATEGY_qFEDAVG : self . _strategy_server = qfedavg_server . qFedavgServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif strategy == message_type . STRATEGY_FEDNOVA : self . _strategy_server = fednova_server . FedNovaServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif isinstance ( strategy , StrategyServer ): self . _strategy_server = strategy # type: ignore self . _addr = addr run () \u00b6 start server Source code in iflearner/business/homo/aggregate_server.py 95 96 97 98 99 def run ( self ) -> None : \"\"\"start server\"\"\" homo_server_inst = homo_server . HomoServer ( self . _strategy_server ) base_server . start_server ( self . _addr , homo_server_inst )","title":"aggregate_server"},{"location":"api/reference/business/homo/aggregate_server/#iflearner.business.homo.aggregate_server.AggregateServer","text":"The server processes the requests of all parties according to the usage policy. Source code in iflearner/business/homo/aggregate_server.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , addr : str , strategy : Union [ str , StrategyServer ], num_clients : int , strategy_params : Dict [ str , Any ] = {}, epochs : int = 0 , ) -> None : logger . add ( \"log/server.log\" , backtrace = True , diagnose = True ) logger . info ( f \"server address: { addr } , strategy: { strategy } , client number: { num_clients } , epochs: { epochs } , strategy params: { strategy_params } \" ) if isinstance ( strategy , str ): if strategy == message_type . STRATEGY_FEDAVG : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , False , ** strategy_params ) elif strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , True , ** strategy_params ) elif strategy == message_type . STRATEGY_FEDOPT : if strategy_params . get ( \"opt\" ) is None : raise Exception ( \"expect 'opt' when you use fedopt sever\" ) else : opt_type = strategy_params . pop ( \"opt\" ) module = import_module ( f \"iflearner.business.homo.strategy.opt. { opt_type . lower () } \" ) opt_class = getattr ( module , f \" { opt_type } \" ) opt = opt_class ( ** strategy_params ) self . _strategy_server = fedopt_server . FedoptServer ( num_clients , epochs , opt = opt , ) # type: ignore logger . info ( \" \" . join ([ f \" { k } : { v } \" for k , v in strategy_params . items ()]) ) elif strategy == message_type . STRATEGY_qFEDAVG : self . _strategy_server = qfedavg_server . qFedavgServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif strategy == message_type . STRATEGY_FEDNOVA : self . _strategy_server = fednova_server . FedNovaServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif isinstance ( strategy , StrategyServer ): self . _strategy_server = strategy # type: ignore self . _addr = addr","title":"AggregateServer"},{"location":"api/reference/business/homo/aggregate_server/#iflearner.business.homo.aggregate_server.AggregateServer.run","text":"start server Source code in iflearner/business/homo/aggregate_server.py 95 96 97 98 99 def run ( self ) -> None : \"\"\"start server\"\"\" homo_server_inst = homo_server . HomoServer ( self . _strategy_server ) base_server . start_server ( self . _addr , homo_server_inst )","title":"run()"},{"location":"api/reference/business/homo/argument/","text":"","title":"argument"},{"location":"api/reference/business/homo/keras_trainer/","text":"KerasTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual keras trainer. Source code in iflearner/business/homo/keras_trainer.py 27 28 def __init__ ( self , model : keras . models . Sequential ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/keras_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/keras_trainer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"keras_trainer"},{"location":"api/reference/business/homo/keras_trainer/#iflearner.business.homo.keras_trainer.KerasTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual keras trainer. Source code in iflearner/business/homo/keras_trainer.py 27 28 def __init__ ( self , model : keras . models . Sequential ) -> None : self . _model = model","title":"KerasTrainer"},{"location":"api/reference/business/homo/keras_trainer/#iflearner.business.homo.keras_trainer.KerasTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/keras_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters","title":"get()"},{"location":"api/reference/business/homo/keras_trainer/#iflearner.business.homo.keras_trainer.KerasTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/keras_trainer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"set()"},{"location":"api/reference/business/homo/mxnet_trainer/","text":"MxnetTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual mxnet trainer. Source code in iflearner/business/homo/mxnet_trainer.py 27 28 def __init__ ( self , model : mx . gluon . nn . Sequential ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/mxnet_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for key , val in self . _model . collect_params ( \".*weight\" ) . items (): p = val . data () . asnumpy () parameters [ key ] = p return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/mxnet_trainer.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for key , value in parameters . items (): self . _model . collect_params () . setattr ( key , value )","title":"mxnet_trainer"},{"location":"api/reference/business/homo/mxnet_trainer/#iflearner.business.homo.mxnet_trainer.MxnetTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual mxnet trainer. Source code in iflearner/business/homo/mxnet_trainer.py 27 28 def __init__ ( self , model : mx . gluon . nn . Sequential ) -> None : self . _model = model","title":"MxnetTrainer"},{"location":"api/reference/business/homo/mxnet_trainer/#iflearner.business.homo.mxnet_trainer.MxnetTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/mxnet_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for key , val in self . _model . collect_params ( \".*weight\" ) . items (): p = val . data () . asnumpy () parameters [ key ] = p return parameters","title":"get()"},{"location":"api/reference/business/homo/mxnet_trainer/#iflearner.business.homo.mxnet_trainer.MxnetTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/mxnet_trainer.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for key , value in parameters . items (): self . _model . collect_params () . setattr ( key , value )","title":"set()"},{"location":"api/reference/business/homo/pytorch_trainer/","text":"PyTorchTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual pytorch trainer. Source code in iflearner/business/homo/pytorch_trainer.py 27 28 def __init__ ( self , model : torch . nn . Module ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/pytorch_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : parameters [ name ] = param . cpu () . detach () . numpy () else : parameters [ name ] = param . grad . cpu () . detach () . numpy () return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/pytorch_trainer.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : param . data . copy_ ( torch . from_numpy ( parameters [ name ])) else : param . grad . copy_ ( torch . from_numpy ( parameters [ name ]))","title":"pytorch_trainer"},{"location":"api/reference/business/homo/pytorch_trainer/#iflearner.business.homo.pytorch_trainer.PyTorchTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual pytorch trainer. Source code in iflearner/business/homo/pytorch_trainer.py 27 28 def __init__ ( self , model : torch . nn . Module ) -> None : self . _model = model","title":"PyTorchTrainer"},{"location":"api/reference/business/homo/pytorch_trainer/#iflearner.business.homo.pytorch_trainer.PyTorchTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/pytorch_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : parameters [ name ] = param . cpu () . detach () . numpy () else : parameters [ name ] = param . grad . cpu () . detach () . numpy () return parameters","title":"get()"},{"location":"api/reference/business/homo/pytorch_trainer/#iflearner.business.homo.pytorch_trainer.PyTorchTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/pytorch_trainer.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : param . data . copy_ ( torch . from_numpy ( parameters [ name ])) else : param . grad . copy_ ( torch . from_numpy ( parameters [ name ]))","title":"set()"},{"location":"api/reference/business/homo/sklearn_trainer/","text":"SklearnTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual sklearn trainer. Source code in iflearner/business/homo/sklearn_trainer.py 26 27 28 def __init__ ( self , model : Any ) -> None : self . _model : Any = model super () . __init__ () get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/sklearn_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/sklearn_trainer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"Sklearn trainer"},{"location":"api/reference/business/homo/sklearn_trainer/#iflearner.business.homo.sklearn_trainer.SklearnTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual sklearn trainer. Source code in iflearner/business/homo/sklearn_trainer.py 26 27 28 def __init__ ( self , model : Any ) -> None : self . _model : Any = model super () . __init__ ()","title":"SklearnTrainer"},{"location":"api/reference/business/homo/sklearn_trainer/#iflearner.business.homo.sklearn_trainer.SklearnTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/sklearn_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass","title":"get()"},{"location":"api/reference/business/homo/sklearn_trainer/#iflearner.business.homo.sklearn_trainer.SklearnTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/sklearn_trainer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"set()"},{"location":"api/reference/business/homo/tensorflow_trainer/","text":"TensorFlowTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual tensorflow trainer. Source code in iflearner/business/homo/tensorflow_trainer.py 28 29 def __init__ ( self , model : tf . keras . Model ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/tensorflow_trainer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/tensorflow_trainer.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"tensorflow_trainer"},{"location":"api/reference/business/homo/tensorflow_trainer/#iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual tensorflow trainer. Source code in iflearner/business/homo/tensorflow_trainer.py 28 29 def __init__ ( self , model : tf . keras . Model ) -> None : self . _model = model","title":"TensorFlowTrainer"},{"location":"api/reference/business/homo/tensorflow_trainer/#iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/tensorflow_trainer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters","title":"get()"},{"location":"api/reference/business/homo/tensorflow_trainer/#iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/tensorflow_trainer.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"set()"},{"location":"api/reference/business/homo/train_client/","text":"Controller ( args , trainer ) \u00b6 Control the training logic of the client. Source code in iflearner/business/homo/train_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , args : argparse . Namespace , trainer : Trainer ) -> None : logger . add ( f \"log/ { args . name } .log\" , backtrace = True , diagnose = True ) self . _args = args self . _trainer = trainer self . _network_client = homo_client . HomoClient ( self . _args . server , self . _args . name , self . _args . cert ) self . _party_name = self . _args . name self . _sum_random_value = 0.0 self . _epoch = 1 self . _local_training = \"LT\" self . _federated_training = \"FT\" self . _local_training_param = None self . _metric = Metric ( logdir = f \"metric/ { self . _args . name } \" ) do_smpc () \u00b6 The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. Source code in iflearner/business/homo/train_client.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def do_smpc ( self ) -> None : \"\"\"The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. \"\"\" if self . _args . peers is None : return peer_list = self . _args . peers . split ( \";\" ) # type: ignore for index in range ( len ( peer_list )): if index == 0 : srv = peer_server . PeerServer ( len ( peer_list ) - 1 ) t = Thread ( target = base_server . start_server , args = ( peer_list [ index ], srv ) ) t . start () else : cli = peer_client . PeerClient ( peer_list [ index ], self . _party_name , self . _args . peer_cert ) public_key = cli . get_DH_public_key () secret = diffie_hellman_inst . DiffieHellmanInst () . generate_secret ( public_key ) logger . info ( f \"secret: { secret } , type: { type ( secret ) } \" ) random_value = cli . get_SMPC_random_key ( secret ) self . _sum_random_value += random_value logger . info ( f \"random value: { random_value } \" ) self . _sum_random_value += srv . sum_parties_random_value () logger . info ( f \"sum all random values: { self . _sum_random_value } \" ) exit () \u00b6 Before exiting, the client needs to save the metrics and notify the server of the client's status. Source code in iflearner/business/homo/train_client.py 88 89 90 91 92 def exit ( self ) -> None : \"\"\"Before exiting, the client needs to save the metrics and notify the server of the client's status.\"\"\" self . _network_client . transport ( message_type . MSG_COMPLETE ) os . _exit ( 0 ) run () \u00b6 start training Source code in iflearner/business/homo/train_client.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def run ( self ) -> None : \"\"\"start training\"\"\" logger . info ( \"register to server\" ) sample_num = self . _trainer . config () . get ( \"sample_num\" , 0 ) batch_num = self . _trainer . config () . get ( \"batch_num\" , 0 ) while True : try : resp = self . _network_client . transport ( message_type . MSG_REGISTER , homo_pb2 . RegistrationInfo ( sample_num = sample_num , step_num = batch_num ), ) break except Exception as e : logger . info ( e ) time . sleep ( 3 ) logger . info ( f \"use strategy: { resp . strategy } \" ) # if resp.parameters: # data_m = dict() # for k, v in resp.parameters.items(): # data_m[k] = homo_pb2.Parameter(shape=v.shape) # data_m[k].values.extend(v.values) # self._global_params = {} # type: ignore # for k, v in data_m.items(): # self._global_params[k] = np.asarray(v.values).reshape(v.shape) # self._trainer.set(self._global_params) # logger.info(f\"load global model.\") self . do_smpc () if resp . strategy == message_type . STRATEGY_FEDAVG : self . _strategy = fedavg_client . FedavgClient () elif resp . strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy = fedavg_client . FedavgClient ( True ) elif resp . strategy == message_type . STRATEGY_FEDOPT : self . _strategy = fedopt_client . FedoptClient () # type: ignore elif resp . strategy == message_type . STRATEGY_qFEDAVG : self . _strategy = qfedavg_client . qFedavgClient () # type: ignore elif resp . strategy == message_type . STRATEGY_FEDNOVA : self . _strategy = fednova_client . FedNovaClient () # type: ignore self . _network_client . set_strategy ( self . _strategy ) t = Thread ( target = self . _network_client . notice ) t . start () logger . info ( \"report client ready\" ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) learning_type = self . _federated_training current_epoch = 0 while True : if ( self . _strategy . current_stage () == StrategyClient . Stage . Training or learning_type == self . _local_training ): logger . info ( f \"----- fit < { learning_type } > -----\" ) if learning_type == self . _local_training : self . _trainer . set ( self . _local_training_param ) # type: ignore current_epoch = self . _epoch - 1 else : current_epoch = self . _epoch try : self . _strategy . set_trainer_config ( self . _trainer . config ()) fit = self . _trainer . fit ( current_epoch ) if isinstance ( fit , types . GeneratorType ): param = next ( fit ) while True : param = self . _strategy . update_param ( param ) param = fit . send ( param ) except StopIteration : logger . info ( \"epoch end\" ) logger . info ( f \"----- evaluate < { learning_type } > -----\" ) metrics = self . _trainer . evaluate ( current_epoch ) if metrics is not None : for k , v in metrics . items (): self . _metric . add ( k , learning_type , current_epoch , v ) if ( learning_type == self . _federated_training and self . _epoch == 1 ): self . _metric . add ( k , self . _local_training , current_epoch , v ) logger . info ( f \"----- get < { learning_type } > -----\" ) client_param = self . _trainer . get () if self . _args . enable_ll : if learning_type == self . _federated_training : if self . _local_training_param is None : self . _local_training_param = client_param # type: ignore else : learning_type = self . _local_training else : learning_type = self . _federated_training self . _local_training_param = client_param # type: ignore if self . _epoch == self . _args . epochs : self . exit () continue upload_param = self . _strategy . generate_upload_param ( self . _epoch , client_param , metrics ) if self . _sum_random_value != 0.0 : smpc_data = dict () for k , v in upload_param . parameters . items (): smpc_data [ k ] = homo_pb2 . Parameter ( shape = v . shape ) # type: ignore smpc_data [ k ] . values . extend ( [ item + self . _sum_random_value for item in v . values ] # type: ignore ) upload_param = homo_pb2 . UploadParam ( epoch = upload_param . epoch , parameters = smpc_data , metrics = metrics ) if self . _epoch == self . _args . epochs : if self . _args . enable_ll : continue else : self . exit () self . _network_client . transport ( message_type . MSG_UPLOAD_PARAM , upload_param ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _epoch += 1 elif self . _strategy . current_stage () == StrategyClient . Stage . Setting : logger . info ( \"----- set -----\" ) self . _global_params = self . _strategy . aggregate_result () self . _trainer . set ( self . _global_params ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) else : time . sleep ( 1 )","title":"train_client"},{"location":"api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller","text":"Control the training logic of the client. Source code in iflearner/business/homo/train_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , args : argparse . Namespace , trainer : Trainer ) -> None : logger . add ( f \"log/ { args . name } .log\" , backtrace = True , diagnose = True ) self . _args = args self . _trainer = trainer self . _network_client = homo_client . HomoClient ( self . _args . server , self . _args . name , self . _args . cert ) self . _party_name = self . _args . name self . _sum_random_value = 0.0 self . _epoch = 1 self . _local_training = \"LT\" self . _federated_training = \"FT\" self . _local_training_param = None self . _metric = Metric ( logdir = f \"metric/ { self . _args . name } \" )","title":"Controller"},{"location":"api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller.do_smpc","text":"The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. Source code in iflearner/business/homo/train_client.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def do_smpc ( self ) -> None : \"\"\"The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. \"\"\" if self . _args . peers is None : return peer_list = self . _args . peers . split ( \";\" ) # type: ignore for index in range ( len ( peer_list )): if index == 0 : srv = peer_server . PeerServer ( len ( peer_list ) - 1 ) t = Thread ( target = base_server . start_server , args = ( peer_list [ index ], srv ) ) t . start () else : cli = peer_client . PeerClient ( peer_list [ index ], self . _party_name , self . _args . peer_cert ) public_key = cli . get_DH_public_key () secret = diffie_hellman_inst . DiffieHellmanInst () . generate_secret ( public_key ) logger . info ( f \"secret: { secret } , type: { type ( secret ) } \" ) random_value = cli . get_SMPC_random_key ( secret ) self . _sum_random_value += random_value logger . info ( f \"random value: { random_value } \" ) self . _sum_random_value += srv . sum_parties_random_value () logger . info ( f \"sum all random values: { self . _sum_random_value } \" )","title":"do_smpc()"},{"location":"api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller.exit","text":"Before exiting, the client needs to save the metrics and notify the server of the client's status. Source code in iflearner/business/homo/train_client.py 88 89 90 91 92 def exit ( self ) -> None : \"\"\"Before exiting, the client needs to save the metrics and notify the server of the client's status.\"\"\" self . _network_client . transport ( message_type . MSG_COMPLETE ) os . _exit ( 0 )","title":"exit()"},{"location":"api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller.run","text":"start training Source code in iflearner/business/homo/train_client.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def run ( self ) -> None : \"\"\"start training\"\"\" logger . info ( \"register to server\" ) sample_num = self . _trainer . config () . get ( \"sample_num\" , 0 ) batch_num = self . _trainer . config () . get ( \"batch_num\" , 0 ) while True : try : resp = self . _network_client . transport ( message_type . MSG_REGISTER , homo_pb2 . RegistrationInfo ( sample_num = sample_num , step_num = batch_num ), ) break except Exception as e : logger . info ( e ) time . sleep ( 3 ) logger . info ( f \"use strategy: { resp . strategy } \" ) # if resp.parameters: # data_m = dict() # for k, v in resp.parameters.items(): # data_m[k] = homo_pb2.Parameter(shape=v.shape) # data_m[k].values.extend(v.values) # self._global_params = {} # type: ignore # for k, v in data_m.items(): # self._global_params[k] = np.asarray(v.values).reshape(v.shape) # self._trainer.set(self._global_params) # logger.info(f\"load global model.\") self . do_smpc () if resp . strategy == message_type . STRATEGY_FEDAVG : self . _strategy = fedavg_client . FedavgClient () elif resp . strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy = fedavg_client . FedavgClient ( True ) elif resp . strategy == message_type . STRATEGY_FEDOPT : self . _strategy = fedopt_client . FedoptClient () # type: ignore elif resp . strategy == message_type . STRATEGY_qFEDAVG : self . _strategy = qfedavg_client . qFedavgClient () # type: ignore elif resp . strategy == message_type . STRATEGY_FEDNOVA : self . _strategy = fednova_client . FedNovaClient () # type: ignore self . _network_client . set_strategy ( self . _strategy ) t = Thread ( target = self . _network_client . notice ) t . start () logger . info ( \"report client ready\" ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) learning_type = self . _federated_training current_epoch = 0 while True : if ( self . _strategy . current_stage () == StrategyClient . Stage . Training or learning_type == self . _local_training ): logger . info ( f \"----- fit < { learning_type } > -----\" ) if learning_type == self . _local_training : self . _trainer . set ( self . _local_training_param ) # type: ignore current_epoch = self . _epoch - 1 else : current_epoch = self . _epoch try : self . _strategy . set_trainer_config ( self . _trainer . config ()) fit = self . _trainer . fit ( current_epoch ) if isinstance ( fit , types . GeneratorType ): param = next ( fit ) while True : param = self . _strategy . update_param ( param ) param = fit . send ( param ) except StopIteration : logger . info ( \"epoch end\" ) logger . info ( f \"----- evaluate < { learning_type } > -----\" ) metrics = self . _trainer . evaluate ( current_epoch ) if metrics is not None : for k , v in metrics . items (): self . _metric . add ( k , learning_type , current_epoch , v ) if ( learning_type == self . _federated_training and self . _epoch == 1 ): self . _metric . add ( k , self . _local_training , current_epoch , v ) logger . info ( f \"----- get < { learning_type } > -----\" ) client_param = self . _trainer . get () if self . _args . enable_ll : if learning_type == self . _federated_training : if self . _local_training_param is None : self . _local_training_param = client_param # type: ignore else : learning_type = self . _local_training else : learning_type = self . _federated_training self . _local_training_param = client_param # type: ignore if self . _epoch == self . _args . epochs : self . exit () continue upload_param = self . _strategy . generate_upload_param ( self . _epoch , client_param , metrics ) if self . _sum_random_value != 0.0 : smpc_data = dict () for k , v in upload_param . parameters . items (): smpc_data [ k ] = homo_pb2 . Parameter ( shape = v . shape ) # type: ignore smpc_data [ k ] . values . extend ( [ item + self . _sum_random_value for item in v . values ] # type: ignore ) upload_param = homo_pb2 . UploadParam ( epoch = upload_param . epoch , parameters = smpc_data , metrics = metrics ) if self . _epoch == self . _args . epochs : if self . _args . enable_ll : continue else : self . exit () self . _network_client . transport ( message_type . MSG_UPLOAD_PARAM , upload_param ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _epoch += 1 elif self . _strategy . current_stage () == StrategyClient . Stage . Setting : logger . info ( \"----- set -----\" ) self . _global_params = self . _strategy . aggregate_result () self . _trainer . set ( self . _global_params ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) else : time . sleep ( 1 )","title":"run()"},{"location":"api/reference/business/homo/trainer/","text":"Trainer \u00b6 Bases: ABC The base class of trainer. ParameterType \u00b6 Bases: IntEnum Define the type of parameter. config () \u00b6 get training configuration. Returns: Type Description Dict [ str , float ] return a dict, at least including the following keys: Dict [ str , float ] learning_rate Dict [ str , float ] batch_num Dict [ str , float ] sample_num Source code in iflearner/business/homo/trainer.py 64 65 66 67 68 69 70 71 72 73 def config ( self ) -> Dict [ str , float ]: \"\"\"get training configuration. Returns: return a dict, at least including the following keys: learning_rate batch_num sample_num \"\"\" return dict () evaluate ( epoch ) abstractmethod \u00b6 evaluate model and return metrics. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description Dict [ str , float ] dict, k: str (metric name), v: float (metric value) Source code in iflearner/business/homo/trainer.py 87 88 89 90 91 92 93 94 95 96 97 @abstractmethod def evaluate ( self , epoch : int ) -> Dict [ str , float ]: \"\"\"evaluate model and return metrics. Args: epoch: the current index of epoch Returns: dict, k: str (metric name), v: float (metric value) \"\"\" pass fit ( epoch ) abstractmethod \u00b6 fit model on one epoch. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description None None Source code in iflearner/business/homo/trainer.py 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def fit ( self , epoch : int ) -> None : \"\"\"fit model on one epoch. Args: epoch: the current index of epoch Returns: None \"\"\" pass get ( param_type = ParameterType . ParameterModel ) abstractmethod \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/trainer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def get ( self , param_type : ParameterType = ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass set ( parameters , param_type = ParameterType . ParameterModel ) abstractmethod \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Source code in iflearner/business/homo/trainer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @abstractmethod def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : ParameterType = ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"trainer"},{"location":"api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer","text":"Bases: ABC The base class of trainer.","title":"Trainer"},{"location":"api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.ParameterType","text":"Bases: IntEnum Define the type of parameter.","title":"ParameterType"},{"location":"api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.config","text":"get training configuration. Returns: Type Description Dict [ str , float ] return a dict, at least including the following keys: Dict [ str , float ] learning_rate Dict [ str , float ] batch_num Dict [ str , float ] sample_num Source code in iflearner/business/homo/trainer.py 64 65 66 67 68 69 70 71 72 73 def config ( self ) -> Dict [ str , float ]: \"\"\"get training configuration. Returns: return a dict, at least including the following keys: learning_rate batch_num sample_num \"\"\" return dict ()","title":"config()"},{"location":"api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.evaluate","text":"evaluate model and return metrics. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description Dict [ str , float ] dict, k: str (metric name), v: float (metric value) Source code in iflearner/business/homo/trainer.py 87 88 89 90 91 92 93 94 95 96 97 @abstractmethod def evaluate ( self , epoch : int ) -> Dict [ str , float ]: \"\"\"evaluate model and return metrics. Args: epoch: the current index of epoch Returns: dict, k: str (metric name), v: float (metric value) \"\"\" pass","title":"evaluate()"},{"location":"api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.fit","text":"fit model on one epoch. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description None None Source code in iflearner/business/homo/trainer.py 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def fit ( self , epoch : int ) -> None : \"\"\"fit model on one epoch. Args: epoch: the current index of epoch Returns: None \"\"\" pass","title":"fit()"},{"location":"api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/trainer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def get ( self , param_type : ParameterType = ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass","title":"get()"},{"location":"api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Source code in iflearner/business/homo/trainer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @abstractmethod def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : ParameterType = ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"set()"},{"location":"api/reference/business/homo/strategy/","text":"FedAdagrad ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedAdam ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedYogi ( learning_rate = 0.1 , betas = ( 0.9 , 0.99 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedavgServer ( num_clients , total_epoch , scaffold = False , weighted_fedavg = False ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {} FedoptServer ( num_clients , total_epoch , opt ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" ) qFedavgServer ( num_clients , total_epoch , q = 1 , learning_rate = 0.1 ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {} norm_grad ( grad ) \u00b6 normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad step ( deltas , hs ) \u00b6 a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"Index"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedAdagrad","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdagrad"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedAdam","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdam"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.opt.fedadam.FedAdam.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedYogi","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedYogi"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {}","title":"FedavgServer"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedoptServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" )","title":"FedoptServer"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.qFedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {}","title":"qFedavgServer"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.norm_grad","text":"normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad","title":"norm_grad()"},{"location":"api/reference/business/homo/strategy/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.step","text":"a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"step()"},{"location":"api/reference/business/homo/strategy/fedavg_client/","text":"FedavgClient ( scaffold = False ) \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of fedavg on client side. Source code in iflearner/business/homo/strategy/fedavg_client.py 27 28 29 def __init__ ( self , scaffold : bool = False ) -> None : super () . __init__ () self . _scaffold = scaffold","title":"fedavg_client"},{"location":"api/reference/business/homo/strategy/fedavg_client/#iflearner.business.homo.strategy.fedavg_client.FedavgClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of fedavg on client side. Source code in iflearner/business/homo/strategy/fedavg_client.py 27 28 29 def __init__ ( self , scaffold : bool = False ) -> None : super () . __init__ () self . _scaffold = scaffold","title":"FedavgClient"},{"location":"api/reference/business/homo/strategy/fedavg_server/","text":"FedavgServer ( num_clients , total_epoch , scaffold = False , weighted_fedavg = False ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {}","title":"fedavg_server"},{"location":"api/reference/business/homo/strategy/fedavg_server/#iflearner.business.homo.strategy.fedavg_server.FedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {}","title":"FedavgServer"},{"location":"api/reference/business/homo/strategy/fednova_client/","text":"FedNovaClient () \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of fednova on client side. Source code in iflearner/business/homo/strategy/fednova_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"fednova_client"},{"location":"api/reference/business/homo/strategy/fednova_client/#iflearner.business.homo.strategy.fednova_client.FedNovaClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of fednova on client side. Source code in iflearner/business/homo/strategy/fednova_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"FedNovaClient"},{"location":"api/reference/business/homo/strategy/fednova_server/","text":"FedNovaServer ( num_clients , total_epoch ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fednova on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning _clients_samples dict samples of each client _step_nums dict step numbers for each client to optimize its model Source code in iflearner/business/homo/strategy/fednova_server.py 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , num_clients : int , total_epoch : int , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _clients_samples : dict = {} self . _step_nums : dict = {}","title":"fednova_server"},{"location":"api/reference/business/homo/strategy/fednova_server/#iflearner.business.homo.strategy.fednova_server.FedNovaServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fednova on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning _clients_samples dict samples of each client _step_nums dict step numbers for each client to optimize its model Source code in iflearner/business/homo/strategy/fednova_server.py 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , num_clients : int , total_epoch : int , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _clients_samples : dict = {} self . _step_nums : dict = {}","title":"FedNovaServer"},{"location":"api/reference/business/homo/strategy/fedopt_client/","text":"FedoptClient () \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of fedopt on client side. Source code in iflearner/business/homo/strategy/fedopt_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"fedopt_client"},{"location":"api/reference/business/homo/strategy/fedopt_client/#iflearner.business.homo.strategy.fedopt_client.FedoptClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of fedopt on client side. Source code in iflearner/business/homo/strategy/fedopt_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"FedoptClient"},{"location":"api/reference/business/homo/strategy/fedopt_server/","text":"FedoptServer ( num_clients , total_epoch , opt ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" )","title":"fedopt_server"},{"location":"api/reference/business/homo/strategy/fedopt_server/#iflearner.business.homo.strategy.fedopt_server.FedoptServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" )","title":"FedoptServer"},{"location":"api/reference/business/homo/strategy/qfedavg_client/","text":"qFedavgClient () \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of qfedavg on client side. Source code in iflearner/business/homo/strategy/qfedavg_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"qfedavg_client"},{"location":"api/reference/business/homo/strategy/qfedavg_client/#iflearner.business.homo.strategy.qfedavg_client.qFedavgClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of qfedavg on client side. Source code in iflearner/business/homo/strategy/qfedavg_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"qFedavgClient"},{"location":"api/reference/business/homo/strategy/qfedavg_server/","text":"qFedavgServer ( num_clients , total_epoch , q = 1 , learning_rate = 0.1 ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {} norm_grad ( grad ) \u00b6 normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad step ( deltas , hs ) \u00b6 a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"qfedavg_server"},{"location":"api/reference/business/homo/strategy/qfedavg_server/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {}","title":"qFedavgServer"},{"location":"api/reference/business/homo/strategy/qfedavg_server/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.norm_grad","text":"normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad","title":"norm_grad()"},{"location":"api/reference/business/homo/strategy/qfedavg_server/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.step","text":"a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"step()"},{"location":"api/reference/business/homo/strategy/strategy_client/","text":"StrategyClient () \u00b6 Bases: ABC Implement the strategy of client. Attributes: Name Type Description _custom_handlers Dict [ str , Any ] custom handlers _trainer_config Dict [ str , Any ] the trainer config of client _current_stage Stage current stage of client _aggregate_result aggregate model parameters result with grpc format _aggregate_result_np aggregate model parameters result with numpy format _aggregate_c None _smpc bool use smpc to start federated learning if _smpc is True _sum_random_value random value using in smpc Source code in iflearner/business/homo/strategy/strategy_client.py 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () self . _current_stage = self . Stage . Waiting self . _aggregate_result = None self . _aggregate_result_np = None self . _aggregate_c = None self . _local_c = None self . _local_c_initial = None self . _smpc = False self . _sum_random_value = 0.0 self . _gradient_suffix = \"_gradient\" Stage \u00b6 Bases: IntEnum Enum the stage of client. aggregate_result () \u00b6 get the aggregated model parameters. Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the aggregated model parameters of grpc format Source code in iflearner/business/homo/strategy/strategy_client.py 76 77 78 79 80 81 82 def aggregate_result ( self ) -> homo_pb2 . AggregateResult : \"\"\"get the aggregated model parameters. Returns: homo_pb2.AggregateResult: the aggregated model parameters of grpc format \"\"\" return self . _aggregate_result_np current_stage () \u00b6 the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Name Type Description Stage Stage the current stage Source code in iflearner/business/homo/strategy/strategy_client.py 150 151 152 153 154 155 156 def current_stage ( self ) -> Stage : \"\"\"the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Stage: the current stage \"\"\" return self . _current_stage generate_registration_info () \u00b6 Generate the message of MSG_REGISTER. Source code in iflearner/business/homo/strategy/strategy_client.py 72 73 74 def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass generate_upload_param ( epoch , data , metrics = None ) \u00b6 Generate the message of MSG_UPLOAD_PARAM. Parameters: Name Type Description Default epoch int Current epoch of number of client training. required data Dict [ Any , Any ] The data that will be uploaded to server. required metrics Dict [ str , Any ] The client metrics. None Returns: Name Type Description Any Any The grpc format data that can be send to server Source code in iflearner/business/homo/strategy/strategy_client.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ], metrics : Dict [ str , float ] = None , ) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM. Args: epoch (int): Current epoch of number of client training. data (Dict[Any, Any]): The data that will be uploaded to server. metrics (Dict[str, Any]): The client metrics. Returns: Any: The grpc format data that can be send to server \"\"\" pb_params = dict () for k , v in data . items (): pb_params [ k ] = homo_pb2 . Parameter ( values = v . ravel (), shape = v . shape ) data = homo_pb2 . UploadParam ( epoch = epoch , parameters = pb_params , metrics = metrics ) return data handler_aggregate_result ( data ) \u00b6 Handle the message of MSG_AGGREGATE_RESULT from the server. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Source code in iflearner/business/homo/strategy/strategy_client.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server. Args: data (homo_pb2.AggregateResult): the aggregated result from server \"\"\" data_m = dict () data_c = dict () for k , v in data . parameters . items (): if k . endswith ( self . _gradient_suffix ): data_c [ k . replace ( self . _gradient_suffix , \"\" )] = homo_pb2 . Parameter ( shape = v . shape ) data_c [ k . replace ( self . _gradient_suffix , \"\" )] . values . extend ( v . values ) else : data_m [ k ] = homo_pb2 . Parameter ( shape = v . shape ) data_m [ k ] . values . extend ( v . values ) self . _aggregate_result = homo_pb2 . AggregateResult ( parameters = data_m ) self . _aggregate_result_np = {} # type: ignore for k , v in data_m . items (): self . _aggregate_result_np [ k ] = np . asarray ( v . values ) . reshape ( v . shape ) # type: ignore self . _aggregate_c = homo_pb2 . AggregateResult ( parameters = data_c ) self . _current_stage = self . Stage . Setting handler_notify_training () \u00b6 Handle the message of MSG_NOTIFY_TRAINING from the server. Source code in iflearner/business/homo/strategy/strategy_client.py 146 147 148 def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" self . _current_stage = self . Stage . Training set_current_stage ( stage ) \u00b6 set current stage. Source code in iflearner/business/homo/strategy/strategy_client.py 158 159 160 161 def set_current_stage ( self , stage : Stage ) -> None : \"\"\"set current stage.\"\"\" self . _current_stage = stage set_global_param ( param ) \u00b6 set global parameters. Parameters: Name Type Description Default param Dict [ str , Any ] parameters required Source code in iflearner/business/homo/strategy/strategy_client.py 163 164 165 166 167 168 169 170 def set_global_param ( self , param : Dict [ str , Any ]) -> None : \"\"\"set global parameters. Args: param (Dict[str, Any]): parameters \"\"\" self . _global_param = param set_trainer_config ( config ) \u00b6 set trainer config. Parameters: Name Type Description Default config Dict [ str , Any ] the config of client Trainer required Source code in iflearner/business/homo/strategy/strategy_client.py 64 65 66 67 68 69 70 def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : \"\"\"set trainer config. Args: config (Dict[str, Any]): the config of client Trainer \"\"\" self . _trainer_config = config update_param ( data ) \u00b6 Update the parameter during training. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the updated result Source code in iflearner/business/homo/strategy/strategy_client.py 108 109 110 111 112 113 114 115 116 117 def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training. Args: data (homo_pb2.AggregateResult): the aggregated result from server Returns: homo_pb2.AggregateResult: the updated result \"\"\" pass","title":"strategy_client"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient","text":"Bases: ABC Implement the strategy of client. Attributes: Name Type Description _custom_handlers Dict [ str , Any ] custom handlers _trainer_config Dict [ str , Any ] the trainer config of client _current_stage Stage current stage of client _aggregate_result aggregate model parameters result with grpc format _aggregate_result_np aggregate model parameters result with numpy format _aggregate_c None _smpc bool use smpc to start federated learning if _smpc is True _sum_random_value random value using in smpc Source code in iflearner/business/homo/strategy/strategy_client.py 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () self . _current_stage = self . Stage . Waiting self . _aggregate_result = None self . _aggregate_result_np = None self . _aggregate_c = None self . _local_c = None self . _local_c_initial = None self . _smpc = False self . _sum_random_value = 0.0 self . _gradient_suffix = \"_gradient\"","title":"StrategyClient"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.Stage","text":"Bases: IntEnum Enum the stage of client.","title":"Stage"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.aggregate_result","text":"get the aggregated model parameters. Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the aggregated model parameters of grpc format Source code in iflearner/business/homo/strategy/strategy_client.py 76 77 78 79 80 81 82 def aggregate_result ( self ) -> homo_pb2 . AggregateResult : \"\"\"get the aggregated model parameters. Returns: homo_pb2.AggregateResult: the aggregated model parameters of grpc format \"\"\" return self . _aggregate_result_np","title":"aggregate_result()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.current_stage","text":"the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Name Type Description Stage Stage the current stage Source code in iflearner/business/homo/strategy/strategy_client.py 150 151 152 153 154 155 156 def current_stage ( self ) -> Stage : \"\"\"the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Stage: the current stage \"\"\" return self . _current_stage","title":"current_stage()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.generate_registration_info","text":"Generate the message of MSG_REGISTER. Source code in iflearner/business/homo/strategy/strategy_client.py 72 73 74 def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass","title":"generate_registration_info()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.generate_upload_param","text":"Generate the message of MSG_UPLOAD_PARAM. Parameters: Name Type Description Default epoch int Current epoch of number of client training. required data Dict [ Any , Any ] The data that will be uploaded to server. required metrics Dict [ str , Any ] The client metrics. None Returns: Name Type Description Any Any The grpc format data that can be send to server Source code in iflearner/business/homo/strategy/strategy_client.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ], metrics : Dict [ str , float ] = None , ) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM. Args: epoch (int): Current epoch of number of client training. data (Dict[Any, Any]): The data that will be uploaded to server. metrics (Dict[str, Any]): The client metrics. Returns: Any: The grpc format data that can be send to server \"\"\" pb_params = dict () for k , v in data . items (): pb_params [ k ] = homo_pb2 . Parameter ( values = v . ravel (), shape = v . shape ) data = homo_pb2 . UploadParam ( epoch = epoch , parameters = pb_params , metrics = metrics ) return data","title":"generate_upload_param()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.handler_aggregate_result","text":"Handle the message of MSG_AGGREGATE_RESULT from the server. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Source code in iflearner/business/homo/strategy/strategy_client.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server. Args: data (homo_pb2.AggregateResult): the aggregated result from server \"\"\" data_m = dict () data_c = dict () for k , v in data . parameters . items (): if k . endswith ( self . _gradient_suffix ): data_c [ k . replace ( self . _gradient_suffix , \"\" )] = homo_pb2 . Parameter ( shape = v . shape ) data_c [ k . replace ( self . _gradient_suffix , \"\" )] . values . extend ( v . values ) else : data_m [ k ] = homo_pb2 . Parameter ( shape = v . shape ) data_m [ k ] . values . extend ( v . values ) self . _aggregate_result = homo_pb2 . AggregateResult ( parameters = data_m ) self . _aggregate_result_np = {} # type: ignore for k , v in data_m . items (): self . _aggregate_result_np [ k ] = np . asarray ( v . values ) . reshape ( v . shape ) # type: ignore self . _aggregate_c = homo_pb2 . AggregateResult ( parameters = data_c ) self . _current_stage = self . Stage . Setting","title":"handler_aggregate_result()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.handler_notify_training","text":"Handle the message of MSG_NOTIFY_TRAINING from the server. Source code in iflearner/business/homo/strategy/strategy_client.py 146 147 148 def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" self . _current_stage = self . Stage . Training","title":"handler_notify_training()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.set_current_stage","text":"set current stage. Source code in iflearner/business/homo/strategy/strategy_client.py 158 159 160 161 def set_current_stage ( self , stage : Stage ) -> None : \"\"\"set current stage.\"\"\" self . _current_stage = stage","title":"set_current_stage()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.set_global_param","text":"set global parameters. Parameters: Name Type Description Default param Dict [ str , Any ] parameters required Source code in iflearner/business/homo/strategy/strategy_client.py 163 164 165 166 167 168 169 170 def set_global_param ( self , param : Dict [ str , Any ]) -> None : \"\"\"set global parameters. Args: param (Dict[str, Any]): parameters \"\"\" self . _global_param = param","title":"set_global_param()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.set_trainer_config","text":"set trainer config. Parameters: Name Type Description Default config Dict [ str , Any ] the config of client Trainer required Source code in iflearner/business/homo/strategy/strategy_client.py 64 65 66 67 68 69 70 def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : \"\"\"set trainer config. Args: config (Dict[str, Any]): the config of client Trainer \"\"\" self . _trainer_config = config","title":"set_trainer_config()"},{"location":"api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.update_param","text":"Update the parameter during training. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the updated result Source code in iflearner/business/homo/strategy/strategy_client.py 108 109 110 111 112 113 114 115 116 117 def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training. Args: data (homo_pb2.AggregateResult): the aggregated result from server Returns: homo_pb2.AggregateResult: the updated result \"\"\" pass","title":"update_param()"},{"location":"api/reference/business/homo/strategy/strategy_server/","text":"StrategyServer ( num_clients , total_epoch ) \u00b6 Bases: ABC Implement the strategy of server. Parameters: Name Type Description Default num_clients int client number required total_epoch int the epoch number of client trainning required Attributes: Name Type Description _num_clients int) client number _total_epoch int the epoch number of client trainning _custom_handlers Dict [ str , Any ] _complete_num int the number of client that has completed federated learning _clients Dict[str, ClientStatus]) a dict storage the client status _training_clients dict) a dict storage the training clients _server_param dict the server model _ready_num int the number of ready client _uploaded_num int the number of client that has uploaded its model parameters _aggregated_num int the number of client that has aggregated its model parameters _on_aggregating bool whether the server is in aggregating stage _params dict the server model parameters Source code in iflearner/business/homo/strategy/strategy_server.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , num_clients , total_epoch ) -> None : self . _num_clients = num_clients self . _total_epoch = total_epoch self . _custom_handlers : Dict [ str , Any ] = dict () self . _complete_num : int = 0 self . _clients : Dict [ str , ClientStatus ] = {} self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _params : dict = {} self . _metric = Metric ( logdir = \"metric\" ) clients_to_json () \u00b6 save clients to json file. Returns: Name Type Description str str json string Source code in iflearner/business/homo/strategy/strategy_server.py 85 86 87 88 89 90 91 92 93 94 95 96 97 def clients_to_json ( self ) -> str : \"\"\"save clients to json file. Returns: str: json string \"\"\" tmp = dict () for k , v in self . _clients . items (): print ( k , v ) tmp [ k ] = v . __dict__ return json . dumps ( tmp ) get_client_notification ( party_name ) \u00b6 Get the notification information of the specified client. Parameters: Name Type Description Default party_name str client name required Returns: Type Description Tuple [ str , Any ] Tuple[str, Any]: the notification message type and notification data Source code in iflearner/business/homo/strategy/strategy_server.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client. Args: party_name (str): client name Returns: Tuple[str, Any]: the notification message type and notification data \"\"\" if party_name in self . _training_clients : if self . _on_aggregating : if not self . _training_clients [ party_name ] . get ( \"aggregating\" , False ): self . _training_clients [ party_name ][ \"aggregating\" ] = True result = homo_pb2 . AggregateResult ( parameters = self . _server_param ) self . _aggregated_num += 1 if self . _aggregated_num == self . _num_clients : self . _aggregated_num = 0 self . _on_aggregating = False self . _training_clients . clear () return message_type . MSG_AGGREGATE_RESULT , result elif not self . _training_clients [ party_name ] . get ( \"training\" , False ): self . _training_clients [ party_name ][ \"training\" ] = True return message_type . MSG_NOTIFY_TRAINING , None return \"\" , None handler_client_ready ( party_name ) \u00b6 Handle the message of MSG_CLIENT_READY from the client. Source code in iflearner/business/homo/strategy/strategy_server.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" logger . info ( f \"Client ready: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _clients [ party_name ] . ready = True self . _clients [ party_name ] . current_epoch += 1 self . _ready_num += 1 if self . _ready_num == self . _num_clients : logger . info ( \"Clients are all ready.\" ) self . _ready_num = 0 for k in self . _clients . keys (): self . _training_clients [ k ] = dict () handler_complete ( party_name ) \u00b6 Handle the message of MSG_COMPLETE from the client. Parameters: Name Type Description Default party_name str client name required Raises: Type Description HomoException if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def handler_complete ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_COMPLETE from the client. Args: party_name (str): client name Raises: HomoException: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client complete: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _complete_num += 1 self . _clients [ party_name ] . complete = True handler_register ( party_name , sample_num = 0 , step_num = 0 ) \u00b6 Handle the message of MSG_REGISTER from the client. Parameters: Name Type Description Default party_name str client name required sample_num Optional [ int ] the total sample number of client party_name . Defaults to 0. 0 step_num int The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. 0 Raises: Type Description HomoException description Returns: Type Description homo_pb2 . RegistrationResponse homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def handler_register ( self , party_name : str , sample_num : Optional [ int ] = 0 , step_num : int = 0 ) -> homo_pb2 . RegistrationResponse : \"\"\"Handle the message of MSG_REGISTER from the client. Args: party_name (str): client name sample_num (Optional[int], optional): the total sample number of client `party_name` . Defaults to 0. step_num (int, optional): The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. Raises: HomoException: _description_ Returns: homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client register: { party_name } \" ) if len ( self . _clients ) >= self . _num_clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Registered clients are full.\" , ) self . _clients [ party_name ] = ClientStatus ( self . _total_epoch ) handler_upload_param ( party_name , data ) \u00b6 Handle the message of MSG_UPLOAD_PARAM from the client. Parameters: Name Type Description Default party_name str client name required data homo_pb2 . UploadParam the data uploaded from party_name , with grpc format required Raises: Type Description HomoException if party_name not in the training_clients list, raise the Forbidden error Source code in iflearner/business/homo/strategy/strategy_server.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client. Args: party_name (str): client name data (homo_pb2.UploadParam): the data uploaded from `party_name`, with grpc format Raises: HomoException: if party_name not in the training_clients list, raise the Forbidden error \"\"\" logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _params is None : self . _params = dict () for param_name , param_info in data . parameters . items (): self . _params [ param_name ] = np . array ( param_info . values ) . reshape ( param_info . shape ) if data . metrics is not None : for k , v in data . metrics . items (): self . _metric . add ( k , party_name , data . epoch , v )","title":"strategy_server"},{"location":"api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer","text":"Bases: ABC Implement the strategy of server. Parameters: Name Type Description Default num_clients int client number required total_epoch int the epoch number of client trainning required Attributes: Name Type Description _num_clients int) client number _total_epoch int the epoch number of client trainning _custom_handlers Dict [ str , Any ] _complete_num int the number of client that has completed federated learning _clients Dict[str, ClientStatus]) a dict storage the client status _training_clients dict) a dict storage the training clients _server_param dict the server model _ready_num int the number of ready client _uploaded_num int the number of client that has uploaded its model parameters _aggregated_num int the number of client that has aggregated its model parameters _on_aggregating bool whether the server is in aggregating stage _params dict the server model parameters Source code in iflearner/business/homo/strategy/strategy_server.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , num_clients , total_epoch ) -> None : self . _num_clients = num_clients self . _total_epoch = total_epoch self . _custom_handlers : Dict [ str , Any ] = dict () self . _complete_num : int = 0 self . _clients : Dict [ str , ClientStatus ] = {} self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _params : dict = {} self . _metric = Metric ( logdir = \"metric\" )","title":"StrategyServer"},{"location":"api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.clients_to_json","text":"save clients to json file. Returns: Name Type Description str str json string Source code in iflearner/business/homo/strategy/strategy_server.py 85 86 87 88 89 90 91 92 93 94 95 96 97 def clients_to_json ( self ) -> str : \"\"\"save clients to json file. Returns: str: json string \"\"\" tmp = dict () for k , v in self . _clients . items (): print ( k , v ) tmp [ k ] = v . __dict__ return json . dumps ( tmp )","title":"clients_to_json()"},{"location":"api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.get_client_notification","text":"Get the notification information of the specified client. Parameters: Name Type Description Default party_name str client name required Returns: Type Description Tuple [ str , Any ] Tuple[str, Any]: the notification message type and notification data Source code in iflearner/business/homo/strategy/strategy_server.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client. Args: party_name (str): client name Returns: Tuple[str, Any]: the notification message type and notification data \"\"\" if party_name in self . _training_clients : if self . _on_aggregating : if not self . _training_clients [ party_name ] . get ( \"aggregating\" , False ): self . _training_clients [ party_name ][ \"aggregating\" ] = True result = homo_pb2 . AggregateResult ( parameters = self . _server_param ) self . _aggregated_num += 1 if self . _aggregated_num == self . _num_clients : self . _aggregated_num = 0 self . _on_aggregating = False self . _training_clients . clear () return message_type . MSG_AGGREGATE_RESULT , result elif not self . _training_clients [ party_name ] . get ( \"training\" , False ): self . _training_clients [ party_name ][ \"training\" ] = True return message_type . MSG_NOTIFY_TRAINING , None return \"\" , None","title":"get_client_notification()"},{"location":"api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_client_ready","text":"Handle the message of MSG_CLIENT_READY from the client. Source code in iflearner/business/homo/strategy/strategy_server.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" logger . info ( f \"Client ready: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _clients [ party_name ] . ready = True self . _clients [ party_name ] . current_epoch += 1 self . _ready_num += 1 if self . _ready_num == self . _num_clients : logger . info ( \"Clients are all ready.\" ) self . _ready_num = 0 for k in self . _clients . keys (): self . _training_clients [ k ] = dict ()","title":"handler_client_ready()"},{"location":"api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_complete","text":"Handle the message of MSG_COMPLETE from the client. Parameters: Name Type Description Default party_name str client name required Raises: Type Description HomoException if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def handler_complete ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_COMPLETE from the client. Args: party_name (str): client name Raises: HomoException: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client complete: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _complete_num += 1 self . _clients [ party_name ] . complete = True","title":"handler_complete()"},{"location":"api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_register","text":"Handle the message of MSG_REGISTER from the client. Parameters: Name Type Description Default party_name str client name required sample_num Optional [ int ] the total sample number of client party_name . Defaults to 0. 0 step_num int The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. 0 Raises: Type Description HomoException description Returns: Type Description homo_pb2 . RegistrationResponse homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def handler_register ( self , party_name : str , sample_num : Optional [ int ] = 0 , step_num : int = 0 ) -> homo_pb2 . RegistrationResponse : \"\"\"Handle the message of MSG_REGISTER from the client. Args: party_name (str): client name sample_num (Optional[int], optional): the total sample number of client `party_name` . Defaults to 0. step_num (int, optional): The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. Raises: HomoException: _description_ Returns: homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client register: { party_name } \" ) if len ( self . _clients ) >= self . _num_clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Registered clients are full.\" , ) self . _clients [ party_name ] = ClientStatus ( self . _total_epoch )","title":"handler_register()"},{"location":"api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_upload_param","text":"Handle the message of MSG_UPLOAD_PARAM from the client. Parameters: Name Type Description Default party_name str client name required data homo_pb2 . UploadParam the data uploaded from party_name , with grpc format required Raises: Type Description HomoException if party_name not in the training_clients list, raise the Forbidden error Source code in iflearner/business/homo/strategy/strategy_server.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client. Args: party_name (str): client name data (homo_pb2.UploadParam): the data uploaded from `party_name`, with grpc format Raises: HomoException: if party_name not in the training_clients list, raise the Forbidden error \"\"\" logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _params is None : self . _params = dict () for param_name , param_info in data . parameters . items (): self . _params [ param_name ] = np . array ( param_info . values ) . reshape ( param_info . shape ) if data . metrics is not None : for k , v in data . metrics . items (): self . _metric . add ( k , party_name , data . epoch , v )","title":"handler_upload_param()"},{"location":"api/reference/business/homo/strategy/opt/","text":"FedAdagrad ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedAdam ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedAvgm ( learning_rate = 1 , momentum = 0.0 ) \u00b6 Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedYogi ( learning_rate = 0.1 , betas = ( 0.9 , 0.99 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"Index"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedAdagrad","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdagrad"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedAdam","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdam"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedadam.FedAdam.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedAvgm","text":"Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum","title":"FedAvgm"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedavgm.FedAvgm.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedYogi","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedYogi"},{"location":"api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/fedadagrad/","text":"FedAdagrad ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedadagrad"},{"location":"api/reference/business/homo/strategy/opt/fedadagrad/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdagrad"},{"location":"api/reference/business/homo/strategy/opt/fedadagrad/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/fedadam/","text":"FedAdam ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedadam"},{"location":"api/reference/business/homo/strategy/opt/fedadam/#iflearner.business.homo.strategy.opt.fedadam.FedAdam","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdam"},{"location":"api/reference/business/homo/strategy/opt/fedadam/#iflearner.business.homo.strategy.opt.fedadam.FedAdam.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/fedavgm/","text":"FedAvgm ( learning_rate = 1 , momentum = 0.0 ) \u00b6 Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedavgm"},{"location":"api/reference/business/homo/strategy/opt/fedavgm/#iflearner.business.homo.strategy.opt.fedavgm.FedAvgm","text":"Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum","title":"FedAvgm"},{"location":"api/reference/business/homo/strategy/opt/fedavgm/#iflearner.business.homo.strategy.opt.fedavgm.FedAvgm.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/fedopt/","text":"FedOpt ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Implementation based on https://arxiv.org/abs/2003.00295 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedopt.py 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t self . _params : dict = {} set_params ( params ) \u00b6 set params to self._params. Parameters: Name Type Description Default params _type_ parameters of server model required Source code in iflearner/business/homo/strategy/opt/fedopt.py 57 58 59 60 61 62 63 def set_params ( self , params ): \"\"\"set params to self._params. Args: params (_type_): parameters of server model \"\"\" self . _params = params step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedopt.py 43 44 45 46 47 48 49 50 51 52 53 54 55 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]], ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" pass","title":"fedopt"},{"location":"api/reference/business/homo/strategy/opt/fedopt/#iflearner.business.homo.strategy.opt.fedopt.FedOpt","text":"Implementation based on https://arxiv.org/abs/2003.00295 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedopt.py 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t self . _params : dict = {}","title":"FedOpt"},{"location":"api/reference/business/homo/strategy/opt/fedopt/#iflearner.business.homo.strategy.opt.fedopt.FedOpt.set_params","text":"set params to self._params. Parameters: Name Type Description Default params _type_ parameters of server model required Source code in iflearner/business/homo/strategy/opt/fedopt.py 57 58 59 60 61 62 63 def set_params ( self , params ): \"\"\"set params to self._params. Args: params (_type_): parameters of server model \"\"\" self . _params = params","title":"set_params()"},{"location":"api/reference/business/homo/strategy/opt/fedopt/#iflearner.business.homo.strategy.opt.fedopt.FedOpt.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedopt.py 43 44 45 46 47 48 49 50 51 52 53 54 55 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]], ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" pass","title":"step()"},{"location":"api/reference/business/homo/strategy/opt/fedyogi/","text":"FedYogi ( learning_rate = 0.1 , betas = ( 0.9 , 0.99 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedyogi"},{"location":"api/reference/business/homo/strategy/opt/fedyogi/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedYogi"},{"location":"api/reference/business/homo/strategy/opt/fedyogi/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"api/reference/business/util/","text":"AccuracyMetric ( metric_name = 'accuracy' , file_dir = '' ) \u00b6 Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , ) BaseMetric ( metric_name , x_label , y_label , file_dir = './' ) \u00b6 Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir add ( x , y , train_type = TrainType . FederatedTrain ) \u00b6 add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore F1Metric ( metric_name = 'f1' , file_dir = '' ) \u00b6 Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir ) LossMetric ( metric_name = 'loss' , file_dir = '' ) \u00b6 Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir ) Metrics ( file_dir = './' ) \u00b6 Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True ) add ( metric ) \u00b6 add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric ) dump () \u00b6 save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f ) load () \u00b6 load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric plot () \u00b6 plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot ()","title":"Index"},{"location":"api/reference/business/util/#iflearner.business.util.AccuracyMetric","text":"Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , )","title":"AccuracyMetric"},{"location":"api/reference/business/util/#iflearner.business.util.BaseMetric","text":"Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir","title":"BaseMetric"},{"location":"api/reference/business/util/#iflearner.business.util.metric_dev.BaseMetric.add","text":"add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore","title":"add()"},{"location":"api/reference/business/util/#iflearner.business.util.F1Metric","text":"Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir )","title":"F1Metric"},{"location":"api/reference/business/util/#iflearner.business.util.LossMetric","text":"Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir )","title":"LossMetric"},{"location":"api/reference/business/util/#iflearner.business.util.Metrics","text":"Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True )","title":"Metrics"},{"location":"api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.add","text":"add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric )","title":"add()"},{"location":"api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.dump","text":"save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f )","title":"dump()"},{"location":"api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.load","text":"load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric","title":"load()"},{"location":"api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.plot","text":"plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot ()","title":"plot()"},{"location":"api/reference/business/util/metric/","text":"Metric ( logdir ) \u00b6 Integrate visualdl to visualize metrics. Source code in iflearner/business/util/metric.py 23 24 25 26 27 28 29 def __init__ ( self , logdir : str ) -> None : \"\"\"Init class with log directory.\"\"\" self . _tag_prefix = \"train\" self . _logdir = logdir self . _writers : Dict [ str , LogWriter ] = dict () self . _figs : Dict [ str , Any ] = dict () add ( name , label , x , y ) \u00b6 Add a point. Parameters: Name Type Description Default name str The name of metric, eg: acc, loss. required label str The label of metric, eg: local learning, federated learning. required x Any The x of point, eg: 1, 2, 3... required y Any The y of point, eg: 95.5, 96.0, 96.5... required Source code in iflearner/business/util/metric.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def add ( self , name : str , label : str , x : Any , y : Any ) -> None : \"\"\"Add a point. Args: name: The name of metric, eg: acc, loss. label: The label of metric, eg: local learning, federated learning. x: The x of point, eg: 1, 2, 3... y: The y of point, eg: 95.5, 96.0, 96.5... \"\"\" if label not in self . _writers : self . _writers [ label ] = LogWriter ( logdir = f \" { self . _logdir } / { label } \" , display_name = label ) self . _writers [ label ] . add_scalar ( f \" { self . _tag_prefix } / { name } \" , y , x )","title":"Metric"},{"location":"api/reference/business/util/metric/#iflearner.business.util.metric.Metric","text":"Integrate visualdl to visualize metrics. Source code in iflearner/business/util/metric.py 23 24 25 26 27 28 29 def __init__ ( self , logdir : str ) -> None : \"\"\"Init class with log directory.\"\"\" self . _tag_prefix = \"train\" self . _logdir = logdir self . _writers : Dict [ str , LogWriter ] = dict () self . _figs : Dict [ str , Any ] = dict ()","title":"Metric"},{"location":"api/reference/business/util/metric/#iflearner.business.util.metric.Metric.add","text":"Add a point. Parameters: Name Type Description Default name str The name of metric, eg: acc, loss. required label str The label of metric, eg: local learning, federated learning. required x Any The x of point, eg: 1, 2, 3... required y Any The y of point, eg: 95.5, 96.0, 96.5... required Source code in iflearner/business/util/metric.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def add ( self , name : str , label : str , x : Any , y : Any ) -> None : \"\"\"Add a point. Args: name: The name of metric, eg: acc, loss. label: The label of metric, eg: local learning, federated learning. x: The x of point, eg: 1, 2, 3... y: The y of point, eg: 95.5, 96.0, 96.5... \"\"\" if label not in self . _writers : self . _writers [ label ] = LogWriter ( logdir = f \" { self . _logdir } / { label } \" , display_name = label ) self . _writers [ label ] . add_scalar ( f \" { self . _tag_prefix } / { name } \" , y , x )","title":"add()"},{"location":"api/reference/business/util/metric_dev/","text":"AccuracyMetric ( metric_name = 'accuracy' , file_dir = '' ) \u00b6 Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , ) BaseMetric ( metric_name , x_label , y_label , file_dir = './' ) \u00b6 Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir add ( x , y , train_type = TrainType . FederatedTrain ) \u00b6 add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore F1Metric ( metric_name = 'f1' , file_dir = '' ) \u00b6 Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir ) LossMetric ( metric_name = 'loss' , file_dir = '' ) \u00b6 Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir ) Metrics ( file_dir = './' ) \u00b6 Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True ) add ( metric ) \u00b6 add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric ) dump () \u00b6 save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f ) load () \u00b6 load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric plot () \u00b6 plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot () TrainType \u00b6 Bases: Enum define the type of train. supported local and federated","title":"Metric dev"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.AccuracyMetric","text":"Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , )","title":"AccuracyMetric"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.BaseMetric","text":"Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir","title":"BaseMetric"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.BaseMetric.add","text":"add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore","title":"add()"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.F1Metric","text":"Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir )","title":"F1Metric"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.LossMetric","text":"Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir )","title":"LossMetric"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics","text":"Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True )","title":"Metrics"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.add","text":"add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric )","title":"add()"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.dump","text":"save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f )","title":"dump()"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.load","text":"load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric","title":"load()"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.plot","text":"plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot ()","title":"plot()"},{"location":"api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.TrainType","text":"Bases: Enum define the type of train. supported local and federated","title":"TrainType"},{"location":"api/reference/communication/","text":"","title":"Index"},{"location":"api/reference/communication/base/","text":"","title":"Index"},{"location":"api/reference/communication/base/base_client/","text":"BaseClient ( addr , cert_path = None ) \u00b6 Provides methods that implement functionality of base client. Source code in iflearner/communication/base/base_client.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , addr : str , cert_path : str = None ) -> None : options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ] if cert_path is None : channel = grpc . insecure_channel ( addr , options = options ) else : with open ( cert_path , \"rb\" ) as f : cert_bytes = f . read () channel = grpc . secure_channel ( addr , grpc . ssl_channel_credentials ( cert_bytes ), options = options ) self . _stub : base_pb2_grpc . BaseStub = base_pb2_grpc . BaseStub ( channel )","title":"base_client"},{"location":"api/reference/communication/base/base_client/#iflearner.communication.base.base_client.BaseClient","text":"Provides methods that implement functionality of base client. Source code in iflearner/communication/base/base_client.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , addr : str , cert_path : str = None ) -> None : options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ] if cert_path is None : channel = grpc . insecure_channel ( addr , options = options ) else : with open ( cert_path , \"rb\" ) as f : cert_bytes = f . read () channel = grpc . secure_channel ( addr , grpc . ssl_channel_credentials ( cert_bytes ), options = options ) self . _stub : base_pb2_grpc . BaseStub = base_pb2_grpc . BaseStub ( channel )","title":"BaseClient"},{"location":"api/reference/communication/base/base_exception/","text":"BaseException ( code , message ) \u00b6 Bases: Exception Define the base exception class. You can inherit the base class and implement your business logic. Attributes: Name Type Description code error code message error details Source code in iflearner/communication/base/base_exception.py 25 26 27 def __init__ ( self , code : int , message : str ) -> None : self . code = code self . message = message","title":"base_exception"},{"location":"api/reference/communication/base/base_exception/#iflearner.communication.base.base_exception.BaseException","text":"Bases: Exception Define the base exception class. You can inherit the base class and implement your business logic. Attributes: Name Type Description code error code message error details Source code in iflearner/communication/base/base_exception.py 25 26 27 def __init__ ( self , code : int , message : str ) -> None : self . code = code self . message = message","title":"BaseException"},{"location":"api/reference/communication/base/base_pb2/","text":"Generated protocol buffer code.","title":"Base pb2"},{"location":"api/reference/communication/base/base_pb2_grpc/","text":"Client and server classes corresponding to protobuf-defined services. Base \u00b6 Bases: object Missing associated documentation comment in .proto file. BaseServicer \u00b6 Bases: object Missing associated documentation comment in .proto file. callback ( request , context ) \u00b6 Use this function to wait for server information. Source code in iflearner/communication/base/base_pb2_grpc.py 51 52 53 54 55 56 def callback ( self , request , context ): \"\"\"Use this function to wait for server information. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' ) post ( request , context ) \u00b6 Use this function to transport information asynchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 44 45 46 47 48 49 def post ( self , request , context ): \"\"\"Use this function to transport information asynchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' ) send ( request , context ) \u00b6 Use this function to transport information synchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 37 38 39 40 41 42 def send ( self , request , context ): \"\"\"Use this function to transport information synchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' ) BaseStub ( channel ) \u00b6 Bases: object Missing associated documentation comment in .proto file. Parameters: Name Type Description Default channel A grpc.Channel. required Source code in iflearner/communication/base/base_pb2_grpc.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , channel ): \"\"\"Constructor. Args: channel: A grpc.Channel. \"\"\" self . send = channel . unary_unary ( '/Base/send' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . post = channel . unary_unary ( '/Base/post' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . callback = channel . unary_unary ( '/Base/callback' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , )","title":"Base pb2 grpc"},{"location":"api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.Base","text":"Bases: object Missing associated documentation comment in .proto file.","title":"Base"},{"location":"api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer","text":"Bases: object Missing associated documentation comment in .proto file.","title":"BaseServicer"},{"location":"api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer.callback","text":"Use this function to wait for server information. Source code in iflearner/communication/base/base_pb2_grpc.py 51 52 53 54 55 56 def callback ( self , request , context ): \"\"\"Use this function to wait for server information. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' )","title":"callback()"},{"location":"api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer.post","text":"Use this function to transport information asynchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 44 45 46 47 48 49 def post ( self , request , context ): \"\"\"Use this function to transport information asynchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' )","title":"post()"},{"location":"api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer.send","text":"Use this function to transport information synchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 37 38 39 40 41 42 def send ( self , request , context ): \"\"\"Use this function to transport information synchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' )","title":"send()"},{"location":"api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseStub","text":"Bases: object Missing associated documentation comment in .proto file. Parameters: Name Type Description Default channel A grpc.Channel. required Source code in iflearner/communication/base/base_pb2_grpc.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , channel ): \"\"\"Constructor. Args: channel: A grpc.Channel. \"\"\" self . send = channel . unary_unary ( '/Base/send' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . post = channel . unary_unary ( '/Base/post' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . callback = channel . unary_unary ( '/Base/callback' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , )","title":"BaseStub"},{"location":"api/reference/communication/base/base_server/","text":"BaseServer \u00b6 Bases: base_pb2_grpc . BaseServicer , ABC Provides methods that implement functionality of base server. start_server ( addr , servicer ) \u00b6 Start server at the address. Source code in iflearner/communication/base/base_server.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def start_server ( addr : str , servicer : BaseServer ) -> None : \"\"\"Start server at the address.\"\"\" server = grpc . server ( futures . ThreadPoolExecutor ( max_workers = 10 ), options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ], ) base_pb2_grpc . add_BaseServicer_to_server ( servicer , server ) server . add_insecure_port ( addr ) server . start () server . wait_for_termination ()","title":"base_server"},{"location":"api/reference/communication/base/base_server/#iflearner.communication.base.base_server.BaseServer","text":"Bases: base_pb2_grpc . BaseServicer , ABC Provides methods that implement functionality of base server.","title":"BaseServer"},{"location":"api/reference/communication/base/base_server/#iflearner.communication.base.base_server.start_server","text":"Start server at the address. Source code in iflearner/communication/base/base_server.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def start_server ( addr : str , servicer : BaseServer ) -> None : \"\"\"Start server at the address.\"\"\" server = grpc . server ( futures . ThreadPoolExecutor ( max_workers = 10 ), options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ], ) base_pb2_grpc . add_BaseServicer_to_server ( servicer , server ) server . add_insecure_port ( addr ) server . start () server . wait_for_termination ()","title":"start_server()"},{"location":"api/reference/communication/base/constant/","text":"","title":"Constant"},{"location":"api/reference/communication/hetero/","text":"","title":"Index"},{"location":"api/reference/communication/homo/","text":"","title":"Index"},{"location":"api/reference/communication/homo/homo_client/","text":"HomoClient ( server_addr , party_name , cert_path = None ) \u00b6 Bases: base_client . BaseClient Implement homogeneous client base on base_client.BaseClient. Source code in iflearner/communication/homo/homo_client.py 30 31 32 33 34 35 def __init__ ( self , server_addr : str , party_name : str , cert_path : str = None ) -> None : super () . __init__ ( server_addr , cert_path ) self . _party_name = party_name self . _strategy : strategy_client . StrategyClient = None # type: ignore notice () \u00b6 Receive notifications from the server regularly. Source code in iflearner/communication/homo/homo_client.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def notice ( self ) -> None : \"\"\"Receive notifications from the server regularly.\"\"\" while True : start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name ) resp = self . _callback ( req ) if resp . code != 0 : raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message ) if resp . type == message_type . MSG_AGGREGATE_RESULT : data = homo_pb2 . AggregateResult () data . ParseFromString ( resp . data ) self . _strategy . handler_aggregate_result ( data ) elif resp . type == message_type . MSG_NOTIFY_TRAINING : self . _strategy . handler_notify_training () elif resp . type in self . _strategy . custom_handlers : self . _strategy . custom_handlers [ resp . type ]() # type: ignore if resp . type != \"\" : stop = timeit . default_timer () logger . info ( f \"IN: party: message type: { resp . type } , time: { 1000 * ( stop - start ) } ms\" ) time . sleep ( message_type . MSG_HEARTBEAT_INTERVAL ) transport ( type , data = None ) \u00b6 Transport data to server. Source code in iflearner/communication/homo/homo_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def transport ( self , type : str , data : Any = None ) -> homo_pb2 . RegistrationResponse : \"\"\"Transport data to server.\"\"\" start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = type ) if data is not None : req . data = data . SerializeToString () resp = None if type == message_type . MSG_REGISTER : resp = self . _send ( req ) elif type == message_type . MSG_CLIENT_READY : resp = self . _send ( req ) elif type == message_type . MSG_UPLOAD_PARAM : resp = self . _post ( req ) elif type == message_type . MSG_COMPLETE : resp = self . _send ( req ) stop = timeit . default_timer () logger . info ( f \"OUT: message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp . code != 0 : # type: ignore raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message # type: ignore ) if type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationResponse () data . ParseFromString ( resp . data ) # type: ignore return data","title":"homo_client"},{"location":"api/reference/communication/homo/homo_client/#iflearner.communication.homo.homo_client.HomoClient","text":"Bases: base_client . BaseClient Implement homogeneous client base on base_client.BaseClient. Source code in iflearner/communication/homo/homo_client.py 30 31 32 33 34 35 def __init__ ( self , server_addr : str , party_name : str , cert_path : str = None ) -> None : super () . __init__ ( server_addr , cert_path ) self . _party_name = party_name self . _strategy : strategy_client . StrategyClient = None # type: ignore","title":"HomoClient"},{"location":"api/reference/communication/homo/homo_client/#iflearner.communication.homo.homo_client.HomoClient.notice","text":"Receive notifications from the server regularly. Source code in iflearner/communication/homo/homo_client.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def notice ( self ) -> None : \"\"\"Receive notifications from the server regularly.\"\"\" while True : start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name ) resp = self . _callback ( req ) if resp . code != 0 : raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message ) if resp . type == message_type . MSG_AGGREGATE_RESULT : data = homo_pb2 . AggregateResult () data . ParseFromString ( resp . data ) self . _strategy . handler_aggregate_result ( data ) elif resp . type == message_type . MSG_NOTIFY_TRAINING : self . _strategy . handler_notify_training () elif resp . type in self . _strategy . custom_handlers : self . _strategy . custom_handlers [ resp . type ]() # type: ignore if resp . type != \"\" : stop = timeit . default_timer () logger . info ( f \"IN: party: message type: { resp . type } , time: { 1000 * ( stop - start ) } ms\" ) time . sleep ( message_type . MSG_HEARTBEAT_INTERVAL )","title":"notice()"},{"location":"api/reference/communication/homo/homo_client/#iflearner.communication.homo.homo_client.HomoClient.transport","text":"Transport data to server. Source code in iflearner/communication/homo/homo_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def transport ( self , type : str , data : Any = None ) -> homo_pb2 . RegistrationResponse : \"\"\"Transport data to server.\"\"\" start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = type ) if data is not None : req . data = data . SerializeToString () resp = None if type == message_type . MSG_REGISTER : resp = self . _send ( req ) elif type == message_type . MSG_CLIENT_READY : resp = self . _send ( req ) elif type == message_type . MSG_UPLOAD_PARAM : resp = self . _post ( req ) elif type == message_type . MSG_COMPLETE : resp = self . _send ( req ) stop = timeit . default_timer () logger . info ( f \"OUT: message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp . code != 0 : # type: ignore raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message # type: ignore ) if type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationResponse () data . ParseFromString ( resp . data ) # type: ignore return data","title":"transport()"},{"location":"api/reference/communication/homo/homo_exception/","text":"HomoException ( code , message ) \u00b6 Bases: base_exception . BaseException Source code in iflearner/communication/homo/homo_exception.py 30 31 32 33 def __init__ ( self , code : HomoResponseCode , message : str ) -> None : super () . __init__ ( code . value , f \" { code . __class__ . __name__ } . { code . name } - { message } \" ) HomoResponseCode \u00b6 Bases: IntEnum Define response code","title":"homo_exception"},{"location":"api/reference/communication/homo/homo_exception/#iflearner.communication.homo.homo_exception.HomoException","text":"Bases: base_exception . BaseException Source code in iflearner/communication/homo/homo_exception.py 30 31 32 33 def __init__ ( self , code : HomoResponseCode , message : str ) -> None : super () . __init__ ( code . value , f \" { code . __class__ . __name__ } . { code . name } - { message } \" )","title":"HomoException"},{"location":"api/reference/communication/homo/homo_exception/#iflearner.communication.homo.homo_exception.HomoException.HomoResponseCode","text":"Bases: IntEnum Define response code","title":"HomoResponseCode"},{"location":"api/reference/communication/homo/homo_pb2/","text":"Generated protocol buffer code.","title":"Homo pb2"},{"location":"api/reference/communication/homo/homo_pb2_grpc/","text":"Client and server classes corresponding to protobuf-defined services.","title":"Homo pb2 grpc"},{"location":"api/reference/communication/homo/homo_server/","text":"HomoServer ( strategy ) \u00b6 Bases: base_server . BaseServer Implement homogeneous server base on base_server.BaseServer. Source code in iflearner/communication/homo/homo_server.py 29 30 31 def __init__ ( self , strategy : strategy_server . StrategyServer ) -> None : self . _callback_messages : dict = dict () self . _strategy = strategy callback ( request , context ) \u00b6 The channel of pushing message to clients initiatively. Source code in iflearner/communication/homo/homo_server.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def callback ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"The channel of pushing message to clients initiatively.\"\"\" start = timeit . default_timer () type , resp_data = self . _strategy . get_client_notification ( request . party_name ) if type is not None : stop = timeit . default_timer () logger . info ( f \"OUT: party: { request . party_name } , message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp_data is None : return base_pb2 . BaseResponse ( type = type ) return base_pb2 . BaseResponse ( type = type , data = resp_data . SerializeToString ()) post ( request , context ) \u00b6 Handle client requests asynchronously. Source code in iflearner/communication/homo/homo_server.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def post ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests asynchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_UPLOAD_PARAM : req_data = homo_pb2 . UploadParam () req_data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_upload_param ( request . party_name , req_data ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" ) send ( request , context ) \u00b6 Handle client requests synchronously. Source code in iflearner/communication/homo/homo_server.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests synchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationInfo () data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_register ( request . party_name , data . sample_num , data . step_num ) elif request . type == message_type . MSG_CLIENT_READY : resp_data = self . _strategy . handler_client_ready ( request . party_name ) # type: ignore elif request . type == message_type . MSG_COMPLETE : resp_data = self . _strategy . handler_complete ( request . party_name ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . party_name , request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" )","title":"homo_server"},{"location":"api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer","text":"Bases: base_server . BaseServer Implement homogeneous server base on base_server.BaseServer. Source code in iflearner/communication/homo/homo_server.py 29 30 31 def __init__ ( self , strategy : strategy_server . StrategyServer ) -> None : self . _callback_messages : dict = dict () self . _strategy = strategy","title":"HomoServer"},{"location":"api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer.callback","text":"The channel of pushing message to clients initiatively. Source code in iflearner/communication/homo/homo_server.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def callback ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"The channel of pushing message to clients initiatively.\"\"\" start = timeit . default_timer () type , resp_data = self . _strategy . get_client_notification ( request . party_name ) if type is not None : stop = timeit . default_timer () logger . info ( f \"OUT: party: { request . party_name } , message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp_data is None : return base_pb2 . BaseResponse ( type = type ) return base_pb2 . BaseResponse ( type = type , data = resp_data . SerializeToString ())","title":"callback()"},{"location":"api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer.post","text":"Handle client requests asynchronously. Source code in iflearner/communication/homo/homo_server.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def post ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests asynchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_UPLOAD_PARAM : req_data = homo_pb2 . UploadParam () req_data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_upload_param ( request . party_name , req_data ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" )","title":"post()"},{"location":"api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer.send","text":"Handle client requests synchronously. Source code in iflearner/communication/homo/homo_server.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests synchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationInfo () data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_register ( request . party_name , data . sample_num , data . step_num ) elif request . type == message_type . MSG_CLIENT_READY : resp_data = self . _strategy . handler_client_ready ( request . party_name ) # type: ignore elif request . type == message_type . MSG_COMPLETE : resp_data = self . _strategy . handler_complete ( request . party_name ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . party_name , request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" )","title":"send()"},{"location":"api/reference/communication/homo/message_type/","text":"Define the heartbeat interval between client and server. MSG_COMPLETE = 'msg_complete' module-attribute \u00b6 Define the message type for communication. (From server to client) MSG_HEARTBEAT_INTERVAL = 1 module-attribute \u00b6 Define the message type for communication. (From client to server) MSG_NOTIFY_TRAINING = 'msg_notify_training' module-attribute \u00b6 Define the name of strategy.","title":"message_type"},{"location":"api/reference/communication/homo/message_type/#iflearner.communication.homo.message_type.MSG_COMPLETE","text":"Define the message type for communication. (From server to client)","title":"MSG_COMPLETE"},{"location":"api/reference/communication/homo/message_type/#iflearner.communication.homo.message_type.MSG_HEARTBEAT_INTERVAL","text":"Define the message type for communication. (From client to server)","title":"MSG_HEARTBEAT_INTERVAL"},{"location":"api/reference/communication/homo/message_type/#iflearner.communication.homo.message_type.MSG_NOTIFY_TRAINING","text":"Define the name of strategy.","title":"MSG_NOTIFY_TRAINING"},{"location":"api/reference/communication/peer/","text":"","title":"Index"},{"location":"api/reference/communication/peer/aes/","text":"AESCipher ( key ) \u00b6 Bases: object Source code in iflearner/communication/peer/aes.py 10 11 12 def __init__ ( self , key : str ) -> None : self . bs = AES . block_size self . key = hashlib . sha256 ( key . encode ()) . digest () decrypt ( enc ) \u00b6 Decrypt data Source code in iflearner/communication/peer/aes.py 22 23 24 25 26 27 28 def decrypt ( self , enc : List ) -> Any : \"\"\"Decrypt data\"\"\" enc = base64 . b64decode ( enc ) iv = enc [: AES . block_size ] cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return self . _unpad ( cipher . decrypt ( enc [ AES . block_size :])) . decode ( \"utf-8\" ) encrypt ( raw ) \u00b6 Encrypt string to bytes Source code in iflearner/communication/peer/aes.py 14 15 16 17 18 19 20 def encrypt ( self , raw : str ) -> bytes : \"\"\"Encrypt string to bytes\"\"\" raw = self . _pad ( raw ) iv = Random . new () . read ( AES . block_size ) cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return base64 . b64encode ( iv + cipher . encrypt ( raw . encode ()))","title":"aes"},{"location":"api/reference/communication/peer/aes/#iflearner.communication.peer.aes.AESCipher","text":"Bases: object Source code in iflearner/communication/peer/aes.py 10 11 12 def __init__ ( self , key : str ) -> None : self . bs = AES . block_size self . key = hashlib . sha256 ( key . encode ()) . digest ()","title":"AESCipher"},{"location":"api/reference/communication/peer/aes/#iflearner.communication.peer.aes.AESCipher.decrypt","text":"Decrypt data Source code in iflearner/communication/peer/aes.py 22 23 24 25 26 27 28 def decrypt ( self , enc : List ) -> Any : \"\"\"Decrypt data\"\"\" enc = base64 . b64decode ( enc ) iv = enc [: AES . block_size ] cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return self . _unpad ( cipher . decrypt ( enc [ AES . block_size :])) . decode ( \"utf-8\" )","title":"decrypt()"},{"location":"api/reference/communication/peer/aes/#iflearner.communication.peer.aes.AESCipher.encrypt","text":"Encrypt string to bytes Source code in iflearner/communication/peer/aes.py 14 15 16 17 18 19 20 def encrypt ( self , raw : str ) -> bytes : \"\"\"Encrypt string to bytes\"\"\" raw = self . _pad ( raw ) iv = Random . new () . read ( AES . block_size ) cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return base64 . b64encode ( iv + cipher . encrypt ( raw . encode ()))","title":"encrypt()"},{"location":"api/reference/communication/peer/diffie_hellman/","text":"DiffieHellman \u00b6 Bases: object key_pair ( num_bits = 1024 , pair_name = None ) staticmethod \u00b6 Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator Source code in iflearner/communication/peer/diffie_hellman.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @staticmethod def key_pair ( num_bits = 1024 , pair_name = None ): \"\"\" Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator \"\"\" if pair_name is None : if num_bits : return DiffieHellman . _key_pair ( num_bits ) else : return DiffieHellman . _additional_group_1024_160 () assert pair_name in { \"additional_group_1024_160\" , \"additional_group_2048_224\" , \"additional_group_2048_256\" , \"oakley_group_768_768\" , \"oakley_group_1024_1024\" , }, \"unsupported pair name: {0} \" . format ( pair_name ) if pair_name == \"additional_group_1024_160\" : return DiffieHellman . _additional_group_1024_160 () if pair_name == \"additional_group_2048_224\" : return DiffieHellman . _additional_group_2048_224 () if pair_name == \"additional_group_2048_256\" : return DiffieHellman . _additional_group_2048_256 () if pair_name == \"oakley_group_768_768\" : return DiffieHellman . _oakley_group_768_768 () if pair_name == \"oakley_group_1024_1024\" : return DiffieHellman . _oakley_group_1024_1024 ()","title":"diffie_hellman"},{"location":"api/reference/communication/peer/diffie_hellman/#iflearner.communication.peer.diffie_hellman.DiffieHellman","text":"Bases: object","title":"DiffieHellman"},{"location":"api/reference/communication/peer/diffie_hellman/#iflearner.communication.peer.diffie_hellman.DiffieHellman.key_pair","text":"Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator Source code in iflearner/communication/peer/diffie_hellman.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @staticmethod def key_pair ( num_bits = 1024 , pair_name = None ): \"\"\" Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator \"\"\" if pair_name is None : if num_bits : return DiffieHellman . _key_pair ( num_bits ) else : return DiffieHellman . _additional_group_1024_160 () assert pair_name in { \"additional_group_1024_160\" , \"additional_group_2048_224\" , \"additional_group_2048_256\" , \"oakley_group_768_768\" , \"oakley_group_1024_1024\" , }, \"unsupported pair name: {0} \" . format ( pair_name ) if pair_name == \"additional_group_1024_160\" : return DiffieHellman . _additional_group_1024_160 () if pair_name == \"additional_group_2048_224\" : return DiffieHellman . _additional_group_2048_224 () if pair_name == \"additional_group_2048_256\" : return DiffieHellman . _additional_group_2048_256 () if pair_name == \"oakley_group_768_768\" : return DiffieHellman . _oakley_group_768_768 () if pair_name == \"oakley_group_1024_1024\" : return DiffieHellman . _oakley_group_1024_1024 ()","title":"key_pair()"},{"location":"api/reference/communication/peer/diffie_hellman_inst/","text":"DiffieHellmanInst \u00b6 Bases: object The Diffie-Hellman instance for generating public key and secret.","title":"diffie_hellman_inst"},{"location":"api/reference/communication/peer/diffie_hellman_inst/#iflearner.communication.peer.diffie_hellman_inst.DiffieHellmanInst","text":"Bases: object The Diffie-Hellman instance for generating public key and secret.","title":"DiffieHellmanInst"},{"location":"api/reference/communication/peer/message_type/","text":"The public key of diffie hellman . MSG_DH_PUBLIC_KEY = 'msg_dh_public_key' module-attribute \u00b6 The random key in SMPC.","title":"Message type"},{"location":"api/reference/communication/peer/message_type/#iflearner.communication.peer.message_type.MSG_DH_PUBLIC_KEY","text":"The random key in SMPC.","title":"MSG_DH_PUBLIC_KEY"},{"location":"api/reference/communication/peer/peer_client/","text":"PeerClient ( server_addr , party_name , peer_cert = None ) \u00b6 Bases: base_client . BaseClient The client for peer party communication. Source code in iflearner/communication/peer/peer_client.py 29 30 31 def __init__ ( self , server_addr : str , party_name : str , peer_cert : str = None ) -> None : super () . __init__ ( server_addr , peer_cert ) self . _party_name = party_name get_DH_public_key () \u00b6 Get Diffie-Hellman public key from specified server. Returns: Type Description List The public key. Source code in iflearner/communication/peer/peer_client.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def get_DH_public_key ( self ) -> List : \"\"\"Get Diffie-Hellman public key from specified server. Returns: The public key. \"\"\" while True : try : req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_DH_PUBLIC_KEY , data = DiffieHellmanInst . generate_public_key (), ) resp = self . _send ( req ) public_key = resp . data logger . info ( f \"Public key: { public_key } \" ) return public_key except Exception as e : logger . info ( e ) time . sleep ( 3 ) get_SMPC_random_key ( key ) \u00b6 Get random value from the other party. Returns: Type Description float A float value. Source code in iflearner/communication/peer/peer_client.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_SMPC_random_key ( self , key : str ) -> float : \"\"\"Get random value from the other party. Returns: A float value. \"\"\" req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_SMPC_RANDOM_KEY ) resp = self . _send ( req ) random_float = float ( aes . AESCipher ( key ) . decrypt ( resp . data )) logger . info ( f \"Random float: { random_float } \" ) return random_float","title":"peer_client"},{"location":"api/reference/communication/peer/peer_client/#iflearner.communication.peer.peer_client.PeerClient","text":"Bases: base_client . BaseClient The client for peer party communication. Source code in iflearner/communication/peer/peer_client.py 29 30 31 def __init__ ( self , server_addr : str , party_name : str , peer_cert : str = None ) -> None : super () . __init__ ( server_addr , peer_cert ) self . _party_name = party_name","title":"PeerClient"},{"location":"api/reference/communication/peer/peer_client/#iflearner.communication.peer.peer_client.PeerClient.get_DH_public_key","text":"Get Diffie-Hellman public key from specified server. Returns: Type Description List The public key. Source code in iflearner/communication/peer/peer_client.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def get_DH_public_key ( self ) -> List : \"\"\"Get Diffie-Hellman public key from specified server. Returns: The public key. \"\"\" while True : try : req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_DH_PUBLIC_KEY , data = DiffieHellmanInst . generate_public_key (), ) resp = self . _send ( req ) public_key = resp . data logger . info ( f \"Public key: { public_key } \" ) return public_key except Exception as e : logger . info ( e ) time . sleep ( 3 )","title":"get_DH_public_key()"},{"location":"api/reference/communication/peer/peer_client/#iflearner.communication.peer.peer_client.PeerClient.get_SMPC_random_key","text":"Get random value from the other party. Returns: Type Description float A float value. Source code in iflearner/communication/peer/peer_client.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_SMPC_random_key ( self , key : str ) -> float : \"\"\"Get random value from the other party. Returns: A float value. \"\"\" req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_SMPC_RANDOM_KEY ) resp = self . _send ( req ) random_float = float ( aes . AESCipher ( key ) . decrypt ( resp . data )) logger . info ( f \"Random float: { random_float } \" ) return random_float","title":"get_SMPC_random_key()"},{"location":"api/reference/communication/peer/peer_server/","text":"PeerServer ( peer_num ) \u00b6 Bases: base_server . BaseServer The server for peer party communication. Source code in iflearner/communication/peer/peer_server.py 29 30 31 32 33 34 def __init__ ( self , peer_num : int ) -> None : super () . __init__ () self . _parties_secret = dict () self . _parties_random_value = dict () self . _peer_num = peer_num send ( request , context ) \u00b6 Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY. Source code in iflearner/communication/peer/peer_server.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY.\"\"\" logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } \" ) if request . type == message_type . MSG_DH_PUBLIC_KEY : self . _parties_secret [ request . party_name ] = DiffieHellmanInst () . generate_secret ( request . data ) return base_pb2 . BaseResponse ( data = DiffieHellmanInst () . generate_public_key ()) elif request . type == message_type . MSG_SMPC_RANDOM_KEY : random_float = random . uniform ( 0.1 , 1.0 ) logger . info ( f \"Party: { request . party_name } , Random float: { random_float } \" ) self . _parties_random_value [ request . party_name ] = - random_float # return base_pb2.BaseResponse(data=bytearray(struct.pack('f', random_float))) return base_pb2 . BaseResponse ( data = aes . AESCipher ( self . _parties_secret [ request . party_name ]) . encrypt ( str ( random_float ) ) ) sum_parties_random_value () \u00b6 When the values from each party are received, we add all the values together. Source code in iflearner/communication/peer/peer_server.py 36 37 38 39 40 41 42 43 def sum_parties_random_value ( self ) -> float : \"\"\"When the values from each party are received, we add all the values together.\"\"\" while True : if self . _peer_num == len ( self . _parties_random_value . values ()): return sum ( self . _parties_random_value . values ()) time . sleep ( 1 )","title":"peer_server"},{"location":"api/reference/communication/peer/peer_server/#iflearner.communication.peer.peer_server.PeerServer","text":"Bases: base_server . BaseServer The server for peer party communication. Source code in iflearner/communication/peer/peer_server.py 29 30 31 32 33 34 def __init__ ( self , peer_num : int ) -> None : super () . __init__ () self . _parties_secret = dict () self . _parties_random_value = dict () self . _peer_num = peer_num","title":"PeerServer"},{"location":"api/reference/communication/peer/peer_server/#iflearner.communication.peer.peer_server.PeerServer.send","text":"Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY. Source code in iflearner/communication/peer/peer_server.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY.\"\"\" logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } \" ) if request . type == message_type . MSG_DH_PUBLIC_KEY : self . _parties_secret [ request . party_name ] = DiffieHellmanInst () . generate_secret ( request . data ) return base_pb2 . BaseResponse ( data = DiffieHellmanInst () . generate_public_key ()) elif request . type == message_type . MSG_SMPC_RANDOM_KEY : random_float = random . uniform ( 0.1 , 1.0 ) logger . info ( f \"Party: { request . party_name } , Random float: { random_float } \" ) self . _parties_random_value [ request . party_name ] = - random_float # return base_pb2.BaseResponse(data=bytearray(struct.pack('f', random_float))) return base_pb2 . BaseResponse ( data = aes . AESCipher ( self . _parties_secret [ request . party_name ]) . encrypt ( str ( random_float ) ) )","title":"send()"},{"location":"api/reference/communication/peer/peer_server/#iflearner.communication.peer.peer_server.PeerServer.sum_parties_random_value","text":"When the values from each party are received, we add all the values together. Source code in iflearner/communication/peer/peer_server.py 36 37 38 39 40 41 42 43 def sum_parties_random_value ( self ) -> float : \"\"\"When the values from each party are received, we add all the values together.\"\"\" while True : if self . _peer_num == len ( self . _parties_random_value . values ()): return sum ( self . _parties_random_value . values ()) time . sleep ( 1 )","title":"sum_parties_random_value()"},{"location":"api/reference/datasets/","text":"","title":"Index"},{"location":"api/reference/datasets/cifar/","text":"CIFAR10 ( root , download = False ) \u00b6 Bases: FLDateset CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True. required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. False Source code in iflearner/datasets/cifar.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , root : str , download : bool = False , ) -> None : super ( CIFAR10 , self ) . __init__ () self . root = root if download : self . download () if not self . _check_integrity (): raise RuntimeError ( \"Dataset not found or corrupted.\" + \" You can use download=True to download it\" ) self . train_x = [] self . test_x = [] self . train_targets = [] self . test_targets = [] # now load the picked numpy arrays for file_name , checksum in self . train_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . train_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . train_targets . extend ( entry [ \"labels\" ]) else : self . train_targets . extend ( entry [ \"fine_labels\" ]) self . train_x = np . vstack ( self . train_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . train_x = self . train_x . transpose (( 0 , 2 , 3 , 1 )) # convert to HWC for file_name , checksum in self . test_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . test_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . test_targets . extend ( entry [ \"labels\" ]) else : self . test_targets . extend ( entry [ \"fine_labels\" ]) self . test_x = np . vstack ( self . test_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . test_x = self . test_x . transpose (( 0 , 2 , 3 , 1 )) self . _load_meta () CIFAR100 \u00b6 Bases: CIFAR10 CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. This is a subclass of the CIFAR10 Dataset.","title":"Cifar"},{"location":"api/reference/datasets/cifar/#iflearner.datasets.cifar.CIFAR10","text":"Bases: FLDateset CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True. required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. False Source code in iflearner/datasets/cifar.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , root : str , download : bool = False , ) -> None : super ( CIFAR10 , self ) . __init__ () self . root = root if download : self . download () if not self . _check_integrity (): raise RuntimeError ( \"Dataset not found or corrupted.\" + \" You can use download=True to download it\" ) self . train_x = [] self . test_x = [] self . train_targets = [] self . test_targets = [] # now load the picked numpy arrays for file_name , checksum in self . train_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . train_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . train_targets . extend ( entry [ \"labels\" ]) else : self . train_targets . extend ( entry [ \"fine_labels\" ]) self . train_x = np . vstack ( self . train_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . train_x = self . train_x . transpose (( 0 , 2 , 3 , 1 )) # convert to HWC for file_name , checksum in self . test_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . test_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . test_targets . extend ( entry [ \"labels\" ]) else : self . test_targets . extend ( entry [ \"fine_labels\" ]) self . test_x = np . vstack ( self . test_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . test_x = self . test_x . transpose (( 0 , 2 , 3 , 1 )) self . _load_meta ()","title":"CIFAR10"},{"location":"api/reference/datasets/cifar/#iflearner.datasets.cifar.CIFAR100","text":"Bases: CIFAR10 CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. This is a subclass of the CIFAR10 Dataset.","title":"CIFAR100"},{"location":"api/reference/datasets/fl_dataset/","text":"","title":"Fl dataset"},{"location":"api/reference/datasets/mnist/","text":"EMNIST ( root , split = 'mnist' , ** kwargs ) \u00b6 Bases: MNIST EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_researc h/emnist> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where EMNIST/processed/training.pt and EMNIST/processed/test.pt exist. required split string The dataset has 6 different splits: byclass , bymerge , balanced , letters , digits and mnist . This argument specifies which one to use. 'mnist' train bool If True, creates dataset from training.pt , otherwise from test.pt . required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. required transform callable A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop required target_transform callable A function/transform that takes in the target and transforms it. required Source code in iflearner/datasets/mnist.py 209 210 211 212 213 214 def __init__ ( self , root : str , split : str = \"mnist\" , ** kwargs : Any ) -> None : self . split = verify_str_arg ( split , \"split\" , self . splits ) self . training_file = self . _training_file ( split ) self . test_file = self . _test_file ( split ) super ( EMNIST , self ) . __init__ ( root , ** kwargs ) self . classes = self . classes_split_dict [ self . split ] download () \u00b6 Download the EMNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def download ( self ) -> None : \"\"\"Download the EMNIST data if it doesn't exist in processed_folder already.\"\"\" import shutil if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files print ( \"Downloading and extracting zip archive\" ) download_and_extract_archive ( self . url , download_root = self . raw_folder , filename = \"emnist.zip\" , remove_finished = True , md5 = self . md5 , ) gzip_folder = os . path . join ( self . raw_folder , \"gzip\" ) for gzip_file in os . listdir ( gzip_folder ): if gzip_file . endswith ( \".gz\" ): extract_archive ( os . path . join ( gzip_folder , gzip_file ), gzip_folder ) # process and save as torch files for split in self . splits : print ( \"Processing \" + split ) training_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -train-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -train-labels-idx1-ubyte\" . format ( split ) ) ), ) test_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -test-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -test-labels-idx1-ubyte\" . format ( split ) ) ), ) with open ( os . path . join ( self . processed_folder , self . _training_file ( split )), \"wb\" ) as f : pickle . dump ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . _test_file ( split )), \"wb\" ) as f : pickle . dump ( test_set , f ) shutil . rmtree ( gzip_folder ) print ( \"Done!\" ) MNIST ( root , download = False ) \u00b6 Bases: FLDateset Source code in iflearner/datasets/mnist.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , root : str , download : bool = False ): super () . __init__ () self . root = root self . raw_folder = os . path . join ( self . root , self . __class__ . __name__ , \"raw\" ) self . processed_folder = os . path . join ( self . root , self . __class__ . __name__ , \"processed\" ) if download : self . download () with open ( os . path . join ( self . processed_folder , self . training_file ), \"rb\" ) as f : self . train_x , self . train_targets = pickle . load ( f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"rb\" ) as f : self . test_x , self . test_targets = pickle . load ( f ) download () \u00b6 Download the MNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download ( self ) -> None : \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\" if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files for url , md5 in self . resources : filename = url . rpartition ( \"/\" )[ 2 ] download_and_extract_archive ( url , download_root = self . raw_folder , filename = filename , md5 = md5 ) train_set = ( read_image_file ( os . path . join ( self . raw_folder , \"train-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"train-labels-idx1-ubyte\" )), ) test_set = ( read_image_file ( os . path . join ( self . raw_folder , \"t10k-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"t10k-labels-idx1-ubyte\" )), ) with open ( os . path . join ( self . processed_folder , self . training_file ), \"wb\" ) as f : pickle . dump ( train_set , f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"wb\" ) as f : pickle . dump ( test_set , f )","title":"Mnist"},{"location":"api/reference/datasets/mnist/#iflearner.datasets.mnist.EMNIST","text":"Bases: MNIST EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_researc h/emnist> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where EMNIST/processed/training.pt and EMNIST/processed/test.pt exist. required split string The dataset has 6 different splits: byclass , bymerge , balanced , letters , digits and mnist . This argument specifies which one to use. 'mnist' train bool If True, creates dataset from training.pt , otherwise from test.pt . required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. required transform callable A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop required target_transform callable A function/transform that takes in the target and transforms it. required Source code in iflearner/datasets/mnist.py 209 210 211 212 213 214 def __init__ ( self , root : str , split : str = \"mnist\" , ** kwargs : Any ) -> None : self . split = verify_str_arg ( split , \"split\" , self . splits ) self . training_file = self . _training_file ( split ) self . test_file = self . _test_file ( split ) super ( EMNIST , self ) . __init__ ( root , ** kwargs ) self . classes = self . classes_split_dict [ self . split ]","title":"EMNIST"},{"location":"api/reference/datasets/mnist/#iflearner.datasets.mnist.EMNIST.download","text":"Download the EMNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def download ( self ) -> None : \"\"\"Download the EMNIST data if it doesn't exist in processed_folder already.\"\"\" import shutil if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files print ( \"Downloading and extracting zip archive\" ) download_and_extract_archive ( self . url , download_root = self . raw_folder , filename = \"emnist.zip\" , remove_finished = True , md5 = self . md5 , ) gzip_folder = os . path . join ( self . raw_folder , \"gzip\" ) for gzip_file in os . listdir ( gzip_folder ): if gzip_file . endswith ( \".gz\" ): extract_archive ( os . path . join ( gzip_folder , gzip_file ), gzip_folder ) # process and save as torch files for split in self . splits : print ( \"Processing \" + split ) training_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -train-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -train-labels-idx1-ubyte\" . format ( split ) ) ), ) test_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -test-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -test-labels-idx1-ubyte\" . format ( split ) ) ), ) with open ( os . path . join ( self . processed_folder , self . _training_file ( split )), \"wb\" ) as f : pickle . dump ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . _test_file ( split )), \"wb\" ) as f : pickle . dump ( test_set , f ) shutil . rmtree ( gzip_folder ) print ( \"Done!\" )","title":"download()"},{"location":"api/reference/datasets/mnist/#iflearner.datasets.mnist.MNIST","text":"Bases: FLDateset Source code in iflearner/datasets/mnist.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , root : str , download : bool = False ): super () . __init__ () self . root = root self . raw_folder = os . path . join ( self . root , self . __class__ . __name__ , \"raw\" ) self . processed_folder = os . path . join ( self . root , self . __class__ . __name__ , \"processed\" ) if download : self . download () with open ( os . path . join ( self . processed_folder , self . training_file ), \"rb\" ) as f : self . train_x , self . train_targets = pickle . load ( f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"rb\" ) as f : self . test_x , self . test_targets = pickle . load ( f )","title":"MNIST"},{"location":"api/reference/datasets/mnist/#iflearner.datasets.mnist.MNIST.download","text":"Download the MNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download ( self ) -> None : \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\" if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files for url , md5 in self . resources : filename = url . rpartition ( \"/\" )[ 2 ] download_and_extract_archive ( url , download_root = self . raw_folder , filename = filename , md5 = md5 ) train_set = ( read_image_file ( os . path . join ( self . raw_folder , \"train-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"train-labels-idx1-ubyte\" )), ) test_set = ( read_image_file ( os . path . join ( self . raw_folder , \"t10k-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"t10k-labels-idx1-ubyte\" )), ) with open ( os . path . join ( self . processed_folder , self . training_file ), \"wb\" ) as f : pickle . dump ( train_set , f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"wb\" ) as f : pickle . dump ( test_set , f )","title":"download()"},{"location":"api/reference/datasets/sampler/","text":"","title":"Sampler"},{"location":"api/reference/datasets/split_dataset/","text":"","title":"Split dataset"},{"location":"api/reference/datasets/utils/","text":"download_file_from_google_drive ( file_id , root , filename = None , md5 = None ) \u00b6 Download a Google Drive file from and place it in root. Parameters: Name Type Description Default file_id str id of file to be downloaded required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the id of the file. None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def download_file_from_google_drive ( file_id : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ): \"\"\"Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url import requests url = \"https://docs.google.com/uc?export=download\" root = os . path . expanduser ( root ) if not filename : filename = file_id fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : session = requests . Session () response = session . get ( url , params = { \"id\" : file_id }, stream = True ) token = _get_confirm_token ( response ) if token : params = { \"id\" : file_id , \"confirm\" : token } response = session . get ( url , params = params , stream = True ) if _quota_exceeded ( response ): msg = ( f \"The daily quota of the file { filename } is exceeded and it \" f \"can't be downloaded. This is a limitation of Google Drive \" f \"and can only be overcome by trying again later.\" ) raise RuntimeError ( msg ) _save_response_content ( response , fpath ) download_url ( url , root , filename = None , md5 = None ) \u00b6 Download a file from a url and place it in root. Parameters: Name Type Description Default url str URL to download file from required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the basename of the URL None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download_url ( url : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ) -> None : \"\"\"Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" import urllib root = os . path . expanduser ( root ) if not filename : filename = os . path . basename ( url ) fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) # check if file is already present locally if check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : # download the file try : print ( \"Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) # type: ignore[attr-defined] except ( urllib . error . URLError , IOError ) as e : if url [: 5 ] == \"https\" : url = url . replace ( \"https:\" , \"http:\" ) print ( \"Failed download. Trying https -> http instead.\" \" Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) else : raise e # check integrity of downloaded file if not check_integrity ( fpath , md5 ): raise RuntimeError ( \"File not found or corrupted.\" ) list_dir ( root , prefix = False ) \u00b6 List all directories at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required prefix bool If true, prepends the path to each result, otherwise only returns the name of the directories found False Source code in iflearner/datasets/utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def list_dir ( root : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all directories at a given root. Args: root (str): Path to directory whose folders need to be listed prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the directories found \"\"\" root = os . path . expanduser ( root ) directories = [ p for p in os . listdir ( root ) if os . path . isdir ( os . path . join ( root , p ))] if prefix is True : directories = [ os . path . join ( root , d ) for d in directories ] return directories list_files ( root , suffix , prefix = False ) \u00b6 List all files ending with a suffix at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required suffix str or tuple Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly required prefix bool If true, prepends the path to each result, otherwise only returns the name of the files found False Source code in iflearner/datasets/utils.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def list_files ( root : str , suffix : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all files ending with a suffix at a given root. Args: root (str): Path to directory whose folders need to be listed suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the files found \"\"\" root = os . path . expanduser ( root ) files = [ p for p in os . listdir ( root ) if os . path . isfile ( os . path . join ( root , p )) and p . endswith ( suffix ) ] if prefix is True : files = [ os . path . join ( root , d ) for d in files ] return files open_maybe_compressed_file ( path ) \u00b6 Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument path is a string and ends with '.gz' or '.xz'. Source code in iflearner/datasets/utils.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def open_maybe_compressed_file ( path : Union [ str , IO ]) -> Union [ IO , gzip . GzipFile ]: \"\"\"Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument `path` is a string and ends with '.gz' or '.xz'. \"\"\" import torch if not isinstance ( path , ( str , bytes )): return path if path . endswith ( \".gz\" ): return gzip . open ( path , \"rb\" ) if path . endswith ( \".xz\" ): return lzma . open ( path , \"rb\" ) return open ( path , \"rb\" ) read_sn3_pascalvincent_tensor ( path , strict = True ) \u00b6 Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. Source code in iflearner/datasets/utils.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def read_sn3_pascalvincent_tensor ( path : Union [ str , IO ], strict : bool = True ) -> np . ndarray : \"\"\"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. \"\"\" # read with open_maybe_compressed_file ( path ) as f : data = f . read () # parse magic = get_int ( data [ 0 : 4 ]) nd = magic % 256 ty = magic // 256 assert nd >= 1 and nd <= 3 assert ty >= 8 and ty <= 14 m = SN3_PASCALVINCENT_TYPEMAP [ ty ] s = [ get_int ( data [ 4 * ( i + 1 ): 4 * ( i + 2 )]) for i in range ( nd )] parsed = np . frombuffer ( data , dtype = m [ 1 ], offset = ( 4 * ( nd + 1 ))) assert parsed . shape [ 0 ] == np . prod ( s ) or not strict return parsed . astype ( m [ 1 ]) . reshape ( s )","title":"Utils"},{"location":"api/reference/datasets/utils/#iflearner.datasets.utils.download_file_from_google_drive","text":"Download a Google Drive file from and place it in root. Parameters: Name Type Description Default file_id str id of file to be downloaded required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the id of the file. None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def download_file_from_google_drive ( file_id : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ): \"\"\"Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url import requests url = \"https://docs.google.com/uc?export=download\" root = os . path . expanduser ( root ) if not filename : filename = file_id fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : session = requests . Session () response = session . get ( url , params = { \"id\" : file_id }, stream = True ) token = _get_confirm_token ( response ) if token : params = { \"id\" : file_id , \"confirm\" : token } response = session . get ( url , params = params , stream = True ) if _quota_exceeded ( response ): msg = ( f \"The daily quota of the file { filename } is exceeded and it \" f \"can't be downloaded. This is a limitation of Google Drive \" f \"and can only be overcome by trying again later.\" ) raise RuntimeError ( msg ) _save_response_content ( response , fpath )","title":"download_file_from_google_drive()"},{"location":"api/reference/datasets/utils/#iflearner.datasets.utils.download_url","text":"Download a file from a url and place it in root. Parameters: Name Type Description Default url str URL to download file from required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the basename of the URL None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download_url ( url : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ) -> None : \"\"\"Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" import urllib root = os . path . expanduser ( root ) if not filename : filename = os . path . basename ( url ) fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) # check if file is already present locally if check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : # download the file try : print ( \"Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) # type: ignore[attr-defined] except ( urllib . error . URLError , IOError ) as e : if url [: 5 ] == \"https\" : url = url . replace ( \"https:\" , \"http:\" ) print ( \"Failed download. Trying https -> http instead.\" \" Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) else : raise e # check integrity of downloaded file if not check_integrity ( fpath , md5 ): raise RuntimeError ( \"File not found or corrupted.\" )","title":"download_url()"},{"location":"api/reference/datasets/utils/#iflearner.datasets.utils.list_dir","text":"List all directories at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required prefix bool If true, prepends the path to each result, otherwise only returns the name of the directories found False Source code in iflearner/datasets/utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def list_dir ( root : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all directories at a given root. Args: root (str): Path to directory whose folders need to be listed prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the directories found \"\"\" root = os . path . expanduser ( root ) directories = [ p for p in os . listdir ( root ) if os . path . isdir ( os . path . join ( root , p ))] if prefix is True : directories = [ os . path . join ( root , d ) for d in directories ] return directories","title":"list_dir()"},{"location":"api/reference/datasets/utils/#iflearner.datasets.utils.list_files","text":"List all files ending with a suffix at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required suffix str or tuple Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly required prefix bool If true, prepends the path to each result, otherwise only returns the name of the files found False Source code in iflearner/datasets/utils.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def list_files ( root : str , suffix : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all files ending with a suffix at a given root. Args: root (str): Path to directory whose folders need to be listed suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the files found \"\"\" root = os . path . expanduser ( root ) files = [ p for p in os . listdir ( root ) if os . path . isfile ( os . path . join ( root , p )) and p . endswith ( suffix ) ] if prefix is True : files = [ os . path . join ( root , d ) for d in files ] return files","title":"list_files()"},{"location":"api/reference/datasets/utils/#iflearner.datasets.utils.open_maybe_compressed_file","text":"Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument path is a string and ends with '.gz' or '.xz'. Source code in iflearner/datasets/utils.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def open_maybe_compressed_file ( path : Union [ str , IO ]) -> Union [ IO , gzip . GzipFile ]: \"\"\"Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument `path` is a string and ends with '.gz' or '.xz'. \"\"\" import torch if not isinstance ( path , ( str , bytes )): return path if path . endswith ( \".gz\" ): return gzip . open ( path , \"rb\" ) if path . endswith ( \".xz\" ): return lzma . open ( path , \"rb\" ) return open ( path , \"rb\" )","title":"open_maybe_compressed_file()"},{"location":"api/reference/datasets/utils/#iflearner.datasets.utils.read_sn3_pascalvincent_tensor","text":"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. Source code in iflearner/datasets/utils.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def read_sn3_pascalvincent_tensor ( path : Union [ str , IO ], strict : bool = True ) -> np . ndarray : \"\"\"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. \"\"\" # read with open_maybe_compressed_file ( path ) as f : data = f . read () # parse magic = get_int ( data [ 0 : 4 ]) nd = magic % 256 ty = magic // 256 assert nd >= 1 and nd <= 3 assert ty >= 8 and ty <= 14 m = SN3_PASCALVINCENT_TYPEMAP [ ty ] s = [ get_int ( data [ 4 * ( i + 1 ): 4 * ( i + 2 )]) for i in range ( nd )] parsed = np . frombuffer ( data , dtype = m [ 1 ], offset = ( 4 * ( nd + 1 ))) assert parsed . shape [ 0 ] == np . prod ( s ) or not strict return parsed . astype ( m [ 1 ]) . reshape ( s )","title":"read_sn3_pascalvincent_tensor()"},{"location":"about/changelog/","text":"changelog \u00b6 All notable changes to this project will be recorded in this file. iflearner \u00b6 [Unreleased] \u00b6 Add \u00b6 Support the underlying communication protocol of GRPC, and complete the abstraction of the upper layer protocol Support access to deep learning frameworks such as Tensorflow, Pytorch, Mxnet, Keras, etc. Support common aggregation strategies, and support users to customize their own aggregation strategies Support SMPC, differential privacy security encryption strategy","title":"ChangeLog"},{"location":"about/changelog/#changelog","text":"All notable changes to this project will be recorded in this file.","title":"changelog"},{"location":"about/changelog/#iflearner","text":"","title":"iflearner"},{"location":"about/changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"about/changelog/#add","text":"Support the underlying communication protocol of GRPC, and complete the abstraction of the upper layer protocol Support access to deep learning frameworks such as Tensorflow, Pytorch, Mxnet, Keras, etc. Support common aggregation strategies, and support users to customize their own aggregation strategies Support SMPC, differential privacy security encryption strategy","title":"Add"},{"location":"about/contact/","text":"contact us \u00b6 If you have any questions you want to solve or want to join us to build a project together, you can contact us through the following contact channels, We very much welcome you to join us. contact path \u00b6 You can join our WeChat group. For frequently asked questions, we provide you with the corresponding FAQ document. Please use issues to file bugs. Please use pull requests to submit and contribute code.","title":"Contact Us"},{"location":"about/contact/#contact-us","text":"If you have any questions you want to solve or want to join us to build a project together, you can contact us through the following contact channels, We very much welcome you to join us.","title":"contact us"},{"location":"about/contact/#contact-path","text":"You can join our WeChat group. For frequently asked questions, we provide you with the corresponding FAQ document. Please use issues to file bugs. Please use pull requests to submit and contribute code.","title":"contact path"},{"location":"api/api_reference/","text":"API Reference \u00b6 Trainer \u00b6 1. class iflearner.business.homo.trainer.Trainer \u00b6 This is the base client which would to be inherited when you implement your own client, and it has four abstract functions containing get , set , fit and evaluate . def get ( self , param_type = ParameterType . ParameterModel ) -> dict : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement how to get the parameters or gradients from your model, and we call this function to get your model information. Then, we will send it to server for aggregating with other clients. IN: param_type: which one you want to get, parameter or gradient - ParameterType.ParameterModel (default) - ParameterType.ParameterGradient OUT: dict: k: str (the parameter name), v: np.ndarray (the parameter value) def set(self, parameters: dict, param_type=ParameterType.ParameterModel) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement how to set the parameters or gradients to your model , and we will call this function when we got aggregating result from server. IN : dict : the same as the return of `` get `` function param_type : which one you want to set , parameter or gradient - ParameterType . ParameterModel ( default ) - ParameterType . ParameterGradient OUT : none def fit ( self , epoch : int ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement the training process on one epoch, and we will call this function per epoch. IN: epoch: the index of epoch OUT: none def evaluate(self, epoch: int) -> dict: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement the evaluating process for model and return the metrics . We will call this function after called `` fit `` function at every time . IN : epoch : the index of epoch OUT : dict : k : str ( metric name ), v : float ( metric value ) 2. class iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u00b6 This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using PyTorch framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions. 3. class iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer \u00b6 This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using TensorFlow framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions. 4. class iflearner.business.homo.mxnet_trainer.MxnetTrainer \u00b6 This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using Mxnet framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions. 5. class iflearner.business.homo.keras_trainer.KerasTrainer \u00b6 This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using Keras framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions. Command Arguments \u00b6 iflearner.business.homo.argument.parser We provide some command arguments in advance, and you need call parser.parse_args function at the begining of your program. Of course, you can add your own command arguments by calling parser.add_argument function before parsering command arguments. There are default arguments as the follow: name You can specify the name of client, and the name need to be unique which can't be the same as other clients. epochs You can specify the total epochs of training, and when training achieved, the client will exit automatically. server You can specify the server address, eg: \"192.168.0.1:50001\". enable-ll We provide local training way at the same time which just use your own data, so you can compare the results between federal training with local training. The argument value is 1 (enable) or 0 (disable, default). peers We provide SMPC for secure aggregation, and you can specify addresses to enable this feature. eg: '192.168.0.1:50010;192.168.0.2:50010;192.168.0.3:50010' First one address is your own address, and other addresses behind are other clients' addresses. All of addresses use semicolon to separate. Controller \u00b6 class iflearner.business.homo.train_client.Controller This class is the driver of client and control the whole process, so you would instantiate the class to start your client. def __init__ ( self , args , trainer : Trainer ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" class initialization function IN: args: the return of ``iflearner.business.homo.argument.parser.parse_args`` trainer: the instance of ``iflearner.business.homo.trainer.Trainer`` OUT: none def run(self) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" You would call this function after your client has ready , and this function will block until training process has been completed .","title":"API REFERENCE"},{"location":"api/api_reference/#api-reference","text":"","title":"API Reference"},{"location":"api/api_reference/#trainer","text":"","title":"Trainer"},{"location":"api/api_reference/#1-class-iflearnerbusinesshomotrainertrainer","text":"This is the base client which would to be inherited when you implement your own client, and it has four abstract functions containing get , set , fit and evaluate . def get ( self , param_type = ParameterType . ParameterModel ) -> dict : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement how to get the parameters or gradients from your model, and we call this function to get your model information. Then, we will send it to server for aggregating with other clients. IN: param_type: which one you want to get, parameter or gradient - ParameterType.ParameterModel (default) - ParameterType.ParameterGradient OUT: dict: k: str (the parameter name), v: np.ndarray (the parameter value) def set(self, parameters: dict, param_type=ParameterType.ParameterModel) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement how to set the parameters or gradients to your model , and we will call this function when we got aggregating result from server. IN : dict : the same as the return of `` get `` function param_type : which one you want to set , parameter or gradient - ParameterType . ParameterModel ( default ) - ParameterType . ParameterGradient OUT : none def fit ( self , epoch : int ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement the training process on one epoch, and we will call this function per epoch. IN: epoch: the index of epoch OUT: none def evaluate(self, epoch: int) -> dict: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement the evaluating process for model and return the metrics . We will call this function after called `` fit `` function at every time . IN : epoch : the index of epoch OUT : dict : k : str ( metric name ), v : float ( metric value )","title":"1. class iflearner.business.homo.trainer.Trainer"},{"location":"api/api_reference/#2-class-iflearnerbusinesshomopytorch_trainerpytorchtrainer","text":"This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using PyTorch framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions.","title":"2. class iflearner.business.homo.pytorch_trainer.PyTorchTrainer"},{"location":"api/api_reference/#3-class-iflearnerbusinesshomotensorflow_trainertensorflowtrainer","text":"This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using TensorFlow framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions.","title":"3. class iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer"},{"location":"api/api_reference/#4-class-iflearnerbusinesshomomxnet_trainermxnettrainer","text":"This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using Mxnet framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions.","title":"4. class iflearner.business.homo.mxnet_trainer.MxnetTrainer"},{"location":"api/api_reference/#5-class-iflearnerbusinesshomokeras_trainerkerastrainer","text":"This class inherit from iflearner.business.homo.trainer.Trainer and implement two of four functions which are get and set . When you are using Keras framework, you can choose to inherit from this class, not the iflearner.business.homo.trainer.Trainer . Then, you just need to implement fit and evaluate functions.","title":"5. class iflearner.business.homo.keras_trainer.KerasTrainer"},{"location":"api/api_reference/#command-arguments","text":"iflearner.business.homo.argument.parser We provide some command arguments in advance, and you need call parser.parse_args function at the begining of your program. Of course, you can add your own command arguments by calling parser.add_argument function before parsering command arguments. There are default arguments as the follow: name You can specify the name of client, and the name need to be unique which can't be the same as other clients. epochs You can specify the total epochs of training, and when training achieved, the client will exit automatically. server You can specify the server address, eg: \"192.168.0.1:50001\". enable-ll We provide local training way at the same time which just use your own data, so you can compare the results between federal training with local training. The argument value is 1 (enable) or 0 (disable, default). peers We provide SMPC for secure aggregation, and you can specify addresses to enable this feature. eg: '192.168.0.1:50010;192.168.0.2:50010;192.168.0.3:50010' First one address is your own address, and other addresses behind are other clients' addresses. All of addresses use semicolon to separate.","title":"Command Arguments"},{"location":"api/api_reference/#controller","text":"class iflearner.business.homo.train_client.Controller This class is the driver of client and control the whole process, so you would instantiate the class to start your client. def __init__ ( self , args , trainer : Trainer ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" class initialization function IN: args: the return of ``iflearner.business.homo.argument.parser.parse_args`` trainer: the instance of ``iflearner.business.homo.trainer.Trainer`` OUT: none def run(self) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" You would call this function after your client has ready , and this function will block until training process has been completed .","title":"Controller"},{"location":"faq/faq/","text":"FAQ \u00b6","title":"Deploy"},{"location":"faq/faq/#faq","text":"","title":"FAQ"},{"location":"quick_start/installation/","text":"Installation IFLearner \u00b6 Python Version \u00b6 IFLearner requires Python 3.7 or above. Install \u00b6 You can execute the following command for quick installation: pip install iflearner Once the installation is complete, you can verify that the installation was successful by running the following command: python -c \"import iflearner;print(iflearner.__version__)\"","title":"Installation"},{"location":"quick_start/installation/#installation-iflearner","text":"","title":"Installation IFLearner"},{"location":"quick_start/installation/#python-version","text":"IFLearner requires Python 3.7 or above.","title":"Python Version"},{"location":"quick_start/installation/#install","text":"You can execute the following command for quick installation: pip install iflearner Once the installation is complete, you can verify that the installation was successful by running the following command: python -c \"import iflearner;print(iflearner.__version__)\"","title":"Install"},{"location":"quick_start/quickstart_keras/","text":"Quickstart (Keras) \u00b6 In this tutorial, we will describe how to use Ifleaner in the Keras framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use Keras for image classification tasks on MNIST data, we need to go ahead and install the Keras libraries: ``` shell pip install keras == 2 .9.0 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training Ifleaner Client \u00b6 Create a new file named quickstart_keras.py and do the following. 1. Define Model Network \u00b6 Firstly, you need define your model network by using Keras. from typing import Any , Tuple from keras.layers import Activation , Dense from keras.models import Sequential from keras.optimizers import RMSprop from keras.utils import np_utils # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 ) 2. Implement Trainer Class \u00b6 Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.keras_trainer.KerasTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: class Mnist ( KerasTrainer ): def __init__ ( self ): # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 ) self . _model = model super ( Mnist , self ) . __init__ ( model = model ) ( x_train , y_train ), ( x_test , y_test ) = self . _load_data () self . _x_train = x_train self . _y_train = y_train self . _x_test = x_test self . _y_test = y_test @staticmethod def _load_data () -> Dataset : # \u4e0b\u8f7dMNIST\u6570\u636e # X shape(60000, 28*28) y shape(10000, ) ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # \u6570\u636e\u9884\u5904\u7406 x_train = x_train . reshape ( x_train . shape [ 0 ], - 1 ) / 255 # normalize x_test = x_test . reshape ( x_test . shape [ 0 ], - 1 ) / 255 # normalize # \u5c06\u7c7b\u5411\u91cf\u8f6c\u5316\u4e3a\u7c7b\u77e9\u9635 \u6570\u5b57 5 \u8f6c\u6362\u4e3a 0 0 0 0 0 1 0 0 0 0 \u77e9\u9635 y_train = np_utils . to_categorical ( y_train , num_classes = 10 ) y_test = np_utils . to_categorical ( y_test , num_classes = 10 ) return ( x_train , y_train ), ( x_test , y_test ) def fit ( self , epoch : int ): self . _model . fit ( self . _x_train , self . _y_train , epochs = 1 , batch_size = 32 ) def evaluate ( self , epoch : int ) -> dict : loss , accuracy = self . _model . evaluate ( self . _x_test , self . _y_test ) print ( f \"epoch: { epoch } | accuracy: { accuracy } | loss: { loss } \" ) return { \"loss\" : loss , \"accuracy\" : accuracy } 3. Start Ifleaner Client \u00b6 Lastly, you need to write a main function to start your client. You can do it as the follow: if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_keras.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_keras.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms (iflearner) yiyezhiqiu:quickstart_mxnet lucky$ cd ../quickstart_keras/ (iflearner) yiyezhiqiu:quickstart_keras lucky$ python quickstart_keras.py --name client01 --epochs 2 Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:28:25.569565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(RMSprop, self).__init__(name, **kwargs) 2022-08-03 18:28:27.137 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:28:27.384 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 246.81410300000016ms 2022-08-03 18:28:27.385 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:28:27.386 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:28:27.391 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 4.523920999998765ms 2022-08-03 18:28:54.529 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4368589999946835ms 2022-08-03 18:28:55.466 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 6s 2ms/step - loss: 0.3668 - accuracy: 0.8968 2022-08-03 18:29:01.852 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 1s 2ms/step - loss: 0.2343 - accuracy: 0.9348 epoch:1 | accuracy:0.9348000288009644 | loss:0.23433993756771088 2022-08-03 18:29:02.782 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:29:02.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 4.293857000000401ms 2022-08-03 18:29:03.773 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 13.262433999997825ms 2022-08-03 18:29:03.795 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:29:03.797 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.148008999997785ms 2022-08-03 18:29:04.778 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4469219999995175ms 2022-08-03 18:29:04.800 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2399 - accuracy: 0.9317 2022-08-03 18:29:09.112 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.9448 epoch:2 | accuracy:0.9448000192642212 | loss:0.18558283150196075 2022-08-03 18:29:09.686 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.23433993756771088, 0.18558283150196075]) label: LT, points: ([1], [0.23433993756771088]) label: FT, points: ([1, 2], [0.9348000288009644, 0.9448000192642212]) label: LT, points: ([1], [0.9348000288009644]) 2022-08-03 18:29:10.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.3958279999997103ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Tensorflow .","title":"Quickstart Keras"},{"location":"quick_start/quickstart_keras/#quickstart-keras","text":"In this tutorial, we will describe how to use Ifleaner in the Keras framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use Keras for image classification tasks on MNIST data, we need to go ahead and install the Keras libraries: ``` shell pip install keras == 2 .9.0 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training","title":"Quickstart (Keras)"},{"location":"quick_start/quickstart_keras/#ifleaner-client","text":"Create a new file named quickstart_keras.py and do the following.","title":"Ifleaner Client"},{"location":"quick_start/quickstart_keras/#1-define-model-network","text":"Firstly, you need define your model network by using Keras. from typing import Any , Tuple from keras.layers import Activation , Dense from keras.models import Sequential from keras.optimizers import RMSprop from keras.utils import np_utils # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 )","title":"1. Define Model Network"},{"location":"quick_start/quickstart_keras/#2-implement-trainer-class","text":"Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.keras_trainer.KerasTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: class Mnist ( KerasTrainer ): def __init__ ( self ): # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 ) self . _model = model super ( Mnist , self ) . __init__ ( model = model ) ( x_train , y_train ), ( x_test , y_test ) = self . _load_data () self . _x_train = x_train self . _y_train = y_train self . _x_test = x_test self . _y_test = y_test @staticmethod def _load_data () -> Dataset : # \u4e0b\u8f7dMNIST\u6570\u636e # X shape(60000, 28*28) y shape(10000, ) ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # \u6570\u636e\u9884\u5904\u7406 x_train = x_train . reshape ( x_train . shape [ 0 ], - 1 ) / 255 # normalize x_test = x_test . reshape ( x_test . shape [ 0 ], - 1 ) / 255 # normalize # \u5c06\u7c7b\u5411\u91cf\u8f6c\u5316\u4e3a\u7c7b\u77e9\u9635 \u6570\u5b57 5 \u8f6c\u6362\u4e3a 0 0 0 0 0 1 0 0 0 0 \u77e9\u9635 y_train = np_utils . to_categorical ( y_train , num_classes = 10 ) y_test = np_utils . to_categorical ( y_test , num_classes = 10 ) return ( x_train , y_train ), ( x_test , y_test ) def fit ( self , epoch : int ): self . _model . fit ( self . _x_train , self . _y_train , epochs = 1 , batch_size = 32 ) def evaluate ( self , epoch : int ) -> dict : loss , accuracy = self . _model . evaluate ( self . _x_test , self . _y_test ) print ( f \"epoch: { epoch } | accuracy: { accuracy } | loss: { loss } \" ) return { \"loss\" : loss , \"accuracy\" : accuracy }","title":"2. Implement Trainer Class"},{"location":"quick_start/quickstart_keras/#3-start-ifleaner-client","text":"Lastly, you need to write a main function to start your client. You can do it as the follow: if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_keras.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_keras.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms (iflearner) yiyezhiqiu:quickstart_mxnet lucky$ cd ../quickstart_keras/ (iflearner) yiyezhiqiu:quickstart_keras lucky$ python quickstart_keras.py --name client01 --epochs 2 Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:28:25.569565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(RMSprop, self).__init__(name, **kwargs) 2022-08-03 18:28:27.137 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:28:27.384 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 246.81410300000016ms 2022-08-03 18:28:27.385 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:28:27.386 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:28:27.391 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 4.523920999998765ms 2022-08-03 18:28:54.529 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4368589999946835ms 2022-08-03 18:28:55.466 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 6s 2ms/step - loss: 0.3668 - accuracy: 0.8968 2022-08-03 18:29:01.852 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 1s 2ms/step - loss: 0.2343 - accuracy: 0.9348 epoch:1 | accuracy:0.9348000288009644 | loss:0.23433993756771088 2022-08-03 18:29:02.782 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:29:02.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 4.293857000000401ms 2022-08-03 18:29:03.773 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 13.262433999997825ms 2022-08-03 18:29:03.795 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:29:03.797 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.148008999997785ms 2022-08-03 18:29:04.778 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4469219999995175ms 2022-08-03 18:29:04.800 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2399 - accuracy: 0.9317 2022-08-03 18:29:09.112 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.9448 epoch:2 | accuracy:0.9448000192642212 | loss:0.18558283150196075 2022-08-03 18:29:09.686 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.23433993756771088, 0.18558283150196075]) label: LT, points: ([1], [0.23433993756771088]) label: FT, points: ([1, 2], [0.9348000288009644, 0.9448000192642212]) label: LT, points: ([1], [0.9348000288009644]) 2022-08-03 18:29:10.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.3958279999997103ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Tensorflow .","title":"3. Start Ifleaner Client"},{"location":"quick_start/quickstart_mxnet/","text":"Quickstart (Mxnet) \u00b6 In this tutorial, we will describe how to use Ifleaner in the Mxnet framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use Mxnet for image classification tasks on MNIST data, we need to go ahead and install the Mxnet libraries: ``` shell pip install mxnet == 1 .9.1 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training Ifleaner Client \u00b6 Create a new file named quickstart_mxnet.py and do the following. 1. Define Model Network \u00b6 Firstly, you need define your model network by using Mxnet. import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn def model (): net = nn . Sequential () net . add ( nn . Dense ( 256 , activation = \"relu\" )) net . add ( nn . Dense ( 64 , activation = \"relu\" )) net . add ( nn . Dense ( 10 )) net . collect_params () . initialize () return net 2. Implement Trainer Class \u00b6 Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.mxnet_trainer.MxnetTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: from typing import Any , Tuple import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn from iflearner.business.homo.argument import parser from iflearner.business.homo.mxnet_trainer import MxnetTrainer from iflearner.business.homo.train_client import Controller class Mnist ( MxnetTrainer ): def __init__ ( self ): self . _model = model () init = nd . random . uniform ( shape = ( 2 , 784 )) self . _model ( init ) super () . __init__ ( model = self . _model ) self . _train_data , self . _val_data = self . _load_data () self . _DEVICE = [ mx . gpu () if mx . test_utils . list_gpus () else mx . cpu ()] @staticmethod def _load_data () -> Tuple [ Any , Any ]: print ( \"Download Dataset\" ) mnist = mx . test_utils . get_mnist () batch_size = 100 train_data = mx . io . NDArrayIter ( mnist [ \"train_data\" ], mnist [ \"train_label\" ], batch_size , shuffle = True ) val_data = mx . io . NDArrayIter ( mnist [ \"test_data\" ], mnist [ \"test_label\" ], batch_size ) return train_data , val_data def fit ( self , epoch : int ): trainer = gluon . Trainer ( self . _model . collect_params (), \"sgd\" , { \"learning_rate\" : 0.01 } ) accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) softmax_cross_entropy_loss = gluon . loss . SoftmaxCrossEntropyLoss () self . _train_data . reset () num_examples = 0 for batch in self . _train_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] with ag . record (): for x , y in zip ( data , label ): z = self . _model ( x ) loss = softmax_cross_entropy_loss ( z , y ) loss . backward () outputs . append ( z . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) trainer . step ( batch . data [ 0 ] . shape [ 0 ]) trainings_metric = metrics . get_name_value () [ accuracy , loss ] = trainings_metric print ( f \"epoch : { epoch } : accuracy: { float ( accuracy [ 1 ]) } loss\uff1a { float ( loss [ 1 ]) } \" ) def evaluate ( self , epoch : int ) -> dict : accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) self . _val_data . reset () num_examples = 0 for batch in self . _val_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] for x in data : outputs . append ( self . _model ( x ) . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) metrics . update ( label , outputs ) [ accuracy , loss ] = metrics . get_name_value () print ( f \"Evaluation accuracy: { accuracy } loss: { loss } \" ) return { \"loss\" : float ( loss [ 1 ]), \"accuracy\" : float ( accuracy [ 1 ])} 3. tart Ifleaner Client \u00b6 Lastly, you need to write a main function to start your client. You can do it as the follow: from iflearner.business.homo.argument import parser if __name__ == \"__main__\" : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_mxnet.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_mxnet.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Mxnet .","title":"Quickstart Mxnet"},{"location":"quick_start/quickstart_mxnet/#quickstart-mxnet","text":"In this tutorial, we will describe how to use Ifleaner in the Mxnet framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use Mxnet for image classification tasks on MNIST data, we need to go ahead and install the Mxnet libraries: ``` shell pip install mxnet == 1 .9.1 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training","title":"Quickstart (Mxnet)"},{"location":"quick_start/quickstart_mxnet/#ifleaner-client","text":"Create a new file named quickstart_mxnet.py and do the following.","title":"Ifleaner Client"},{"location":"quick_start/quickstart_mxnet/#1-define-model-network","text":"Firstly, you need define your model network by using Mxnet. import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn def model (): net = nn . Sequential () net . add ( nn . Dense ( 256 , activation = \"relu\" )) net . add ( nn . Dense ( 64 , activation = \"relu\" )) net . add ( nn . Dense ( 10 )) net . collect_params () . initialize () return net","title":"1. Define Model Network"},{"location":"quick_start/quickstart_mxnet/#2-implement-trainer-class","text":"Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.mxnet_trainer.MxnetTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: from typing import Any , Tuple import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn from iflearner.business.homo.argument import parser from iflearner.business.homo.mxnet_trainer import MxnetTrainer from iflearner.business.homo.train_client import Controller class Mnist ( MxnetTrainer ): def __init__ ( self ): self . _model = model () init = nd . random . uniform ( shape = ( 2 , 784 )) self . _model ( init ) super () . __init__ ( model = self . _model ) self . _train_data , self . _val_data = self . _load_data () self . _DEVICE = [ mx . gpu () if mx . test_utils . list_gpus () else mx . cpu ()] @staticmethod def _load_data () -> Tuple [ Any , Any ]: print ( \"Download Dataset\" ) mnist = mx . test_utils . get_mnist () batch_size = 100 train_data = mx . io . NDArrayIter ( mnist [ \"train_data\" ], mnist [ \"train_label\" ], batch_size , shuffle = True ) val_data = mx . io . NDArrayIter ( mnist [ \"test_data\" ], mnist [ \"test_label\" ], batch_size ) return train_data , val_data def fit ( self , epoch : int ): trainer = gluon . Trainer ( self . _model . collect_params (), \"sgd\" , { \"learning_rate\" : 0.01 } ) accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) softmax_cross_entropy_loss = gluon . loss . SoftmaxCrossEntropyLoss () self . _train_data . reset () num_examples = 0 for batch in self . _train_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] with ag . record (): for x , y in zip ( data , label ): z = self . _model ( x ) loss = softmax_cross_entropy_loss ( z , y ) loss . backward () outputs . append ( z . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) trainer . step ( batch . data [ 0 ] . shape [ 0 ]) trainings_metric = metrics . get_name_value () [ accuracy , loss ] = trainings_metric print ( f \"epoch : { epoch } : accuracy: { float ( accuracy [ 1 ]) } loss\uff1a { float ( loss [ 1 ]) } \" ) def evaluate ( self , epoch : int ) -> dict : accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) self . _val_data . reset () num_examples = 0 for batch in self . _val_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] for x in data : outputs . append ( self . _model ( x ) . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) metrics . update ( label , outputs ) [ accuracy , loss ] = metrics . get_name_value () print ( f \"Evaluation accuracy: { accuracy } loss: { loss } \" ) return { \"loss\" : float ( loss [ 1 ]), \"accuracy\" : float ( accuracy [ 1 ])}","title":"2. Implement Trainer Class"},{"location":"quick_start/quickstart_mxnet/#3-tart-ifleaner-client","text":"Lastly, you need to write a main function to start your client. You can do it as the follow: from iflearner.business.homo.argument import parser if __name__ == \"__main__\" : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_mxnet.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_mxnet.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Mxnet .","title":"3. tart Ifleaner Client"},{"location":"quick_start/quickstart_opacus/","text":"Quickstart (Opacus) \u00b6 In this tutorial , we will help you understand how to run federated tasks under pytorch using Opacus library combined with differential privacy encryption technology our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use PyTorch for image classification tasks on MNIST data, we need to go ahead and install the Opacus\u3001PyTorch and torchvision libraries: ``` shell pip install opacus == 1 .1.3 torch == 1 .8.1 torchvision == 0 .9.1 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training Ifleaner Client \u00b6 Create a new file named quickstart_pytorch.py and do the following. 1. Define Model Network \u00b6 Firstly, you need define your model network by using PyTorch. from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) 2. Implement Trainer Class \u00b6 Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.pytorch_trainer.PyTorchTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. At the same time, we also integrated the Opacus differential privacy library. You can use this class as the follow: class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 , delta = 1e-5 ) -> None : self . _lr = lr self . _delta = delta self . _device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) print ( f \"device: { self . _device } \" ) model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( model ) optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))] ) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) self . _privacy_engine = PrivacyEngine () self . _model , self . _optimizer , self . _train_data = self . _privacy_engine . make_private ( module = model , optimizer = optimizer , data_loader = train_data , noise_multiplier = 1.1 , max_grad_norm = 1.0 , ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) losses = [] for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () losses . append ( loss . item ()) epsilon , best_alpha = self . _privacy_engine . accountant . get_privacy_spent ( delta = self . _delta ) print ( f \"Train Epoch: { epoch } \\t \" f \"Loss: { np . mean ( losses ) : .6f } \" f \"(\u03b5 = { epsilon : .2f } , \u03b4 = { self . _delta } ) for \u03b1 = { best_alpha } \" ) def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = \"sum\" ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( \"Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)\" . format ( test_loss , correct , len ( self . _test_data . dataset ), 100.0 * correct / len ( self . _test_data . dataset ), ) ) return { \"loss\" : test_loss , \"acc\" : correct / len ( self . _test_data . dataset )} ``` #### 3. Start Ifleaner Client Lastly , you need to write a ` main ` function to start your client . You can do it as the follow : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_pytorch.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_pytorch.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on. warnings.warn( 2022-08-08 20:54:51.294 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 20:54:51.308 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 13.392697000000009ms 2022-08-08 20:54:51.308 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 20:54:51.309 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 20:54:51.311 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5273650000002803ms 2022-08-08 20:54:53.322 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.9921759999999011ms 2022-08-08 20:54:54.325 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" Train Epoch: 1 Loss: 1.963445 (\u03b5 = 0.53, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:55:49.100 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6190, Accuracy: 7907/10000 (79.07%) 2022-08-08 20:55:51.779 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 20:55:51.785 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.787130000003856ms 2022-08-08 20:55:52.656 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.798496000001933ms 2022-08-08 20:55:52.789 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 20:55:52.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5915189999944346ms 2022-08-08 20:55:53.659 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.2224549999947385ms 2022-08-08 20:55:53.799 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 Train Epoch: 2 Loss: 1.975834 (\u03b5 = 0.55, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:56:41.185 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6178, Accuracy: 8213/10000 (82.13%) 2022-08-08 20:56:44.589 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.6190427913188934, 0.617782280254364]) label: LT, points: ([1], [0.6190427913188934]) label: FT, points: ([1, 2], [0.7907, 0.8213]) label: LT, points: ([1], [0.7907]) 2022-08-08 20:56:45.487 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.8401949999997669ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Pytorch .","title":"Quickstart Opacus"},{"location":"quick_start/quickstart_opacus/#quickstart-opacus","text":"In this tutorial , we will help you understand how to run federated tasks under pytorch using Opacus library combined with differential privacy encryption technology our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use PyTorch for image classification tasks on MNIST data, we need to go ahead and install the Opacus\u3001PyTorch and torchvision libraries: ``` shell pip install opacus == 1 .1.3 torch == 1 .8.1 torchvision == 0 .9.1 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training","title":"Quickstart (Opacus)"},{"location":"quick_start/quickstart_opacus/#ifleaner-client","text":"Create a new file named quickstart_pytorch.py and do the following.","title":"Ifleaner Client"},{"location":"quick_start/quickstart_opacus/#1-define-model-network","text":"Firstly, you need define your model network by using PyTorch. from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 )","title":"1. Define Model Network"},{"location":"quick_start/quickstart_opacus/#2-implement-trainer-class","text":"Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.pytorch_trainer.PyTorchTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. At the same time, we also integrated the Opacus differential privacy library. You can use this class as the follow: class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 , delta = 1e-5 ) -> None : self . _lr = lr self . _delta = delta self . _device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) print ( f \"device: { self . _device } \" ) model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( model ) optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))] ) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) self . _privacy_engine = PrivacyEngine () self . _model , self . _optimizer , self . _train_data = self . _privacy_engine . make_private ( module = model , optimizer = optimizer , data_loader = train_data , noise_multiplier = 1.1 , max_grad_norm = 1.0 , ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) losses = [] for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () losses . append ( loss . item ()) epsilon , best_alpha = self . _privacy_engine . accountant . get_privacy_spent ( delta = self . _delta ) print ( f \"Train Epoch: { epoch } \\t \" f \"Loss: { np . mean ( losses ) : .6f } \" f \"(\u03b5 = { epsilon : .2f } , \u03b4 = { self . _delta } ) for \u03b1 = { best_alpha } \" ) def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = \"sum\" ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( \"Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)\" . format ( test_loss , correct , len ( self . _test_data . dataset ), 100.0 * correct / len ( self . _test_data . dataset ), ) ) return { \"loss\" : test_loss , \"acc\" : correct / len ( self . _test_data . dataset )} ``` #### 3. Start Ifleaner Client Lastly , you need to write a ` main ` function to start your client . You can do it as the follow : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_pytorch.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_pytorch.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on. warnings.warn( 2022-08-08 20:54:51.294 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 20:54:51.308 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 13.392697000000009ms 2022-08-08 20:54:51.308 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 20:54:51.309 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 20:54:51.311 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5273650000002803ms 2022-08-08 20:54:53.322 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.9921759999999011ms 2022-08-08 20:54:54.325 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" Train Epoch: 1 Loss: 1.963445 (\u03b5 = 0.53, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:55:49.100 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6190, Accuracy: 7907/10000 (79.07%) 2022-08-08 20:55:51.779 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 20:55:51.785 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.787130000003856ms 2022-08-08 20:55:52.656 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.798496000001933ms 2022-08-08 20:55:52.789 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 20:55:52.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5915189999944346ms 2022-08-08 20:55:53.659 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.2224549999947385ms 2022-08-08 20:55:53.799 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 Train Epoch: 2 Loss: 1.975834 (\u03b5 = 0.55, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:56:41.185 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6178, Accuracy: 8213/10000 (82.13%) 2022-08-08 20:56:44.589 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.6190427913188934, 0.617782280254364]) label: LT, points: ([1], [0.6190427913188934]) label: FT, points: ([1, 2], [0.7907, 0.8213]) label: LT, points: ([1], [0.7907]) 2022-08-08 20:56:45.487 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.8401949999997669ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Pytorch .","title":"2. Implement Trainer Class"},{"location":"quick_start/quickstart_pytorch/","text":"Quickstart (PyTorch) \u00b6 In this tutorial, we will describe how to use Ifleaner in the PyTorch framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use PyTorch for image classification tasks on MNIST data, we need to go ahead and install the PyTorch and torchvision libraries: ``` shell pip install torch == 1 .7.1 torchvision == 0 .8.2 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training Ifleaner Client \u00b6 Create a new file named quickstart_pytorch.py and do the following. 1. Define Model Network \u00b6 Firstly, you need define your model network by using PyTorch. from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) 2. Implement Trainer Class \u00b6 Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.pytorch_trainer.PyTorchTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. Start Ifleaner Client Lastly , you need to write a ` main ` function to start your client . You can do it as the follow : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_pytorch.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_pytorch.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu 2022-08-03 17:33:49.148 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 17:33:49.165 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 16.620276000000047ms 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 17:33:49.170 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.700055999999895ms 2022-08-03 17:33:54.192 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.3457160000003299ms 2022-08-03 17:33:55.188 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-03 17:34:43.583 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1023, Accuracy: 9696/10000 (96.96%) 2022-08-03 17:34:48.140 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 17:34:48.354 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 210.4355039999959ms 2022-08-03 17:34:48.426 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 15.4244869999971ms 2022-08-03 17:34:49.359 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 17:34:49.362 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.6616899999988277ms 2022-08-03 17:34:50.437 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.947831000002509ms 2022-08-03 17:34:51.367 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-03 17:35:38.518 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0833, Accuracy: 9758/10000 (97.58%) 2022-08-03 17:35:43.808 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.10231972066191956, 0.08325759547855704]) label: LT, points: ([1], [0.10231972066191956]) label: FT, points: ([1, 2], [0.9696, 0.9758]) label: LT, points: ([1], [0.9696]) 2022-08-03 17:35:44.596 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.4475409999903377ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Pytorch .","title":"Quickstart Pytorch"},{"location":"quick_start/quickstart_pytorch/#quickstart-pytorch","text":"In this tutorial, we will describe how to use Ifleaner in the PyTorch framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use PyTorch for image classification tasks on MNIST data, we need to go ahead and install the PyTorch and torchvision libraries: ``` shell pip install torch == 1 .7.1 torchvision == 0 .8.2 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training","title":"Quickstart (PyTorch)"},{"location":"quick_start/quickstart_pytorch/#ifleaner-client","text":"Create a new file named quickstart_pytorch.py and do the following.","title":"Ifleaner Client"},{"location":"quick_start/quickstart_pytorch/#1-define-model-network","text":"Firstly, you need define your model network by using PyTorch. from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 )","title":"1. Define Model Network"},{"location":"quick_start/quickstart_pytorch/#2-implement-trainer-class","text":"Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.pytorch_trainer.PyTorchTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. Start Ifleaner Client Lastly , you need to write a ` main ` function to start your client . You can do it as the follow : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_pytorch.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_pytorch.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu 2022-08-03 17:33:49.148 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 17:33:49.165 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 16.620276000000047ms 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 17:33:49.170 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.700055999999895ms 2022-08-03 17:33:54.192 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.3457160000003299ms 2022-08-03 17:33:55.188 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-03 17:34:43.583 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1023, Accuracy: 9696/10000 (96.96%) 2022-08-03 17:34:48.140 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 17:34:48.354 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 210.4355039999959ms 2022-08-03 17:34:48.426 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 15.4244869999971ms 2022-08-03 17:34:49.359 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 17:34:49.362 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.6616899999988277ms 2022-08-03 17:34:50.437 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.947831000002509ms 2022-08-03 17:34:51.367 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-03 17:35:38.518 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0833, Accuracy: 9758/10000 (97.58%) 2022-08-03 17:35:43.808 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.10231972066191956, 0.08325759547855704]) label: LT, points: ([1], [0.10231972066191956]) label: FT, points: ([1, 2], [0.9696, 0.9758]) label: LT, points: ([1], [0.9696]) 2022-08-03 17:35:44.596 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.4475409999903377ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Pytorch .","title":"2. Implement Trainer Class"},{"location":"quick_start/quickstart_smpc/","text":"Quickstart (SMPC) \u00b6 In this tutorial, we will describe how to use IFLeaner to complete image classification federated training under the MNIST dataset with SMPC. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use PyTorch for image classification tasks on MNIST data, we need to go ahead and install the PyTorch and torchvision libraries: ``` shell pip install torch == 1 .7.1 torchvision == 0 .8.2 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training Ifleaner Client \u00b6 Create a new file named quickstart_pytorch.py and do the following. 1. Define Model Network \u00b6 Firstly, you need define your model network by using PyTorch. from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) 2. Implement Trainer Class \u00b6 Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.pytorch_trainer.PyTorchTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. Start Ifleaner Client Lastly , you need to write a ` main ` function to start your client . You can do it as the follow : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_pytorch.py --name client01 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50012;0.0.0.0:50013\" Configure peers to use smpc, peers are configured as the listening address of all clients, and the first address is the listening address of the client Open another terminal and start the second client: python quickstart_pytorch.py --name client02 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50013;0.0.0.0:50012\" Configure peers to use smpc, peers are configured as the listening address of all clients, and the first address is the listening address of the client After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client1', epochs=10, server='0.0.0.0:50001', enable_ll=0, peers='0.0.0.0:50012;0.0.0.0:50013', cert=None) device: cpu 2022-08-08 19:39:37.971 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 19:39:37.976 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 4.347216000000209ms 2022-08-08 19:39:37.977 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 19:39:37.980 | INFO | iflearner.communication.peer.peer_client:get_DH_public_key:43 - Public key: b'\\x01\\x01\\xd0\\xf2\\xa5\\xc0-\\x7f\\x1b\\x88\\xcb\\xc8\\\\\\x91Ra,4\\n\\xd4]\\x97\\x99zs\\xae7\\x1cK]]\\x0c\\x06\\x85\\xa1\\xb5\\x82\\x03.\\x9a\\xe0m\\xa3>#\\xf7(\\xb3x\\x89m\\xfa\\xfbu\\x9ca\\x95\\xf4\\x80GA\\xd8z\\x8fKs\\xe0\\x98\\xe3\\x7fX.\\xe2Ej\\x04c\\x08\\xcf\\xdeF\\'\\xcc(\"@q[\\xa5\\xdf\\xb4#\\x1c\\xd6\\xd8\\xd1\\x05?\\x06tO\\xfa~Z\\x12\\x14\\x1e\\xba\\xbe\\xaa\\xe5/}\\xb1Y\\xde]\\xd8\\\\\\x17\\x9cE\\xf3Z\\xae(\\xbfDsf' 2022-08-08 19:39:37.983 | INFO | iflearner.business.homo.train_client:do_smpc:73 - secret: 122099796455175621216112096188958830464477667871351715488133066776583905683428683953037290811910449486061004359077608812182806437003480898524406977052511968565063977753455848242565116696939311359584774021461025748485006418803112297959930199282888061745382056940467943791248215701671956530776589875636959329985, type: <class 'str'> 2022-08-08 19:39:37.988 | INFO | iflearner.communication.peer.peer_client:get_SMPC_random_key:56 - Random float: 0.6339090663897411 2022-08-08 19:39:37.988 | INFO | iflearner.business.homo.train_client:do_smpc:77 - random value: 0.6339090663897411 2022-08-08 19:39:48.296 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_dh_public_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_smpc_random_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:56 - Party: client2, Random float: 0.3922334767649399 2022-08-08 19:39:49.023 | INFO | iflearner.business.homo.train_client:do_smpc:80 - sum all random values: 0.24167558962480118 2022-08-08 19:39:49.025 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 19:39:49.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.0064270000013096ms 2022-08-08 19:39:50.030 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.7326500000010014ms 2022-08-08 19:39:51.033 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-08 19:40:20.664 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1058, Accuracy: 9694/10000 (96.94%) 2022-08-08 19:40:23.830 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:40:23.858 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.50900600000449ms 2022-08-08 19:40:24.168 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 10.504099000002043ms 2022-08-08 19:40:24.862 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:40:24.865 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2220939999991742ms 2022-08-08 19:40:25.173 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.2624359999975354ms 2022-08-08 19:40:25.871 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:00.230 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0998, Accuracy: 9726/10000 (97.26%) 2022-08-08 19:41:03.992 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:04.020 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.794709000000353ms 2022-08-08 19:41:04.367 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.483094000003689ms 2022-08-08 19:41:05.024 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:05.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.9964000000101123ms 2022-08-08 19:41:05.374 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6533630000026278ms 2022-08-08 19:41:06.032 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 3, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:33.934 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0871, Accuracy: 9743/10000 (97.43%) 2022-08-08 19:41:37.425 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:37.492 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 18.108728000001406ms 2022-08-08 19:41:37.514 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.546628999994255ms 2022-08-08 19:41:38.496 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:38.498 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2796400000070207ms 2022-08-08 19:41:38.519 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 0.9813249999979234ms 2022-08-08 19:41:39.503 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 4, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:12.085 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0989, Accuracy: 9705/10000 (97.05%) 2022-08-08 19:42:15.499 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:15.513 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.581355999978996ms 2022-08-08 19:42:15.694 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.077353999993875ms 2022-08-08 19:42:16.517 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:16.519 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3156910000020616ms 2022-08-08 19:42:16.701 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.0472559999973328ms 2022-08-08 19:42:17.524 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 5, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:53.300 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0639, Accuracy: 9806/10000 (98.06%) 2022-08-08 19:42:56.654 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:56.667 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.813790000028348ms 2022-08-08 19:42:56.892 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.262933999982124ms 2022-08-08 19:42:57.672 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:57.675 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5553380000028483ms 2022-08-08 19:42:57.898 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6520599999978458ms 2022-08-08 19:42:58.679 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 6, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:26.257 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0753, Accuracy: 9787/10000 (97.87%) 2022-08-08 19:43:30.128 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:43:30.143 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.0744939999985945ms 2022-08-08 19:43:31.048 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.83907899998826ms 2022-08-08 19:43:31.148 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:43:31.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.054303000022628ms 2022-08-08 19:43:32.055 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.705947000004926ms 2022-08-08 19:43:32.153 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 7, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:58.396 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0736, Accuracy: 9768/10000 (97.68%) 2022-08-08 19:44:01.113 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:01.125 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.890878999972756ms 2022-08-08 19:44:02.184 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.515048000025672ms 2022-08-08 19:44:03.132 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:03.135 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.91251899997269ms 2022-08-08 19:44:03.188 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6274970000154099ms 2022-08-08 19:44:04.140 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 8, the size of training dataset: 60000, batch size: 938 2022-08-08 19:44:34.161 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0646, Accuracy: 9801/10000 (98.01%) 2022-08-08 19:44:37.132 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:37.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.143503999993754ms 2022-08-08 19:44:37.328 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.2212239999771555ms 2022-08-08 19:44:38.153 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:38.156 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.8954930000063541ms 2022-08-08 19:44:38.335 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.6670950000493576ms 2022-08-08 19:44:39.161 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 9, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:04.166 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0694, Accuracy: 9781/10000 (97.81%) 2022-08-08 19:45:06.821 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:45:06.841 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.79627700000674ms 2022-08-08 19:45:07.453 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.497581000005084ms 2022-08-08 19:45:07.846 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:45:07.848 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.1865850000276623ms 2022-08-08 19:45:09.465 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6427019999696313ms 2022-08-08 19:45:09.858 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 10, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:48.696 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0615, Accuracy: 9814/10000 (98.14%) 2022-08-08 19:45:53.514 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.10584523560330272, 0.09984834497272968, 0.0871084996862337, 0.09891319364532829, 0.063905619766374, 0.07528823107918725, 0.07361029261836957, 0.06460160582875542, 0.0694242621988058, 0.06149101790403947]) label: LT, points: ([1], [0.10584523560330272]) label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.9694, 0.9726, 0.9743, 0.9705, 0.9806, 0.9787, 0.9768, 0.9801, 0.9781, 0.9814]) label: LT, points: ([1], [0.9694]) 2022-08-08 19:45:54.253 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 2.060518000007505ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_SMPC .","title":"Quickstart SMPC"},{"location":"quick_start/quickstart_smpc/#quickstart-smpc","text":"In this tutorial, we will describe how to use IFLeaner to complete image classification federated training under the MNIST dataset with SMPC. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use PyTorch for image classification tasks on MNIST data, we need to go ahead and install the PyTorch and torchvision libraries: ``` shell pip install torch == 1 .7.1 torchvision == 0 .8.2 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training","title":"Quickstart (SMPC)"},{"location":"quick_start/quickstart_smpc/#ifleaner-client","text":"Create a new file named quickstart_pytorch.py and do the following.","title":"Ifleaner Client"},{"location":"quick_start/quickstart_smpc/#1-define-model-network","text":"Firstly, you need define your model network by using PyTorch. from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 )","title":"1. Define Model Network"},{"location":"quick_start/quickstart_smpc/#2-implement-trainer-class","text":"Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.pytorch_trainer.PyTorchTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. Start Ifleaner Client Lastly , you need to write a ` main ` function to start your client . You can do it as the follow : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_pytorch.py --name client01 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50012;0.0.0.0:50013\" Configure peers to use smpc, peers are configured as the listening address of all clients, and the first address is the listening address of the client Open another terminal and start the second client: python quickstart_pytorch.py --name client02 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50013;0.0.0.0:50012\" Configure peers to use smpc, peers are configured as the listening address of all clients, and the first address is the listening address of the client After both clients are ready and started, we can see log messages similar to the following on either client terminal: Namespace(name='client1', epochs=10, server='0.0.0.0:50001', enable_ll=0, peers='0.0.0.0:50012;0.0.0.0:50013', cert=None) device: cpu 2022-08-08 19:39:37.971 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 19:39:37.976 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 4.347216000000209ms 2022-08-08 19:39:37.977 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 19:39:37.980 | INFO | iflearner.communication.peer.peer_client:get_DH_public_key:43 - Public key: b'\\x01\\x01\\xd0\\xf2\\xa5\\xc0-\\x7f\\x1b\\x88\\xcb\\xc8\\\\\\x91Ra,4\\n\\xd4]\\x97\\x99zs\\xae7\\x1cK]]\\x0c\\x06\\x85\\xa1\\xb5\\x82\\x03.\\x9a\\xe0m\\xa3>#\\xf7(\\xb3x\\x89m\\xfa\\xfbu\\x9ca\\x95\\xf4\\x80GA\\xd8z\\x8fKs\\xe0\\x98\\xe3\\x7fX.\\xe2Ej\\x04c\\x08\\xcf\\xdeF\\'\\xcc(\"@q[\\xa5\\xdf\\xb4#\\x1c\\xd6\\xd8\\xd1\\x05?\\x06tO\\xfa~Z\\x12\\x14\\x1e\\xba\\xbe\\xaa\\xe5/}\\xb1Y\\xde]\\xd8\\\\\\x17\\x9cE\\xf3Z\\xae(\\xbfDsf' 2022-08-08 19:39:37.983 | INFO | iflearner.business.homo.train_client:do_smpc:73 - secret: 122099796455175621216112096188958830464477667871351715488133066776583905683428683953037290811910449486061004359077608812182806437003480898524406977052511968565063977753455848242565116696939311359584774021461025748485006418803112297959930199282888061745382056940467943791248215701671956530776589875636959329985, type: <class 'str'> 2022-08-08 19:39:37.988 | INFO | iflearner.communication.peer.peer_client:get_SMPC_random_key:56 - Random float: 0.6339090663897411 2022-08-08 19:39:37.988 | INFO | iflearner.business.homo.train_client:do_smpc:77 - random value: 0.6339090663897411 2022-08-08 19:39:48.296 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_dh_public_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_smpc_random_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:56 - Party: client2, Random float: 0.3922334767649399 2022-08-08 19:39:49.023 | INFO | iflearner.business.homo.train_client:do_smpc:80 - sum all random values: 0.24167558962480118 2022-08-08 19:39:49.025 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 19:39:49.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.0064270000013096ms 2022-08-08 19:39:50.030 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.7326500000010014ms 2022-08-08 19:39:51.033 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-08 19:40:20.664 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1058, Accuracy: 9694/10000 (96.94%) 2022-08-08 19:40:23.830 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:40:23.858 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.50900600000449ms 2022-08-08 19:40:24.168 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 10.504099000002043ms 2022-08-08 19:40:24.862 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:40:24.865 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2220939999991742ms 2022-08-08 19:40:25.173 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.2624359999975354ms 2022-08-08 19:40:25.871 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:00.230 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0998, Accuracy: 9726/10000 (97.26%) 2022-08-08 19:41:03.992 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:04.020 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.794709000000353ms 2022-08-08 19:41:04.367 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.483094000003689ms 2022-08-08 19:41:05.024 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:05.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.9964000000101123ms 2022-08-08 19:41:05.374 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6533630000026278ms 2022-08-08 19:41:06.032 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 3, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:33.934 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0871, Accuracy: 9743/10000 (97.43%) 2022-08-08 19:41:37.425 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:37.492 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 18.108728000001406ms 2022-08-08 19:41:37.514 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.546628999994255ms 2022-08-08 19:41:38.496 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:38.498 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2796400000070207ms 2022-08-08 19:41:38.519 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 0.9813249999979234ms 2022-08-08 19:41:39.503 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 4, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:12.085 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0989, Accuracy: 9705/10000 (97.05%) 2022-08-08 19:42:15.499 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:15.513 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.581355999978996ms 2022-08-08 19:42:15.694 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.077353999993875ms 2022-08-08 19:42:16.517 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:16.519 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3156910000020616ms 2022-08-08 19:42:16.701 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.0472559999973328ms 2022-08-08 19:42:17.524 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 5, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:53.300 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0639, Accuracy: 9806/10000 (98.06%) 2022-08-08 19:42:56.654 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:56.667 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.813790000028348ms 2022-08-08 19:42:56.892 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.262933999982124ms 2022-08-08 19:42:57.672 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:57.675 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5553380000028483ms 2022-08-08 19:42:57.898 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6520599999978458ms 2022-08-08 19:42:58.679 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 6, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:26.257 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0753, Accuracy: 9787/10000 (97.87%) 2022-08-08 19:43:30.128 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:43:30.143 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.0744939999985945ms 2022-08-08 19:43:31.048 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.83907899998826ms 2022-08-08 19:43:31.148 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:43:31.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.054303000022628ms 2022-08-08 19:43:32.055 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.705947000004926ms 2022-08-08 19:43:32.153 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 7, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:58.396 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0736, Accuracy: 9768/10000 (97.68%) 2022-08-08 19:44:01.113 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:01.125 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.890878999972756ms 2022-08-08 19:44:02.184 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.515048000025672ms 2022-08-08 19:44:03.132 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:03.135 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.91251899997269ms 2022-08-08 19:44:03.188 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6274970000154099ms 2022-08-08 19:44:04.140 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 8, the size of training dataset: 60000, batch size: 938 2022-08-08 19:44:34.161 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0646, Accuracy: 9801/10000 (98.01%) 2022-08-08 19:44:37.132 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:37.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.143503999993754ms 2022-08-08 19:44:37.328 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.2212239999771555ms 2022-08-08 19:44:38.153 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:38.156 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.8954930000063541ms 2022-08-08 19:44:38.335 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.6670950000493576ms 2022-08-08 19:44:39.161 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 9, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:04.166 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0694, Accuracy: 9781/10000 (97.81%) 2022-08-08 19:45:06.821 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:45:06.841 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.79627700000674ms 2022-08-08 19:45:07.453 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.497581000005084ms 2022-08-08 19:45:07.846 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:45:07.848 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.1865850000276623ms 2022-08-08 19:45:09.465 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6427019999696313ms 2022-08-08 19:45:09.858 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 10, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:48.696 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0615, Accuracy: 9814/10000 (98.14%) 2022-08-08 19:45:53.514 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.10584523560330272, 0.09984834497272968, 0.0871084996862337, 0.09891319364532829, 0.063905619766374, 0.07528823107918725, 0.07361029261836957, 0.06460160582875542, 0.0694242621988058, 0.06149101790403947]) label: LT, points: ([1], [0.10584523560330272]) label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.9694, 0.9726, 0.9743, 0.9705, 0.9806, 0.9787, 0.9768, 0.9801, 0.9781, 0.9814]) label: LT, points: ([1], [0.9694]) 2022-08-08 19:45:54.253 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 2.060518000007505ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_SMPC .","title":"2. Implement Trainer Class"},{"location":"quick_start/quickstart_tensorflow/","text":"Quickstart (TensorFlow) \u00b6 In this tutorial, we will describe how to use Ifleaner in the Tensorflow framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use Tensorflow for image classification tasks on MNIST data, we need to go ahead and install the Tensorflow libraries: ``` shell pip install tensorflow == 2 .9.1 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training Ifleaner Client \u00b6 Create a new file named quickstart_tensorflow.py and do the following. 1. Define Model Network \u00b6 Firstly, you need define your model network by using TensorFlow. from tensorflow.keras.layers import Dense , Flatten , Conv2D from tensorflow.keras import Model class MyModel ( Model ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = Conv2D ( 32 , 3 , activation = 'relu' ) self . flatten = Flatten () self . d1 = Dense ( 128 , activation = 'relu' ) self . d2 = Dense ( 10 ) def call ( self , x ): x = self . conv1 ( x ) x = self . flatten ( x ) x = self . d1 ( x ) return self . d2 ( x ) 2. Implement Trainer Class \u00b6 Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: import tensorflow as tf from iflearner.business.homo.tensorflow_trainer import TensorFlowTrainer from iflearner.business.homo.train_client import Controller mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Add a channels dimension x_train = x_train [ ... , tf . newaxis ] . astype ( \"float32\" ) x_test = x_test [ ... , tf . newaxis ] . astype ( \"float32\" ) train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train )) . shuffle ( 10000 ) . batch ( 32 ) test_ds = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) . batch ( 32 ) loss_object = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) optimizer = tf . keras . optimizers . Adam () train_loss = tf . keras . metrics . Mean ( name = 'train_loss' ) train_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'train_accuracy' ) test_loss = tf . keras . metrics . Mean ( name = 'test_loss' ) test_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'test_accuracy' ) class Mnist ( TensorFlowTrainer ): def __init__ ( self , model ) -> None : super () . __init__ ( model ) train_loss . reset_states () train_accuracy . reset_states () test_loss . reset_states () test_accuracy . reset_states () def fit ( self , epoch ): for images , labels in train_ds : # train_step(images, labels) self . _fit ( images , labels ) @tf . function def _fit ( self , images , labels ): with tf . GradientTape () as tape : # training=True is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = True ) loss = loss_object ( labels , predictions ) gradients = tape . gradient ( loss , model . trainable_variables ) optimizer . apply_gradients ( zip ( gradients , model . trainable_variables )) train_loss ( loss ) train_accuracy ( labels , predictions ) def evaluate ( self , epoch ): for test_images , test_labels in test_ds : # test_step(test_images, test_labels) self . _evaluate ( test_images , test_labels ) print ( f 'Epoch { epoch } , ' f 'Loss: { train_loss . result () } , ' f 'Accuracy: { train_accuracy . result () * 100 } , ' f 'Test Loss: { test_loss . result () } , ' f 'Test Accuracy: { test_accuracy . result () * 100 } ' ) return { 'Accuracy' : train_accuracy . result () * 100 } @tf . function def _evaluate ( self , images , labels ): # training=False is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = False ) t_loss = loss_object ( labels , predictions ) test_loss ( t_loss ) 3. Start Ifleaner Client \u00b6 Lastly, you need to write a main function to start your client. You can do it as the follow: if __name__ == '__main__' : args = parser . parse_args () print ( args ) model = MyModel () mnist = Mnist ( model ) controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_tensorflow.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_tensorflow.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: 2022-08-03 18:07:07.406604: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:07:07.456 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:07:07.479 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 22.46353199999973ms 2022-08-03 18:07:07.479 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:07:07.480 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:07:07.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3502309999999795ms 2022-08-03 18:07:11.500 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.1013739999992112ms 2022-08-03 18:07:12.497 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:08:19.804 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 1, Loss: 0.1414850801229477, Accuracy: 95.73500061035156, Test Loss: 0.0603780597448349, Test Accuracy: 0.0 2022-08-03 18:08:22.759 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:08:24.130 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 149.0828160000035ms 2022-08-03 18:08:31.445 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 3241.557815999997ms 2022-08-03 18:08:32.446 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:08:32.474 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.617487000004303ms 2022-08-03 18:08:33.469 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 8.709660999997482ms 2022-08-03 18:08:33.479 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:09:46.374 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 2, Loss: 0.11132627725601196, Accuracy: 96.65583038330078, Test Loss: 0.0679144412279129, Test Accuracy: 0.0 2022-08-03 18:09:49.340 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>, <tf.Tensor: shape=(), dtype=float32, numpy=96.65583>]) label: LT, points: ([1], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>]) 2022-08-03 18:09:51.374 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.561914999996361ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Tensorflow .","title":"Quickstart Tensorflow"},{"location":"quick_start/quickstart_tensorflow/#quickstart-tensorflow","text":"In this tutorial, we will describe how to use Ifleaner in the Tensorflow framework to complete image classification federated training on the MNIST dataset. our example contains two clients and one server by default. In each round of training, the client is responsible for training and uploading the model parameters to the server, the server aggregates, and sends the aggregated global model parameters to each client, and then each client updates the aggregated model parameters. Multiple rounds will be repeated. First of all, we highly recommend to create a python virtual environment to run, you can use virtual tools such as virtualenv, pyenv, conda, etc. Next, we can quickly install the IFLearner library with the following command: pip install iflearner ```` Also, since we want to use Tensorflow for image classification tasks on MNIST data, we need to go ahead and install the Tensorflow libraries: ``` shell pip install tensorflow == 2 .9.1 ```` ### Ifleaner Server 1 . Create a new file named ` server.py ` , import iflearner: ``` python from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () You can start the server with the follow command: python server.py -n 2 -n 2: Receive two clients for federated training","title":"Quickstart (TensorFlow)"},{"location":"quick_start/quickstart_tensorflow/#ifleaner-client","text":"Create a new file named quickstart_tensorflow.py and do the following.","title":"Ifleaner Client"},{"location":"quick_start/quickstart_tensorflow/#1-define-model-network","text":"Firstly, you need define your model network by using TensorFlow. from tensorflow.keras.layers import Dense , Flatten , Conv2D from tensorflow.keras import Model class MyModel ( Model ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = Conv2D ( 32 , 3 , activation = 'relu' ) self . flatten = Flatten () self . d1 = Dense ( 128 , activation = 'relu' ) self . d2 = Dense ( 10 ) def call ( self , x ): x = self . conv1 ( x ) x = self . flatten ( x ) x = self . d1 ( x ) return self . d2 ( x )","title":"1. Define Model Network"},{"location":"quick_start/quickstart_tensorflow/#2-implement-trainer-class","text":"Secondly, you need implement your trainer class, inheriting from the iflearner.business.homo.trainer.Trainer class. The class need to implement four functions, which are get , set , fit and evaluate . We also have provided a iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer inheriting from the iflearner.business.homo.trainer.Trainer class, which has implement usual get and set functions. You can use this class as the follow: import tensorflow as tf from iflearner.business.homo.tensorflow_trainer import TensorFlowTrainer from iflearner.business.homo.train_client import Controller mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Add a channels dimension x_train = x_train [ ... , tf . newaxis ] . astype ( \"float32\" ) x_test = x_test [ ... , tf . newaxis ] . astype ( \"float32\" ) train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train )) . shuffle ( 10000 ) . batch ( 32 ) test_ds = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) . batch ( 32 ) loss_object = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) optimizer = tf . keras . optimizers . Adam () train_loss = tf . keras . metrics . Mean ( name = 'train_loss' ) train_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'train_accuracy' ) test_loss = tf . keras . metrics . Mean ( name = 'test_loss' ) test_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'test_accuracy' ) class Mnist ( TensorFlowTrainer ): def __init__ ( self , model ) -> None : super () . __init__ ( model ) train_loss . reset_states () train_accuracy . reset_states () test_loss . reset_states () test_accuracy . reset_states () def fit ( self , epoch ): for images , labels in train_ds : # train_step(images, labels) self . _fit ( images , labels ) @tf . function def _fit ( self , images , labels ): with tf . GradientTape () as tape : # training=True is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = True ) loss = loss_object ( labels , predictions ) gradients = tape . gradient ( loss , model . trainable_variables ) optimizer . apply_gradients ( zip ( gradients , model . trainable_variables )) train_loss ( loss ) train_accuracy ( labels , predictions ) def evaluate ( self , epoch ): for test_images , test_labels in test_ds : # test_step(test_images, test_labels) self . _evaluate ( test_images , test_labels ) print ( f 'Epoch { epoch } , ' f 'Loss: { train_loss . result () } , ' f 'Accuracy: { train_accuracy . result () * 100 } , ' f 'Test Loss: { test_loss . result () } , ' f 'Test Accuracy: { test_accuracy . result () * 100 } ' ) return { 'Accuracy' : train_accuracy . result () * 100 } @tf . function def _evaluate ( self , images , labels ): # training=False is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = False ) t_loss = loss_object ( labels , predictions ) test_loss ( t_loss )","title":"2. Implement Trainer Class"},{"location":"quick_start/quickstart_tensorflow/#3-start-ifleaner-client","text":"Lastly, you need to write a main function to start your client. You can do it as the follow: if __name__ == '__main__' : args = parser . parse_args () print ( args ) model = MyModel () mnist = Mnist ( model ) controller = Controller ( args , mnist ) controller . run () In the main function, you need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. You can use follow command to start the first client: python quickstart_tensorflow.py --name client01 --epochs 2 Open another terminal and start the second client: python quickstart_tensorflow.py --name client02 --epochs 2 After both clients are ready and started, we can see log messages similar to the following on either client terminal: 2022-08-03 18:07:07.406604: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:07:07.456 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:07:07.479 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 22.46353199999973ms 2022-08-03 18:07:07.479 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:07:07.480 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:07:07.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3502309999999795ms 2022-08-03 18:07:11.500 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.1013739999992112ms 2022-08-03 18:07:12.497 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:08:19.804 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 1, Loss: 0.1414850801229477, Accuracy: 95.73500061035156, Test Loss: 0.0603780597448349, Test Accuracy: 0.0 2022-08-03 18:08:22.759 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:08:24.130 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 149.0828160000035ms 2022-08-03 18:08:31.445 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 3241.557815999997ms 2022-08-03 18:08:32.446 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:08:32.474 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.617487000004303ms 2022-08-03 18:08:33.469 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 8.709660999997482ms 2022-08-03 18:08:33.479 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:09:46.374 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 2, Loss: 0.11132627725601196, Accuracy: 96.65583038330078, Test Loss: 0.0679144412279129, Test Accuracy: 0.0 2022-08-03 18:09:49.340 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>, <tf.Tensor: shape=(), dtype=float32, numpy=96.65583>]) label: LT, points: ([1], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>]) 2022-08-03 18:09:51.374 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.561914999996361ms congratulations! You have successfully built and run your first federated learning system. The complete source code reference for this example Quickstart_Tensorflow .","title":"3. Start Ifleaner Client"},{"location":"tutorial/argument/","text":"Startup Options \u00b6 Client \u00b6 You need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. The list of client default options is as follows: option type describe default name str name of client client epochs int number of total epochs to run 10 server str the address of connecting aggerating server localhost:50001 enable-ll int enable local training (1 \u3001 0) 0 peers str enabled SMPC if the argument had specified. all clients' addresses and use semicolon separate all addresses. First one is your own address. cert str path of server SSL cert. use secure channel to connect to server if not none Server \u00b6 The list of server options is as follows: option type describe default num int the number of all clients 0 epochs int the total epoch addr str The aggregation server itself listens to the address (used for client connections) \"0.0.0.0:50001\" http_addr str Federation training status listening address (for viewing federation training status) \"0.0.0.0:50002\" strategy str the aggregation starategy (FedAvg\u3001Scaffold\u3001FedOpt\u3001qFedAvg\u3001FedNova) FedAvg strategy_params dict specify the params of strategy {}","title":"Startup Option"},{"location":"tutorial/argument/#startup-options","text":"","title":"Startup Options"},{"location":"tutorial/argument/#client","text":"You need import parser from iflearner.business.homo.argument firstly and then call parser.parse_args , because we provided some common arguments that need to be parsered. If you want to add addtional arguments for yourself, you can call parser.add_argument repeatedly to add them before parser.parse_args has been called. After parsered arguments, you can create your trainer instance base on previous implemented class, and put it with args to train_client.Controller . In the end, you just need call controller.run to run your client. The list of client default options is as follows: option type describe default name str name of client client epochs int number of total epochs to run 10 server str the address of connecting aggerating server localhost:50001 enable-ll int enable local training (1 \u3001 0) 0 peers str enabled SMPC if the argument had specified. all clients' addresses and use semicolon separate all addresses. First one is your own address. cert str path of server SSL cert. use secure channel to connect to server if not none","title":"Client"},{"location":"tutorial/argument/#server","text":"The list of server options is as follows: option type describe default num int the number of all clients 0 epochs int the total epoch addr str The aggregation server itself listens to the address (used for client connections) \"0.0.0.0:50001\" http_addr str Federation training status listening address (for viewing federation training status) \"0.0.0.0:50002\" strategy str the aggregation starategy (FedAvg\u3001Scaffold\u3001FedOpt\u3001qFedAvg\u3001FedNova) FedAvg strategy_params dict specify the params of strategy {}","title":"Server"},{"location":"tutorial/contributor_guide/","text":"Contributor Guide \u00b6 Prerequisites \u00b6 Python 3.7 or above Development Environment Setup \u00b6 First, clone the IFLearner repository from GitHub: $ git clone https://github.com/iflytek/iflearner.git $ cd iflearner Then, you need to create a python virtual environment using virtual tools like conda, pyenv, etc. Below is an example of a command to create a virtual environment using conda: $ conda create -n iflearner python == 3 .9 $ conda activate iflearner Finally, you need to install the dependencies required by iflearner: $ pip install -r requirements.txt Development Script \u00b6 We provide some development scripts, you can find them in the ./dev directory. Code Auto-Format And Auto-Test \u00b6 First, execute the script to automatically format: $ ./dev/format.sh Second, execute the test script. Then, you should follow the code guidelines and make code adjustments as prompted. $ ./dev/test.sh Build Documentation \u00b6 IFLearner uses mkdocs to build documentation, you can go to the ./doc directory and follow the readme tutorial to build documentation. Pack Whl Release \u00b6 IFLearner uses setup to pack release. You can use the following command to package: python setup.py bdist_wheel The iflearner- .whl and iflearner- .tar.gz packages will be stored in the ./dist subdirectory. Publish Release \u00b6 If you have permission to publish release packages, you can do so as follows. First, configure the pypirc file. #Linux ## vim ~/.pypirc # Windows ## C:\\Users\\Username\\.pypirc : <<'COMMENT' [distutils] index-servers=pypi [pypi] repository=https://upload.pypi.org/legacy/ username=<username> password=<password> COMMENT Then, package release: shell python setup.py sdist ```` Finally, to publish release: shell twine upload dist/* -r pypi ````","title":"Contributor Guide"},{"location":"tutorial/contributor_guide/#contributor-guide","text":"","title":"Contributor Guide"},{"location":"tutorial/contributor_guide/#prerequisites","text":"Python 3.7 or above","title":"Prerequisites"},{"location":"tutorial/contributor_guide/#development-environment-setup","text":"First, clone the IFLearner repository from GitHub: $ git clone https://github.com/iflytek/iflearner.git $ cd iflearner Then, you need to create a python virtual environment using virtual tools like conda, pyenv, etc. Below is an example of a command to create a virtual environment using conda: $ conda create -n iflearner python == 3 .9 $ conda activate iflearner Finally, you need to install the dependencies required by iflearner: $ pip install -r requirements.txt","title":"Development Environment Setup"},{"location":"tutorial/contributor_guide/#development-script","text":"We provide some development scripts, you can find them in the ./dev directory.","title":"Development Script"},{"location":"tutorial/contributor_guide/#code-auto-format-and-auto-test","text":"First, execute the script to automatically format: $ ./dev/format.sh Second, execute the test script. Then, you should follow the code guidelines and make code adjustments as prompted. $ ./dev/test.sh","title":"Code Auto-Format And Auto-Test"},{"location":"tutorial/contributor_guide/#build-documentation","text":"IFLearner uses mkdocs to build documentation, you can go to the ./doc directory and follow the readme tutorial to build documentation.","title":"Build Documentation"},{"location":"tutorial/contributor_guide/#pack-whl-release","text":"IFLearner uses setup to pack release. You can use the following command to package: python setup.py bdist_wheel The iflearner- .whl and iflearner- .tar.gz packages will be stored in the ./dist subdirectory.","title":"Pack Whl Release"},{"location":"tutorial/contributor_guide/#publish-release","text":"If you have permission to publish release packages, you can do so as follows. First, configure the pypirc file. #Linux ## vim ~/.pypirc # Windows ## C:\\Users\\Username\\.pypirc : <<'COMMENT' [distutils] index-servers=pypi [pypi] repository=https://upload.pypi.org/legacy/ username=<username> password=<password> COMMENT Then, package release: shell python setup.py sdist ```` Finally, to publish release: shell twine upload dist/* -r pypi ````","title":"Publish Release"},{"location":"tutorial/custom_aggregation_strategy/","text":"To build your own homo strategy. \u00b6 1.Build Your Server \u00b6 In order to build Federate Learning Server, you should inherit iflearner.business.homo.strategy.strategy_server.StrategyServer and implement all the abstract methods. handler_register : Handle the message of MSG_REGISTER from the client. handler_client_ready : Handle the message of MSG_CLIENT_READY from the client. handler_upload_param : Handle the message of MSG_UPLOAD_PARAM from the client. The content of this message is determined by the client's Trainner.get() method. get_client_notification : Get the notification information of the specified client. The following is a simple example. For more details, you can refer to the fedavg implementation, or contact us. class StrategyServer (): \"\"\"Implement the strategy of server.\"\"\" def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers def handler_register ( self , party_name : str , sample_num : Optional [ int ] = None , step_num : Optional [ int ] = None , ) -> None : \"\"\"Handle the message of MSG_REGISTER from the client.\"\"\" pass def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" pass def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client.\"\"\" pass def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client.\"\"\" pass class MyStrategyServer ( StrategyServer ): '''''' strategy = MyStrategyServer ( ... ) server = AggregateServer ( args . addr , strategy , args . num ) server . run () 2. Build Your Client \u00b6 In order to build Federate Learning Client, you should inherit iflearner.business.homo.strategy.strategy_client.StrategyClient and implement all the abstract methods. class StrategyClient ( ABC ): \"\"\"Implement the strategy of client.\"\"\" class Stage ( IntEnum ): \"\"\"Enum the stage of client.\"\"\" Waiting = auto () Training = auto () Setting = auto () def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : self . _trainer_config = config @abstractmethod def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass @abstractmethod def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ]) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM.\"\"\" pass @abstractmethod def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training.\"\"\" pass @abstractmethod def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server.\"\"\" pass @abstractmethod def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" pass class MyStrategyClient ( StrategyClient ): '''''' mnist = Mnist () controller = Controller ( args , mnist , MyStrategyClient ()) controller . run () Example \u00b6 In this example, we will implement FedDyn step by step. Please refer to the original paper for FedDyn's algorithm flow. The complete source code reference for this example FedDyn . Build FedDyn Server \u00b6 Inherit iflearner.business.homo.strategy.strategy_server.StrategyServer class and override and implement some methods according to the algorithm flow. In FedDyn, we just need to override handler_upload_param and __init__ . from time import sleep from typing import Any , Dict , Optional , Tuple import numpy as np import torch from loguru import logger from torch import dtype , nn from torch.nn import functional as F from iflearner.business.homo.strategy import strategy_server from iflearner.communication.homo import homo_pb2 , message_type from iflearner.communication.homo.homo_exception import HomoException class FedDynServer ( strategy_server . StrategyServer ): \"\"\"Implement the strategy of feddyn on server side.\"\"\" \"\"\" num_clients: client numuber alpha: a static coefficient \"\"\" def __init__ ( self , num_clients : int , learning_rate = 0.1 , alpha = 0.1 , params : Dict [ str , np . ndarray ] = None , ) -> None : super () . __init__ () self . _num_clients = num_clients self . _lr = learning_rate self . _alpha = alpha self . _params = params logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _clients_samples : dict = {} self . _h = { name : np . zeros_like ( p ) . reshape ( - 1 ) for name , p in self . _params . items () } def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _uploaded_num == self . _num_clients : self . _uploaded_num = 0 aggregate_result = dict () grad = dict () logger . info ( f \"Faddyn params, param num: { len ( data . parameters ) } \" ) for param_name , param_info in data . parameters . items (): aggregate_result [ param_name ] = homo_pb2 . Parameter ( shape = param_info . shape ) params = [] for v in self . _training_clients . values (): params . append ( v [ \"param\" ][ param_name ] . values ) avg_param = [ sum ( x ) * ( 1 / self . _num_clients ) for x in zip ( * params )] grad [ param_name ] = np . array ( avg_param , dtype = \"float32\" ) - self . _params [ param_name ] . reshape (( - 1 )) self . _h [ param_name ] = ( self . _h [ param_name ] - self . _alpha * grad [ param_name ] ) self . _params [ param_name ] = ( np . array ( avg_param , dtype = \"float32\" ) - ( 1 / self . _alpha ) * self . _h [ param_name ] ) . reshape ( param_info . shape ) aggregate_result [ param_name ] . values . extend ( self . _params [ param_name ] . reshape ( - 1 ) . tolist () ) self . _server_param = aggregate_result # type: ignore self . _on_aggregating = True Build Client \u00b6 According to the FedDyn algorithm flow, we need to override the Trainner.fit method def fit ( self , epoch ): self . _old_weights = deepcopy ( self . _model . state_dict ()) batch_time = AverageMeter ( \"Time\" , \":6.3f\" ) data_time = AverageMeter ( \"Data\" , \":6.3f\" ) losses = AverageMeter ( \"Loss\" , \":.4e\" ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , data_time , losses , top1 , top5 ], prefix = \" {} Epoch: [ {} ]\" . format ( self . _args . name , epoch ), ) # switch to train mode self . _model . train () end = time . time () for _ in range ( 1 ): for i , ( images , target ) in enumerate ( self . _train_loader ): # measure data loading time data_time . update ( time . time () - end ) if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) linear_penalty = sum ( [ torch . sum (( p * self . _old_grad [ name ])) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) quad_penalty = sum ( [ F . mse_loss ( p , self . _old_weights [ name ], reduction = \"sum\" ) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) loss += quad_penalty * self . _alpha / 2 loss -= linear_penalty # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # compute gradient and do SGD step self . _optimizer . zero_grad () loss . backward () if self . _scaffold : g = yield self . get ( self . ParameterType . ParameterGradient ) self . set ( homo_pb2 . AggregateResult ( parameters = g . parameters ), self . ParameterType . ParameterGradient , ) self . _optimizer . step () # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) self . _old_grad = { name : p . grad for name , p in self . _model . named_parameters () }","title":"Custom Aggregation Strategy"},{"location":"tutorial/custom_aggregation_strategy/#to-build-your-own-homo-strategy","text":"","title":"To build your own homo strategy."},{"location":"tutorial/custom_aggregation_strategy/#1build-your-server","text":"In order to build Federate Learning Server, you should inherit iflearner.business.homo.strategy.strategy_server.StrategyServer and implement all the abstract methods. handler_register : Handle the message of MSG_REGISTER from the client. handler_client_ready : Handle the message of MSG_CLIENT_READY from the client. handler_upload_param : Handle the message of MSG_UPLOAD_PARAM from the client. The content of this message is determined by the client's Trainner.get() method. get_client_notification : Get the notification information of the specified client. The following is a simple example. For more details, you can refer to the fedavg implementation, or contact us. class StrategyServer (): \"\"\"Implement the strategy of server.\"\"\" def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers def handler_register ( self , party_name : str , sample_num : Optional [ int ] = None , step_num : Optional [ int ] = None , ) -> None : \"\"\"Handle the message of MSG_REGISTER from the client.\"\"\" pass def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" pass def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client.\"\"\" pass def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client.\"\"\" pass class MyStrategyServer ( StrategyServer ): '''''' strategy = MyStrategyServer ( ... ) server = AggregateServer ( args . addr , strategy , args . num ) server . run ()","title":"1.Build Your Server"},{"location":"tutorial/custom_aggregation_strategy/#2-build-your-client","text":"In order to build Federate Learning Client, you should inherit iflearner.business.homo.strategy.strategy_client.StrategyClient and implement all the abstract methods. class StrategyClient ( ABC ): \"\"\"Implement the strategy of client.\"\"\" class Stage ( IntEnum ): \"\"\"Enum the stage of client.\"\"\" Waiting = auto () Training = auto () Setting = auto () def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : self . _trainer_config = config @abstractmethod def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass @abstractmethod def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ]) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM.\"\"\" pass @abstractmethod def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training.\"\"\" pass @abstractmethod def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server.\"\"\" pass @abstractmethod def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" pass class MyStrategyClient ( StrategyClient ): '''''' mnist = Mnist () controller = Controller ( args , mnist , MyStrategyClient ()) controller . run ()","title":"2. Build Your Client"},{"location":"tutorial/custom_aggregation_strategy/#example","text":"In this example, we will implement FedDyn step by step. Please refer to the original paper for FedDyn's algorithm flow. The complete source code reference for this example FedDyn .","title":"Example"},{"location":"tutorial/custom_aggregation_strategy/#build-feddyn-server","text":"Inherit iflearner.business.homo.strategy.strategy_server.StrategyServer class and override and implement some methods according to the algorithm flow. In FedDyn, we just need to override handler_upload_param and __init__ . from time import sleep from typing import Any , Dict , Optional , Tuple import numpy as np import torch from loguru import logger from torch import dtype , nn from torch.nn import functional as F from iflearner.business.homo.strategy import strategy_server from iflearner.communication.homo import homo_pb2 , message_type from iflearner.communication.homo.homo_exception import HomoException class FedDynServer ( strategy_server . StrategyServer ): \"\"\"Implement the strategy of feddyn on server side.\"\"\" \"\"\" num_clients: client numuber alpha: a static coefficient \"\"\" def __init__ ( self , num_clients : int , learning_rate = 0.1 , alpha = 0.1 , params : Dict [ str , np . ndarray ] = None , ) -> None : super () . __init__ () self . _num_clients = num_clients self . _lr = learning_rate self . _alpha = alpha self . _params = params logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _clients_samples : dict = {} self . _h = { name : np . zeros_like ( p ) . reshape ( - 1 ) for name , p in self . _params . items () } def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _uploaded_num == self . _num_clients : self . _uploaded_num = 0 aggregate_result = dict () grad = dict () logger . info ( f \"Faddyn params, param num: { len ( data . parameters ) } \" ) for param_name , param_info in data . parameters . items (): aggregate_result [ param_name ] = homo_pb2 . Parameter ( shape = param_info . shape ) params = [] for v in self . _training_clients . values (): params . append ( v [ \"param\" ][ param_name ] . values ) avg_param = [ sum ( x ) * ( 1 / self . _num_clients ) for x in zip ( * params )] grad [ param_name ] = np . array ( avg_param , dtype = \"float32\" ) - self . _params [ param_name ] . reshape (( - 1 )) self . _h [ param_name ] = ( self . _h [ param_name ] - self . _alpha * grad [ param_name ] ) self . _params [ param_name ] = ( np . array ( avg_param , dtype = \"float32\" ) - ( 1 / self . _alpha ) * self . _h [ param_name ] ) . reshape ( param_info . shape ) aggregate_result [ param_name ] . values . extend ( self . _params [ param_name ] . reshape ( - 1 ) . tolist () ) self . _server_param = aggregate_result # type: ignore self . _on_aggregating = True","title":"Build FedDyn Server"},{"location":"tutorial/custom_aggregation_strategy/#build-client","text":"According to the FedDyn algorithm flow, we need to override the Trainner.fit method def fit ( self , epoch ): self . _old_weights = deepcopy ( self . _model . state_dict ()) batch_time = AverageMeter ( \"Time\" , \":6.3f\" ) data_time = AverageMeter ( \"Data\" , \":6.3f\" ) losses = AverageMeter ( \"Loss\" , \":.4e\" ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , data_time , losses , top1 , top5 ], prefix = \" {} Epoch: [ {} ]\" . format ( self . _args . name , epoch ), ) # switch to train mode self . _model . train () end = time . time () for _ in range ( 1 ): for i , ( images , target ) in enumerate ( self . _train_loader ): # measure data loading time data_time . update ( time . time () - end ) if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) linear_penalty = sum ( [ torch . sum (( p * self . _old_grad [ name ])) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) quad_penalty = sum ( [ F . mse_loss ( p , self . _old_weights [ name ], reduction = \"sum\" ) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) loss += quad_penalty * self . _alpha / 2 loss -= linear_penalty # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # compute gradient and do SGD step self . _optimizer . zero_grad () loss . backward () if self . _scaffold : g = yield self . get ( self . ParameterType . ParameterGradient ) self . set ( homo_pb2 . AggregateResult ( parameters = g . parameters ), self . ParameterType . ParameterGradient , ) self . _optimizer . step () # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) self . _old_grad = { name : p . grad for name , p in self . _model . named_parameters () }","title":"Build Client"},{"location":"tutorial/metrics_visualization/","text":"Metrics Visualization \u00b6 We have integrated VisualDL inside iflearner, so you can visualize training metrics with it. When you have finished training, there will be a directory called \"metric\" under your training code directory. You can start VisualDL with the following command: visualdl --logdir ./metric --host 127.0.0.1 --port 8082 Then, open the link http://127.0.0.1:8082 in your browser.","title":"Metrics Visualization"},{"location":"tutorial/metrics_visualization/#metrics-visualization","text":"We have integrated VisualDL inside iflearner, so you can visualize training metrics with it. When you have finished training, there will be a directory called \"metric\" under your training code directory. You can start VisualDL with the following command: visualdl --logdir ./metric --host 127.0.0.1 --port 8082 Then, open the link http://127.0.0.1:8082 in your browser.","title":"Metrics Visualization"},{"location":"tutorial/run_in_container/","text":"Run IFLearner in the container \u00b6 We can use containerization technology to run IFLearner in Docker and Jupyterlab. Docker \u00b6 Docker is a package of Linux containers that provides an easy-to-use container interface. It is currently the most popular Linux container solution. Docker packages the application and the program's dependencies in a single file. Running this file will generate a virtual container. Programs run in this virtual container as if they were running on a real physical machine. With Docker, you don't have to worry about the environment. You can find some of our examples and tutorials in iflearner project's ./enviroment/docker directory. Jupyterlab \u00b6 JupyterLab is Jupyter's latest data science production tool, and in a sense, it appeared to replace Jupyter Notebook. But don't worry that Jupyter Notebook will disappear, JupyterLab includes all the features of Jupyter Notebook. JupyterLab is a web-based integrated development environment, you can use it to write notebooks, operate terminals, edit markdown text, open interactive mode, view csv files and pictures and other functions. You can find some of our examples and tutorials in iflearner project's ./enviroment/jupyterlab directory.","title":"Run In Container"},{"location":"tutorial/run_in_container/#run-iflearner-in-the-container","text":"We can use containerization technology to run IFLearner in Docker and Jupyterlab.","title":"Run IFLearner in the container"},{"location":"tutorial/run_in_container/#docker","text":"Docker is a package of Linux containers that provides an easy-to-use container interface. It is currently the most popular Linux container solution. Docker packages the application and the program's dependencies in a single file. Running this file will generate a virtual container. Programs run in this virtual container as if they were running on a real physical machine. With Docker, you don't have to worry about the environment. You can find some of our examples and tutorials in iflearner project's ./enviroment/docker directory.","title":"Docker"},{"location":"tutorial/run_in_container/#jupyterlab","text":"JupyterLab is Jupyter's latest data science production tool, and in a sense, it appeared to replace Jupyter Notebook. But don't worry that Jupyter Notebook will disappear, JupyterLab includes all the features of Jupyter Notebook. JupyterLab is a web-based integrated development environment, you can use it to write notebooks, operate terminals, edit markdown text, open interactive mode, view csv files and pictures and other functions. You can find some of our examples and tutorials in iflearner project's ./enviroment/jupyterlab directory.","title":"Jupyterlab"},{"location":"tutorial/strategy/fednova/","text":"FedNova \u00b6 According to the article Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization , we implement FedNova aggregation algorithm. The complete source code reference for this example fednova . To start a FedNova server \u00b6 strategy = message_type . STRATEGY_FEDNOVA # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedNova To start a client \u00b6 See how to use FedNova involves the number of samples on the client side ( sample_num ) and the number of times each round of training optimizations ( batch_num ), so the Trainer.config method needs to be override to return these two values def config ( self ) -> dict (): return { \"batch_num\" : len ( self . _train_loader ), \"sample_num\" : len ( self . _train_loader ) * self . _train_loader . batch_size , } FedNova also needs to override the Trainer.get method to return the difference between the client current model and the previous round model. def get ( self , param_type = \"\" ): parameters = dict () for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( \"module.\" , \"\" )] = ( p . cpu () . detach () . numpy () - self . _old_weights [ name ] . cpu () . detach () . numpy () ) return parameters","title":"FedNova Strategy"},{"location":"tutorial/strategy/fednova/#fednova","text":"According to the article Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization , we implement FedNova aggregation algorithm. The complete source code reference for this example fednova .","title":"FedNova"},{"location":"tutorial/strategy/fednova/#to-start-a-fednova-server","text":"strategy = message_type . STRATEGY_FEDNOVA # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedNova","title":"To start a FedNova server"},{"location":"tutorial/strategy/fednova/#to-start-a-client","text":"See how to use FedNova involves the number of samples on the client side ( sample_num ) and the number of times each round of training optimizations ( batch_num ), so the Trainer.config method needs to be override to return these two values def config ( self ) -> dict (): return { \"batch_num\" : len ( self . _train_loader ), \"sample_num\" : len ( self . _train_loader ) * self . _train_loader . batch_size , } FedNova also needs to override the Trainer.get method to return the difference between the client current model and the previous round model. def get ( self , param_type = \"\" ): parameters = dict () for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( \"module.\" , \"\" )] = ( p . cpu () . detach () . numpy () - self . _old_weights [ name ] . cpu () . detach () . numpy () ) return parameters","title":"To start a client"},{"location":"tutorial/strategy/fedopt/","text":"FedOpt \u00b6 According to the article Adaptive Federated Optimization , we implement fedopt aggregation algorithm, such as fedadam, fedyogi, fedadagrad... The complete source code reference for this example FedOpt . You can implement your own fedopt aggregation algorithm by inheriting the FedOpt class FedOpt : \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _params = params self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' pass If you want to use optimizers of Pytorch version, the following is an example. You can try anything to optimize your model parameters (make sure the step() return the flattened parameters) class PytorchFedAdam ( FedOpt ): \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , model : nn . Module params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( params , learning_rate , betas , t ) self . _model = model self . _opt = torch . optim . Adam ( self . _model . parameters (), lr , betas = betas ) def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' for name , p in self . _model . named_parameters (): p . grad = torch . from_numpy ( np . array ( pseudo_gradient [ name ] )) . reshape ( p . shape ) . type ( p . grad . dtype ) . to ( p . device ) self . _opt . step () params = dict () for name , param in model . named_parameters (): if param . requires_grad : params [ name ] = param . cpu () . detach () . numpy () . reshape (( - 1 )) return params To start a fedopt server \u00b6 strategy = message_type . STRATEGY_FEDOPT # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedOpt --strategy_params { \"learning_rate\" :1, \"betas\" : [ 0 .9,0.99 ] , \"t\" :0.1, \"opt\" : \"FedAdam\" } To start a client \u00b6 See how to use","title":"Fedopt Strategy"},{"location":"tutorial/strategy/fedopt/#fedopt","text":"According to the article Adaptive Federated Optimization , we implement fedopt aggregation algorithm, such as fedadam, fedyogi, fedadagrad... The complete source code reference for this example FedOpt . You can implement your own fedopt aggregation algorithm by inheriting the FedOpt class FedOpt : \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _params = params self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' pass If you want to use optimizers of Pytorch version, the following is an example. You can try anything to optimize your model parameters (make sure the step() return the flattened parameters) class PytorchFedAdam ( FedOpt ): \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , model : nn . Module params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( params , learning_rate , betas , t ) self . _model = model self . _opt = torch . optim . Adam ( self . _model . parameters (), lr , betas = betas ) def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' for name , p in self . _model . named_parameters (): p . grad = torch . from_numpy ( np . array ( pseudo_gradient [ name ] )) . reshape ( p . shape ) . type ( p . grad . dtype ) . to ( p . device ) self . _opt . step () params = dict () for name , param in model . named_parameters (): if param . requires_grad : params [ name ] = param . cpu () . detach () . numpy () . reshape (( - 1 )) return params","title":"FedOpt"},{"location":"tutorial/strategy/fedopt/#to-start-a-fedopt-server","text":"strategy = message_type . STRATEGY_FEDOPT # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedOpt --strategy_params { \"learning_rate\" :1, \"betas\" : [ 0 .9,0.99 ] , \"t\" :0.1, \"opt\" : \"FedAdam\" }","title":"To start a fedopt server"},{"location":"tutorial/strategy/fedopt/#to-start-a-client","text":"See how to use","title":"To start a client"},{"location":"tutorial/strategy/qfedavg/","text":"qFedAvg \u00b6 According to the article FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING , we implement qfedavg aggregation algorithm. The complete source code reference for this example qFedAvg . To start a qfedav server \u00b6 strategy = message_type . STRATEGY_qFEDAVG # define server type server = AggregateServer ( args . addr , strategy , args . num , q = .2 , learning_rate = 1 ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy qFedAvg --strategy_params { \"q\" :.2, \"learning_rate\" :1 } To start a client \u00b6 See how to use It should be noted that before the client starts fitting, you need to obtain the loss value loss of the current model on the training data. Then, you should override the Trainer.get method on the client side and add the loss keyword to the uploaded parameters. def evaluate_traindata ( self ): batch_time = AverageMeter ( \"Time\" , \":6.3f\" , Summary . AVERAGE ) losses = AverageMeter ( \"Loss\" , \":.4e\" , Summary . AVERAGE ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" , Summary . AVERAGE ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" , Summary . AVERAGE ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , losses , top1 , top5 ], prefix = \"Test on training data: \" ) with torch . no_grad (): end = time . time () for i , ( images , target ) in enumerate ( self . _train_loader ): if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) progress . display_summary () self . _fs = losses . avg def get ( self , param_type = '' ): parameters = dict () parameters [ 'loss' ] = np . array ([ self . _fs ]) for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( 'module.' , '' ) ] = p . cpu () . detach () . numpy () return parameters","title":"Qfedavg Strategy"},{"location":"tutorial/strategy/qfedavg/#qfedavg","text":"According to the article FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING , we implement qfedavg aggregation algorithm. The complete source code reference for this example qFedAvg .","title":"qFedAvg"},{"location":"tutorial/strategy/qfedavg/#to-start-a-qfedav-server","text":"strategy = message_type . STRATEGY_qFEDAVG # define server type server = AggregateServer ( args . addr , strategy , args . num , q = .2 , learning_rate = 1 ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy qFedAvg --strategy_params { \"q\" :.2, \"learning_rate\" :1 }","title":"To start a qfedav server"},{"location":"tutorial/strategy/qfedavg/#to-start-a-client","text":"See how to use It should be noted that before the client starts fitting, you need to obtain the loss value loss of the current model on the training data. Then, you should override the Trainer.get method on the client side and add the loss keyword to the uploaded parameters. def evaluate_traindata ( self ): batch_time = AverageMeter ( \"Time\" , \":6.3f\" , Summary . AVERAGE ) losses = AverageMeter ( \"Loss\" , \":.4e\" , Summary . AVERAGE ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" , Summary . AVERAGE ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" , Summary . AVERAGE ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , losses , top1 , top5 ], prefix = \"Test on training data: \" ) with torch . no_grad (): end = time . time () for i , ( images , target ) in enumerate ( self . _train_loader ): if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) progress . display_summary () self . _fs = losses . avg def get ( self , param_type = '' ): parameters = dict () parameters [ 'loss' ] = np . array ([ self . _fs ]) for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( 'module.' , '' ) ] = p . cpu () . detach () . numpy () return parameters","title":"To start a client"},{"location":"zh/","text":"iFLearner - \u4e00\u4e2a\u5f3a\u5927\u4e14\u8f7b\u91cf\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6 \u00b6 iFLearner\u662f\u4e00\u4e2a\u5f3a\u5927\u4e14\u8f7b\u91cf\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9690\u79c1\u5b89\u5168\u4fdd\u62a4\u7684\u5b89\u5168\u8ba1\u7b97\u6846\u67b6\uff0c \u4e3b\u8981\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u8054\u90a6\u5efa\u6a21\u3002\u5176\u5b89\u5168\u5e95\u5c42\u652f\u6301\u540c\u6001\u52a0\u5bc6\u3001\u79d8\u5bc6\u5171\u4eab\u3001\u5dee\u5206\u9690\u79c1\u7b49\u591a\u79cd\u52a0\u5bc6\u6280\u672f\uff0c \u7b97\u6cd5\u5c42\u652f\u6301\u5404\u7c7b\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u4e14\u540c\u65f6\u652f\u6301Tensorflow\u3001Mxnet\u3001Pytorch\u7b49\u4e3b\u6d41\u6846\u67b6\u3002 \u67b6\u6784 \u00b6 iFLearner\u4e3b\u8981\u57fa\u4e8e\u4ee5\u4e0b\u539f\u5219\u8fdb\u884c\u8bbe\u8ba1: \u4e8b\u4ef6\u9a71\u52a8\u673a\u5236 : \u4f7f\u7528\u4e8b\u4ef6\u9a71\u52a8\u7684\u7f16\u7a0b\u8303\u5f0f\u6765\u6784\u5efa\u8054\u90a6\u5b66\u4e60\uff0c\u5373\u5c06\u8054\u90a6\u5b66\u4e60\u770b\u6210\u662f\u53c2\u4e0e\u65b9\u4e4b\u95f4\u6536\u53d1\u6d88\u606f\u7684\u8fc7\u7a0b\uff0c \u901a\u8fc7\u5b9a\u4e49\u6d88\u606f\u7c7b\u578b\u4ee5\u53ca\u5904\u7406\u6d88\u606f\u7684\u884c\u4e3a\u6765\u63cf\u8ff0\u8054\u90a6\u5b66\u4e60\u8fc7\u7a0b\u3002 \u8bad\u7ec3\u6846\u67b6\u62bd\u8c61 : \u62bd\u8c61\u6df1\u5ea6\u5b66\u4e60\u540e\u7aef\uff0c\u517c\u5bb9\u652f\u6301Tensorflow\u3001Pytorch\u7b49\u591a\u7c7b\u6846\u67b6\u540e\u7aef\u3002 \u6269\u5c55\u6027\u9ad8 \uff1a\u6a21\u5757\u5f0f\u8bbe\u8ba1\uff0c\u7528\u6237\u53ef\u4ee5\u81ea\u5b9a\u4e49\u805a\u5408\u7b56\u7565\uff0c\u52a0\u5bc6\u6a21\u5757\uff0c\u540c\u65f6\u652f\u6301\u5404\u7c7b\u573a\u666f\u4e0b\u7684\u7b97\u6cd5\u3002 \u8f7b\u91cf\u4e14\u7b80\u5355 \uff1a\u8be5\u6846\u67b6Lib\u7ea7\u522b\uff0c\u8db3\u591f\u8f7b\u91cf\uff0c\u540c\u65f6\u7528\u6237\u53ef\u4ee5\u7b80\u5355\u6539\u9020\u81ea\u5df1\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u4e3a\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u3002","title":"Home"},{"location":"zh/#iflearner-","text":"iFLearner\u662f\u4e00\u4e2a\u5f3a\u5927\u4e14\u8f7b\u91cf\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9690\u79c1\u5b89\u5168\u4fdd\u62a4\u7684\u5b89\u5168\u8ba1\u7b97\u6846\u67b6\uff0c \u4e3b\u8981\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u8054\u90a6\u5efa\u6a21\u3002\u5176\u5b89\u5168\u5e95\u5c42\u652f\u6301\u540c\u6001\u52a0\u5bc6\u3001\u79d8\u5bc6\u5171\u4eab\u3001\u5dee\u5206\u9690\u79c1\u7b49\u591a\u79cd\u52a0\u5bc6\u6280\u672f\uff0c \u7b97\u6cd5\u5c42\u652f\u6301\u5404\u7c7b\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u4e14\u540c\u65f6\u652f\u6301Tensorflow\u3001Mxnet\u3001Pytorch\u7b49\u4e3b\u6d41\u6846\u67b6\u3002","title":"iFLearner - \u4e00\u4e2a\u5f3a\u5927\u4e14\u8f7b\u91cf\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6"},{"location":"zh/#_1","text":"iFLearner\u4e3b\u8981\u57fa\u4e8e\u4ee5\u4e0b\u539f\u5219\u8fdb\u884c\u8bbe\u8ba1: \u4e8b\u4ef6\u9a71\u52a8\u673a\u5236 : \u4f7f\u7528\u4e8b\u4ef6\u9a71\u52a8\u7684\u7f16\u7a0b\u8303\u5f0f\u6765\u6784\u5efa\u8054\u90a6\u5b66\u4e60\uff0c\u5373\u5c06\u8054\u90a6\u5b66\u4e60\u770b\u6210\u662f\u53c2\u4e0e\u65b9\u4e4b\u95f4\u6536\u53d1\u6d88\u606f\u7684\u8fc7\u7a0b\uff0c \u901a\u8fc7\u5b9a\u4e49\u6d88\u606f\u7c7b\u578b\u4ee5\u53ca\u5904\u7406\u6d88\u606f\u7684\u884c\u4e3a\u6765\u63cf\u8ff0\u8054\u90a6\u5b66\u4e60\u8fc7\u7a0b\u3002 \u8bad\u7ec3\u6846\u67b6\u62bd\u8c61 : \u62bd\u8c61\u6df1\u5ea6\u5b66\u4e60\u540e\u7aef\uff0c\u517c\u5bb9\u652f\u6301Tensorflow\u3001Pytorch\u7b49\u591a\u7c7b\u6846\u67b6\u540e\u7aef\u3002 \u6269\u5c55\u6027\u9ad8 \uff1a\u6a21\u5757\u5f0f\u8bbe\u8ba1\uff0c\u7528\u6237\u53ef\u4ee5\u81ea\u5b9a\u4e49\u805a\u5408\u7b56\u7565\uff0c\u52a0\u5bc6\u6a21\u5757\uff0c\u540c\u65f6\u652f\u6301\u5404\u7c7b\u573a\u666f\u4e0b\u7684\u7b97\u6cd5\u3002 \u8f7b\u91cf\u4e14\u7b80\u5355 \uff1a\u8be5\u6846\u67b6Lib\u7ea7\u522b\uff0c\u8db3\u591f\u8f7b\u91cf\uff0c\u540c\u65f6\u7528\u6237\u53ef\u4ee5\u7b80\u5355\u6539\u9020\u81ea\u5df1\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u4e3a\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u3002","title":"\u67b6\u6784"},{"location":"zh/api/reference/","text":"","title":"Index"},{"location":"zh/api/reference/business/","text":"","title":"Index"},{"location":"zh/api/reference/business/homo/","text":"","title":"Index"},{"location":"zh/api/reference/business/homo/aggregate_server/","text":"AggregateServer ( addr , strategy , num_clients , strategy_params = {}, epochs = 0 ) \u00b6 The server processes the requests of all parties according to the usage policy. Source code in iflearner/business/homo/aggregate_server.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , addr : str , strategy : Union [ str , StrategyServer ], num_clients : int , strategy_params : Dict [ str , Any ] = {}, epochs : int = 0 , ) -> None : logger . add ( \"log/server.log\" , backtrace = True , diagnose = True ) logger . info ( f \"server address: { addr } , strategy: { strategy } , client number: { num_clients } , epochs: { epochs } , strategy params: { strategy_params } \" ) if isinstance ( strategy , str ): if strategy == message_type . STRATEGY_FEDAVG : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , False , ** strategy_params ) elif strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , True , ** strategy_params ) elif strategy == message_type . STRATEGY_FEDOPT : if strategy_params . get ( \"opt\" ) is None : raise Exception ( \"expect 'opt' when you use fedopt sever\" ) else : opt_type = strategy_params . pop ( \"opt\" ) module = import_module ( f \"iflearner.business.homo.strategy.opt. { opt_type . lower () } \" ) opt_class = getattr ( module , f \" { opt_type } \" ) opt = opt_class ( ** strategy_params ) self . _strategy_server = fedopt_server . FedoptServer ( num_clients , epochs , opt = opt , ) # type: ignore logger . info ( \" \" . join ([ f \" { k } : { v } \" for k , v in strategy_params . items ()]) ) elif strategy == message_type . STRATEGY_qFEDAVG : self . _strategy_server = qfedavg_server . qFedavgServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif strategy == message_type . STRATEGY_FEDNOVA : self . _strategy_server = fednova_server . FedNovaServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif isinstance ( strategy , StrategyServer ): self . _strategy_server = strategy # type: ignore self . _addr = addr run () \u00b6 start server Source code in iflearner/business/homo/aggregate_server.py 95 96 97 98 99 def run ( self ) -> None : \"\"\"start server\"\"\" homo_server_inst = homo_server . HomoServer ( self . _strategy_server ) base_server . start_server ( self . _addr , homo_server_inst )","title":"aggregate_server"},{"location":"zh/api/reference/business/homo/aggregate_server/#iflearner.business.homo.aggregate_server.AggregateServer","text":"The server processes the requests of all parties according to the usage policy. Source code in iflearner/business/homo/aggregate_server.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , addr : str , strategy : Union [ str , StrategyServer ], num_clients : int , strategy_params : Dict [ str , Any ] = {}, epochs : int = 0 , ) -> None : logger . add ( \"log/server.log\" , backtrace = True , diagnose = True ) logger . info ( f \"server address: { addr } , strategy: { strategy } , client number: { num_clients } , epochs: { epochs } , strategy params: { strategy_params } \" ) if isinstance ( strategy , str ): if strategy == message_type . STRATEGY_FEDAVG : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , False , ** strategy_params ) elif strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy_server = fedavg_server . FedavgServer ( num_clients , epochs , True , ** strategy_params ) elif strategy == message_type . STRATEGY_FEDOPT : if strategy_params . get ( \"opt\" ) is None : raise Exception ( \"expect 'opt' when you use fedopt sever\" ) else : opt_type = strategy_params . pop ( \"opt\" ) module = import_module ( f \"iflearner.business.homo.strategy.opt. { opt_type . lower () } \" ) opt_class = getattr ( module , f \" { opt_type } \" ) opt = opt_class ( ** strategy_params ) self . _strategy_server = fedopt_server . FedoptServer ( num_clients , epochs , opt = opt , ) # type: ignore logger . info ( \" \" . join ([ f \" { k } : { v } \" for k , v in strategy_params . items ()]) ) elif strategy == message_type . STRATEGY_qFEDAVG : self . _strategy_server = qfedavg_server . qFedavgServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif strategy == message_type . STRATEGY_FEDNOVA : self . _strategy_server = fednova_server . FedNovaServer ( num_clients , epochs , ** strategy_params ) # type: ignore elif isinstance ( strategy , StrategyServer ): self . _strategy_server = strategy # type: ignore self . _addr = addr","title":"AggregateServer"},{"location":"zh/api/reference/business/homo/aggregate_server/#iflearner.business.homo.aggregate_server.AggregateServer.run","text":"start server Source code in iflearner/business/homo/aggregate_server.py 95 96 97 98 99 def run ( self ) -> None : \"\"\"start server\"\"\" homo_server_inst = homo_server . HomoServer ( self . _strategy_server ) base_server . start_server ( self . _addr , homo_server_inst )","title":"run()"},{"location":"zh/api/reference/business/homo/argument/","text":"","title":"argument"},{"location":"zh/api/reference/business/homo/keras_trainer/","text":"KerasTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual keras trainer. Source code in iflearner/business/homo/keras_trainer.py 27 28 def __init__ ( self , model : keras . models . Sequential ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/keras_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/keras_trainer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"keras_trainer"},{"location":"zh/api/reference/business/homo/keras_trainer/#iflearner.business.homo.keras_trainer.KerasTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual keras trainer. Source code in iflearner/business/homo/keras_trainer.py 27 28 def __init__ ( self , model : keras . models . Sequential ) -> None : self . _model = model","title":"KerasTrainer"},{"location":"zh/api/reference/business/homo/keras_trainer/#iflearner.business.homo.keras_trainer.KerasTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/keras_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters","title":"get()"},{"location":"zh/api/reference/business/homo/keras_trainer/#iflearner.business.homo.keras_trainer.KerasTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/keras_trainer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"set()"},{"location":"zh/api/reference/business/homo/mxnet_trainer/","text":"MxnetTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual mxnet trainer. Source code in iflearner/business/homo/mxnet_trainer.py 27 28 def __init__ ( self , model : mx . gluon . nn . Sequential ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/mxnet_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for key , val in self . _model . collect_params ( \".*weight\" ) . items (): p = val . data () . asnumpy () parameters [ key ] = p return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/mxnet_trainer.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for key , value in parameters . items (): self . _model . collect_params () . setattr ( key , value )","title":"mxnet_trainer"},{"location":"zh/api/reference/business/homo/mxnet_trainer/#iflearner.business.homo.mxnet_trainer.MxnetTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual mxnet trainer. Source code in iflearner/business/homo/mxnet_trainer.py 27 28 def __init__ ( self , model : mx . gluon . nn . Sequential ) -> None : self . _model = model","title":"MxnetTrainer"},{"location":"zh/api/reference/business/homo/mxnet_trainer/#iflearner.business.homo.mxnet_trainer.MxnetTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/mxnet_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for key , val in self . _model . collect_params ( \".*weight\" ) . items (): p = val . data () . asnumpy () parameters [ key ] = p return parameters","title":"get()"},{"location":"zh/api/reference/business/homo/mxnet_trainer/#iflearner.business.homo.mxnet_trainer.MxnetTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/mxnet_trainer.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for key , value in parameters . items (): self . _model . collect_params () . setattr ( key , value )","title":"set()"},{"location":"zh/api/reference/business/homo/pytorch_trainer/","text":"PyTorchTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual pytorch trainer. Source code in iflearner/business/homo/pytorch_trainer.py 27 28 def __init__ ( self , model : torch . nn . Module ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/pytorch_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : parameters [ name ] = param . cpu () . detach () . numpy () else : parameters [ name ] = param . grad . cpu () . detach () . numpy () return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/pytorch_trainer.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : param . data . copy_ ( torch . from_numpy ( parameters [ name ])) else : param . grad . copy_ ( torch . from_numpy ( parameters [ name ]))","title":"pytorch_trainer"},{"location":"zh/api/reference/business/homo/pytorch_trainer/#iflearner.business.homo.pytorch_trainer.PyTorchTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual pytorch trainer. Source code in iflearner/business/homo/pytorch_trainer.py 27 28 def __init__ ( self , model : torch . nn . Module ) -> None : self . _model = model","title":"PyTorchTrainer"},{"location":"zh/api/reference/business/homo/pytorch_trainer/#iflearner.business.homo.pytorch_trainer.PyTorchTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/pytorch_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : parameters [ name ] = param . cpu () . detach () . numpy () else : parameters [ name ] = param . grad . cpu () . detach () . numpy () return parameters","title":"get()"},{"location":"zh/api/reference/business/homo/pytorch_trainer/#iflearner.business.homo.pytorch_trainer.PyTorchTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/pytorch_trainer.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for name , param in self . _model . named_parameters (): if param . requires_grad : if param_type == self . ParameterType . ParameterModel : param . data . copy_ ( torch . from_numpy ( parameters [ name ])) else : param . grad . copy_ ( torch . from_numpy ( parameters [ name ]))","title":"set()"},{"location":"zh/api/reference/business/homo/sklearn_trainer/","text":"SklearnTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual sklearn trainer. Source code in iflearner/business/homo/sklearn_trainer.py 26 27 28 def __init__ ( self , model : Any ) -> None : self . _model : Any = model super () . __init__ () get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/sklearn_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/sklearn_trainer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"Sklearn trainer"},{"location":"zh/api/reference/business/homo/sklearn_trainer/#iflearner.business.homo.sklearn_trainer.SklearnTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual sklearn trainer. Source code in iflearner/business/homo/sklearn_trainer.py 26 27 28 def __init__ ( self , model : Any ) -> None : self . _model : Any = model super () . __init__ ()","title":"SklearnTrainer"},{"location":"zh/api/reference/business/homo/sklearn_trainer/#iflearner.business.homo.sklearn_trainer.SklearnTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/sklearn_trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass","title":"get()"},{"location":"zh/api/reference/business/homo/sklearn_trainer/#iflearner.business.homo.sklearn_trainer.SklearnTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/sklearn_trainer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"set()"},{"location":"zh/api/reference/business/homo/tensorflow_trainer/","text":"TensorFlowTrainer ( model ) \u00b6 Bases: Trainer implement the 'get' and 'set' function for the usual tensorflow trainer. Source code in iflearner/business/homo/tensorflow_trainer.py 28 29 def __init__ ( self , model : tf . keras . Model ) -> None : self . _model = model get ( param_type = Trainer . ParameterType . ParameterModel ) \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/tensorflow_trainer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters set ( parameters , param_type = Trainer . ParameterType . ParameterModel ) \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/tensorflow_trainer.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"tensorflow_trainer"},{"location":"zh/api/reference/business/homo/tensorflow_trainer/#iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer","text":"Bases: Trainer implement the 'get' and 'set' function for the usual tensorflow trainer. Source code in iflearner/business/homo/tensorflow_trainer.py 28 29 def __init__ ( self , model : tf . keras . Model ) -> None : self . _model = model","title":"TensorFlowTrainer"},{"location":"zh/api/reference/business/homo/tensorflow_trainer/#iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/tensorflow_trainer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get ( self , param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" parameters = dict () for item in self . _model . layers : if item . name is not None : i = 0 for weight in item . get_weights (): parameters [ f \" { item . name } - { i } \" ] = weight i += 1 return parameters","title":"get()"},{"location":"zh/api/reference/business/homo/tensorflow_trainer/#iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type Trainer . ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Trainer.ParameterType.ParameterModel Source code in iflearner/business/homo/tensorflow_trainer.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : Trainer . ParameterType = Trainer . ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" for item in self . _model . layers : if item . name is not None and len ( item . get_weights ()) > 0 : i = 0 weights = [] while True : i_name = f \" { item . name } - { i } \" if i_name in parameters : weights . append ( parameters [ i_name ]) else : break i += 1 item . set_weights ( weights )","title":"set()"},{"location":"zh/api/reference/business/homo/train_client/","text":"Controller ( args , trainer ) \u00b6 Control the training logic of the client. Source code in iflearner/business/homo/train_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , args : argparse . Namespace , trainer : Trainer ) -> None : logger . add ( f \"log/ { args . name } .log\" , backtrace = True , diagnose = True ) self . _args = args self . _trainer = trainer self . _network_client = homo_client . HomoClient ( self . _args . server , self . _args . name , self . _args . cert ) self . _party_name = self . _args . name self . _sum_random_value = 0.0 self . _epoch = 1 self . _local_training = \"LT\" self . _federated_training = \"FT\" self . _local_training_param = None self . _metric = Metric ( logdir = f \"metric/ { self . _args . name } \" ) do_smpc () \u00b6 The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. Source code in iflearner/business/homo/train_client.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def do_smpc ( self ) -> None : \"\"\"The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. \"\"\" if self . _args . peers is None : return peer_list = self . _args . peers . split ( \";\" ) # type: ignore for index in range ( len ( peer_list )): if index == 0 : srv = peer_server . PeerServer ( len ( peer_list ) - 1 ) t = Thread ( target = base_server . start_server , args = ( peer_list [ index ], srv ) ) t . start () else : cli = peer_client . PeerClient ( peer_list [ index ], self . _party_name , self . _args . peer_cert ) public_key = cli . get_DH_public_key () secret = diffie_hellman_inst . DiffieHellmanInst () . generate_secret ( public_key ) logger . info ( f \"secret: { secret } , type: { type ( secret ) } \" ) random_value = cli . get_SMPC_random_key ( secret ) self . _sum_random_value += random_value logger . info ( f \"random value: { random_value } \" ) self . _sum_random_value += srv . sum_parties_random_value () logger . info ( f \"sum all random values: { self . _sum_random_value } \" ) exit () \u00b6 Before exiting, the client needs to save the metrics and notify the server of the client's status. Source code in iflearner/business/homo/train_client.py 88 89 90 91 92 def exit ( self ) -> None : \"\"\"Before exiting, the client needs to save the metrics and notify the server of the client's status.\"\"\" self . _network_client . transport ( message_type . MSG_COMPLETE ) os . _exit ( 0 ) run () \u00b6 start training Source code in iflearner/business/homo/train_client.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def run ( self ) -> None : \"\"\"start training\"\"\" logger . info ( \"register to server\" ) sample_num = self . _trainer . config () . get ( \"sample_num\" , 0 ) batch_num = self . _trainer . config () . get ( \"batch_num\" , 0 ) while True : try : resp = self . _network_client . transport ( message_type . MSG_REGISTER , homo_pb2 . RegistrationInfo ( sample_num = sample_num , step_num = batch_num ), ) break except Exception as e : logger . info ( e ) time . sleep ( 3 ) logger . info ( f \"use strategy: { resp . strategy } \" ) # if resp.parameters: # data_m = dict() # for k, v in resp.parameters.items(): # data_m[k] = homo_pb2.Parameter(shape=v.shape) # data_m[k].values.extend(v.values) # self._global_params = {} # type: ignore # for k, v in data_m.items(): # self._global_params[k] = np.asarray(v.values).reshape(v.shape) # self._trainer.set(self._global_params) # logger.info(f\"load global model.\") self . do_smpc () if resp . strategy == message_type . STRATEGY_FEDAVG : self . _strategy = fedavg_client . FedavgClient () elif resp . strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy = fedavg_client . FedavgClient ( True ) elif resp . strategy == message_type . STRATEGY_FEDOPT : self . _strategy = fedopt_client . FedoptClient () # type: ignore elif resp . strategy == message_type . STRATEGY_qFEDAVG : self . _strategy = qfedavg_client . qFedavgClient () # type: ignore elif resp . strategy == message_type . STRATEGY_FEDNOVA : self . _strategy = fednova_client . FedNovaClient () # type: ignore self . _network_client . set_strategy ( self . _strategy ) t = Thread ( target = self . _network_client . notice ) t . start () logger . info ( \"report client ready\" ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) learning_type = self . _federated_training current_epoch = 0 while True : if ( self . _strategy . current_stage () == StrategyClient . Stage . Training or learning_type == self . _local_training ): logger . info ( f \"----- fit < { learning_type } > -----\" ) if learning_type == self . _local_training : self . _trainer . set ( self . _local_training_param ) # type: ignore current_epoch = self . _epoch - 1 else : current_epoch = self . _epoch try : self . _strategy . set_trainer_config ( self . _trainer . config ()) fit = self . _trainer . fit ( current_epoch ) if isinstance ( fit , types . GeneratorType ): param = next ( fit ) while True : param = self . _strategy . update_param ( param ) param = fit . send ( param ) except StopIteration : logger . info ( \"epoch end\" ) logger . info ( f \"----- evaluate < { learning_type } > -----\" ) metrics = self . _trainer . evaluate ( current_epoch ) if metrics is not None : for k , v in metrics . items (): self . _metric . add ( k , learning_type , current_epoch , v ) if ( learning_type == self . _federated_training and self . _epoch == 1 ): self . _metric . add ( k , self . _local_training , current_epoch , v ) logger . info ( f \"----- get < { learning_type } > -----\" ) client_param = self . _trainer . get () if self . _args . enable_ll : if learning_type == self . _federated_training : if self . _local_training_param is None : self . _local_training_param = client_param # type: ignore else : learning_type = self . _local_training else : learning_type = self . _federated_training self . _local_training_param = client_param # type: ignore if self . _epoch == self . _args . epochs : self . exit () continue upload_param = self . _strategy . generate_upload_param ( self . _epoch , client_param , metrics ) if self . _sum_random_value != 0.0 : smpc_data = dict () for k , v in upload_param . parameters . items (): smpc_data [ k ] = homo_pb2 . Parameter ( shape = v . shape ) # type: ignore smpc_data [ k ] . values . extend ( [ item + self . _sum_random_value for item in v . values ] # type: ignore ) upload_param = homo_pb2 . UploadParam ( epoch = upload_param . epoch , parameters = smpc_data , metrics = metrics ) if self . _epoch == self . _args . epochs : if self . _args . enable_ll : continue else : self . exit () self . _network_client . transport ( message_type . MSG_UPLOAD_PARAM , upload_param ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _epoch += 1 elif self . _strategy . current_stage () == StrategyClient . Stage . Setting : logger . info ( \"----- set -----\" ) self . _global_params = self . _strategy . aggregate_result () self . _trainer . set ( self . _global_params ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) else : time . sleep ( 1 )","title":"train_client"},{"location":"zh/api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller","text":"Control the training logic of the client. Source code in iflearner/business/homo/train_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , args : argparse . Namespace , trainer : Trainer ) -> None : logger . add ( f \"log/ { args . name } .log\" , backtrace = True , diagnose = True ) self . _args = args self . _trainer = trainer self . _network_client = homo_client . HomoClient ( self . _args . server , self . _args . name , self . _args . cert ) self . _party_name = self . _args . name self . _sum_random_value = 0.0 self . _epoch = 1 self . _local_training = \"LT\" self . _federated_training = \"FT\" self . _local_training_param = None self . _metric = Metric ( logdir = f \"metric/ { self . _args . name } \" )","title":"Controller"},{"location":"zh/api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller.do_smpc","text":"The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. Source code in iflearner/business/homo/train_client.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def do_smpc ( self ) -> None : \"\"\"The party generates a value among all parties. For example: Party A is 0.1; Party B is 0.2; and Party C is -0.3. So when aggregated, the sum value is 0. \"\"\" if self . _args . peers is None : return peer_list = self . _args . peers . split ( \";\" ) # type: ignore for index in range ( len ( peer_list )): if index == 0 : srv = peer_server . PeerServer ( len ( peer_list ) - 1 ) t = Thread ( target = base_server . start_server , args = ( peer_list [ index ], srv ) ) t . start () else : cli = peer_client . PeerClient ( peer_list [ index ], self . _party_name , self . _args . peer_cert ) public_key = cli . get_DH_public_key () secret = diffie_hellman_inst . DiffieHellmanInst () . generate_secret ( public_key ) logger . info ( f \"secret: { secret } , type: { type ( secret ) } \" ) random_value = cli . get_SMPC_random_key ( secret ) self . _sum_random_value += random_value logger . info ( f \"random value: { random_value } \" ) self . _sum_random_value += srv . sum_parties_random_value () logger . info ( f \"sum all random values: { self . _sum_random_value } \" )","title":"do_smpc()"},{"location":"zh/api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller.exit","text":"Before exiting, the client needs to save the metrics and notify the server of the client's status. Source code in iflearner/business/homo/train_client.py 88 89 90 91 92 def exit ( self ) -> None : \"\"\"Before exiting, the client needs to save the metrics and notify the server of the client's status.\"\"\" self . _network_client . transport ( message_type . MSG_COMPLETE ) os . _exit ( 0 )","title":"exit()"},{"location":"zh/api/reference/business/homo/train_client/#iflearner.business.homo.train_client.Controller.run","text":"start training Source code in iflearner/business/homo/train_client.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def run ( self ) -> None : \"\"\"start training\"\"\" logger . info ( \"register to server\" ) sample_num = self . _trainer . config () . get ( \"sample_num\" , 0 ) batch_num = self . _trainer . config () . get ( \"batch_num\" , 0 ) while True : try : resp = self . _network_client . transport ( message_type . MSG_REGISTER , homo_pb2 . RegistrationInfo ( sample_num = sample_num , step_num = batch_num ), ) break except Exception as e : logger . info ( e ) time . sleep ( 3 ) logger . info ( f \"use strategy: { resp . strategy } \" ) # if resp.parameters: # data_m = dict() # for k, v in resp.parameters.items(): # data_m[k] = homo_pb2.Parameter(shape=v.shape) # data_m[k].values.extend(v.values) # self._global_params = {} # type: ignore # for k, v in data_m.items(): # self._global_params[k] = np.asarray(v.values).reshape(v.shape) # self._trainer.set(self._global_params) # logger.info(f\"load global model.\") self . do_smpc () if resp . strategy == message_type . STRATEGY_FEDAVG : self . _strategy = fedavg_client . FedavgClient () elif resp . strategy == message_type . STRATEGY_SCAFFOLD : self . _strategy = fedavg_client . FedavgClient ( True ) elif resp . strategy == message_type . STRATEGY_FEDOPT : self . _strategy = fedopt_client . FedoptClient () # type: ignore elif resp . strategy == message_type . STRATEGY_qFEDAVG : self . _strategy = qfedavg_client . qFedavgClient () # type: ignore elif resp . strategy == message_type . STRATEGY_FEDNOVA : self . _strategy = fednova_client . FedNovaClient () # type: ignore self . _network_client . set_strategy ( self . _strategy ) t = Thread ( target = self . _network_client . notice ) t . start () logger . info ( \"report client ready\" ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) learning_type = self . _federated_training current_epoch = 0 while True : if ( self . _strategy . current_stage () == StrategyClient . Stage . Training or learning_type == self . _local_training ): logger . info ( f \"----- fit < { learning_type } > -----\" ) if learning_type == self . _local_training : self . _trainer . set ( self . _local_training_param ) # type: ignore current_epoch = self . _epoch - 1 else : current_epoch = self . _epoch try : self . _strategy . set_trainer_config ( self . _trainer . config ()) fit = self . _trainer . fit ( current_epoch ) if isinstance ( fit , types . GeneratorType ): param = next ( fit ) while True : param = self . _strategy . update_param ( param ) param = fit . send ( param ) except StopIteration : logger . info ( \"epoch end\" ) logger . info ( f \"----- evaluate < { learning_type } > -----\" ) metrics = self . _trainer . evaluate ( current_epoch ) if metrics is not None : for k , v in metrics . items (): self . _metric . add ( k , learning_type , current_epoch , v ) if ( learning_type == self . _federated_training and self . _epoch == 1 ): self . _metric . add ( k , self . _local_training , current_epoch , v ) logger . info ( f \"----- get < { learning_type } > -----\" ) client_param = self . _trainer . get () if self . _args . enable_ll : if learning_type == self . _federated_training : if self . _local_training_param is None : self . _local_training_param = client_param # type: ignore else : learning_type = self . _local_training else : learning_type = self . _federated_training self . _local_training_param = client_param # type: ignore if self . _epoch == self . _args . epochs : self . exit () continue upload_param = self . _strategy . generate_upload_param ( self . _epoch , client_param , metrics ) if self . _sum_random_value != 0.0 : smpc_data = dict () for k , v in upload_param . parameters . items (): smpc_data [ k ] = homo_pb2 . Parameter ( shape = v . shape ) # type: ignore smpc_data [ k ] . values . extend ( [ item + self . _sum_random_value for item in v . values ] # type: ignore ) upload_param = homo_pb2 . UploadParam ( epoch = upload_param . epoch , parameters = smpc_data , metrics = metrics ) if self . _epoch == self . _args . epochs : if self . _args . enable_ll : continue else : self . exit () self . _network_client . transport ( message_type . MSG_UPLOAD_PARAM , upload_param ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _epoch += 1 elif self . _strategy . current_stage () == StrategyClient . Stage . Setting : logger . info ( \"----- set -----\" ) self . _global_params = self . _strategy . aggregate_result () self . _trainer . set ( self . _global_params ) self . _strategy . set_current_stage ( StrategyClient . Stage . Waiting ) self . _network_client . transport ( message_type . MSG_CLIENT_READY , None ) else : time . sleep ( 1 )","title":"run()"},{"location":"zh/api/reference/business/homo/trainer/","text":"Trainer \u00b6 Bases: ABC The base class of trainer. ParameterType \u00b6 Bases: IntEnum Define the type of parameter. config () \u00b6 get training configuration. Returns: Type Description Dict [ str , float ] return a dict, at least including the following keys: Dict [ str , float ] learning_rate Dict [ str , float ] batch_num Dict [ str , float ] sample_num Source code in iflearner/business/homo/trainer.py 64 65 66 67 68 69 70 71 72 73 def config ( self ) -> Dict [ str , float ]: \"\"\"get training configuration. Returns: return a dict, at least including the following keys: learning_rate batch_num sample_num \"\"\" return dict () evaluate ( epoch ) abstractmethod \u00b6 evaluate model and return metrics. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description Dict [ str , float ] dict, k: str (metric name), v: float (metric value) Source code in iflearner/business/homo/trainer.py 87 88 89 90 91 92 93 94 95 96 97 @abstractmethod def evaluate ( self , epoch : int ) -> Dict [ str , float ]: \"\"\"evaluate model and return metrics. Args: epoch: the current index of epoch Returns: dict, k: str (metric name), v: float (metric value) \"\"\" pass fit ( epoch ) abstractmethod \u00b6 fit model on one epoch. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description None None Source code in iflearner/business/homo/trainer.py 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def fit ( self , epoch : int ) -> None : \"\"\"fit model on one epoch. Args: epoch: the current index of epoch Returns: None \"\"\" pass get ( param_type = ParameterType . ParameterModel ) abstractmethod \u00b6 get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/trainer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def get ( self , param_type : ParameterType = ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass set ( parameters , param_type = ParameterType . ParameterModel ) abstractmethod \u00b6 set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Source code in iflearner/business/homo/trainer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @abstractmethod def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : ParameterType = ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"trainer"},{"location":"zh/api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer","text":"Bases: ABC The base class of trainer.","title":"Trainer"},{"location":"zh/api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.ParameterType","text":"Bases: IntEnum Define the type of parameter.","title":"ParameterType"},{"location":"zh/api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.config","text":"get training configuration. Returns: Type Description Dict [ str , float ] return a dict, at least including the following keys: Dict [ str , float ] learning_rate Dict [ str , float ] batch_num Dict [ str , float ] sample_num Source code in iflearner/business/homo/trainer.py 64 65 66 67 68 69 70 71 72 73 def config ( self ) -> Dict [ str , float ]: \"\"\"get training configuration. Returns: return a dict, at least including the following keys: learning_rate batch_num sample_num \"\"\" return dict ()","title":"config()"},{"location":"zh/api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.evaluate","text":"evaluate model and return metrics. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description Dict [ str , float ] dict, k: str (metric name), v: float (metric value) Source code in iflearner/business/homo/trainer.py 87 88 89 90 91 92 93 94 95 96 97 @abstractmethod def evaluate ( self , epoch : int ) -> Dict [ str , float ]: \"\"\"evaluate model and return metrics. Args: epoch: the current index of epoch Returns: dict, k: str (metric name), v: float (metric value) \"\"\" pass","title":"evaluate()"},{"location":"zh/api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.fit","text":"fit model on one epoch. Parameters: Name Type Description Default epoch int the current index of epoch required Returns: Type Description None None Source code in iflearner/business/homo/trainer.py 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def fit ( self , epoch : int ) -> None : \"\"\"fit model on one epoch. Args: epoch: the current index of epoch Returns: None \"\"\" pass","title":"fit()"},{"location":"zh/api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.get","text":"get parameters form the client, maybe the model parameter or gradient. Parameters: Name Type Description Default param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] dict, k: str (the parameter name), v: np.ndarray (the parameter value) Source code in iflearner/business/homo/trainer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def get ( self , param_type : ParameterType = ParameterType . ParameterModel ) -> Dict [ str , npt . NDArray [ np . float32 ]]: # type: ignore \"\"\"get parameters form the client, maybe the model parameter or gradient. Args: param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: dict, k: str (the parameter name), v: np.ndarray (the parameter value) \"\"\" pass","title":"get()"},{"location":"zh/api/reference/business/homo/trainer/#iflearner.business.homo.trainer.Trainer.set","text":"set parameters to the client, maybe the model parameter or gradient. Parameters: Name Type Description Default parameters Dict [ str , npt . NDArray [ np . float32 ]] Parameters is the same as the return of 'get' function. required param_type ParameterType Param_type is ParameterModel or ParameterGradient, default is ParameterModel. ParameterType.ParameterModel Source code in iflearner/business/homo/trainer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @abstractmethod def set ( self , parameters : Dict [ str , npt . NDArray [ np . float32 ]], # type: ignore param_type : ParameterType = ParameterType . ParameterModel , ) -> None : \"\"\"set parameters to the client, maybe the model parameter or gradient. Args: parameters: Parameters is the same as the return of 'get' function. param_type: Param_type is ParameterModel or ParameterGradient, default is ParameterModel. Returns: None \"\"\" pass","title":"set()"},{"location":"zh/api/reference/business/homo/strategy/","text":"FedAdagrad ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedAdam ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedYogi ( learning_rate = 0.1 , betas = ( 0.9 , 0.99 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedavgServer ( num_clients , total_epoch , scaffold = False , weighted_fedavg = False ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {} FedoptServer ( num_clients , total_epoch , opt ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" ) qFedavgServer ( num_clients , total_epoch , q = 1 , learning_rate = 0.1 ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {} norm_grad ( grad ) \u00b6 normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad step ( deltas , hs ) \u00b6 a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"Index"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedAdagrad","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdagrad"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedAdam","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdam"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.opt.fedadam.FedAdam.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedYogi","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedYogi"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {}","title":"FedavgServer"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.FedoptServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" )","title":"FedoptServer"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.qFedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {}","title":"qFedavgServer"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.norm_grad","text":"normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad","title":"norm_grad()"},{"location":"zh/api/reference/business/homo/strategy/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.step","text":"a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/fedavg_client/","text":"FedavgClient ( scaffold = False ) \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of fedavg on client side. Source code in iflearner/business/homo/strategy/fedavg_client.py 27 28 29 def __init__ ( self , scaffold : bool = False ) -> None : super () . __init__ () self . _scaffold = scaffold","title":"fedavg_client"},{"location":"zh/api/reference/business/homo/strategy/fedavg_client/#iflearner.business.homo.strategy.fedavg_client.FedavgClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of fedavg on client side. Source code in iflearner/business/homo/strategy/fedavg_client.py 27 28 29 def __init__ ( self , scaffold : bool = False ) -> None : super () . __init__ () self . _scaffold = scaffold","title":"FedavgClient"},{"location":"zh/api/reference/business/homo/strategy/fedavg_server/","text":"FedavgServer ( num_clients , total_epoch , scaffold = False , weighted_fedavg = False ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {}","title":"fedavg_server"},{"location":"zh/api/reference/business/homo/strategy/fedavg_server/#iflearner.business.homo.strategy.fedavg_server.FedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning scaffold bool if use the scaffold method. Defaults to False. weighted_fedavg bool if use the weighted sum. Defaults to False. Source code in iflearner/business/homo/strategy/fedavg_server.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , num_clients : int , total_epoch : int , scaffold : bool = False , weighted_fedavg : bool = False , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _scaffold = scaffold self . _weighted_fedavg = weighted_fedavg logger . info ( f \"num_clients: { self . _num_clients } , scaffold: { self . _scaffold } , weighted_fedavg: { self . _weighted_fedavg } \" ) self . _clients_samples : dict = {}","title":"FedavgServer"},{"location":"zh/api/reference/business/homo/strategy/fednova_client/","text":"FedNovaClient () \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of fednova on client side. Source code in iflearner/business/homo/strategy/fednova_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"fednova_client"},{"location":"zh/api/reference/business/homo/strategy/fednova_client/#iflearner.business.homo.strategy.fednova_client.FedNovaClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of fednova on client side. Source code in iflearner/business/homo/strategy/fednova_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"FedNovaClient"},{"location":"zh/api/reference/business/homo/strategy/fednova_server/","text":"FedNovaServer ( num_clients , total_epoch ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fednova on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning _clients_samples dict samples of each client _step_nums dict step numbers for each client to optimize its model Source code in iflearner/business/homo/strategy/fednova_server.py 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , num_clients : int , total_epoch : int , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _clients_samples : dict = {} self . _step_nums : dict = {}","title":"fednova_server"},{"location":"zh/api/reference/business/homo/strategy/fednova_server/#iflearner.business.homo.strategy.fednova_server.FedNovaServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fednova on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning _clients_samples dict samples of each client _step_nums dict step numbers for each client to optimize its model Source code in iflearner/business/homo/strategy/fednova_server.py 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , num_clients : int , total_epoch : int , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _clients_samples : dict = {} self . _step_nums : dict = {}","title":"FedNovaServer"},{"location":"zh/api/reference/business/homo/strategy/fedopt_client/","text":"FedoptClient () \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of fedopt on client side. Source code in iflearner/business/homo/strategy/fedopt_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"fedopt_client"},{"location":"zh/api/reference/business/homo/strategy/fedopt_client/#iflearner.business.homo.strategy.fedopt_client.FedoptClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of fedopt on client side. Source code in iflearner/business/homo/strategy/fedopt_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"FedoptClient"},{"location":"zh/api/reference/business/homo/strategy/fedopt_server/","text":"FedoptServer ( num_clients , total_epoch , opt ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" )","title":"fedopt_server"},{"location":"zh/api/reference/business/homo/strategy/fedopt_server/#iflearner.business.homo.strategy.fedopt_server.FedoptServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of fedopt on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning opt FedOpt the FedOpt method, which is in FedAdam, FedAdagrad, FedYogi or FedAvgM Source code in iflearner/business/homo/strategy/fedopt_server.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , num_clients : int , total_epoch : int , opt : FedOpt , ) -> None : super () . __init__ ( num_clients , total_epoch ) self . _opt = opt logger . info ( f \"num_clients: { self . _num_clients } , opt: { type ( opt ) . __name__ } \" )","title":"FedoptServer"},{"location":"zh/api/reference/business/homo/strategy/qfedavg_client/","text":"qFedavgClient () \u00b6 Bases: strategy_client . StrategyClient Implement the strategy of qfedavg on client side. Source code in iflearner/business/homo/strategy/qfedavg_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"qfedavg_client"},{"location":"zh/api/reference/business/homo/strategy/qfedavg_client/#iflearner.business.homo.strategy.qfedavg_client.qFedavgClient","text":"Bases: strategy_client . StrategyClient Implement the strategy of qfedavg on client side. Source code in iflearner/business/homo/strategy/qfedavg_client.py 21 22 def __init__ ( self ) -> None : super () . __init__ ()","title":"qFedavgClient"},{"location":"zh/api/reference/business/homo/strategy/qfedavg_server/","text":"qFedavgServer ( num_clients , total_epoch , q = 1 , learning_rate = 0.1 ) \u00b6 Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {} norm_grad ( grad ) \u00b6 normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad step ( deltas , hs ) \u00b6 a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"qfedavg_server"},{"location":"zh/api/reference/business/homo/strategy/qfedavg_server/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer","text":"Bases: strategy_server . StrategyServer Implement the strategy of qFedavg on server side. Attributes: Name Type Description num_clients int client number total_epoch int the epoch number of client trainning q float q factor. Defaults to 1. learning_rate float learning rate. Defaults to 0.1. _fs dict loss values of each client Source code in iflearner/business/homo/strategy/qfedavg_server.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , num_clients : int , total_epoch : int , q : float = 1 , learning_rate : float = 0.1 , ) -> None : super () . __init__ ( num_clients , total_epoch ) logger . info ( f \"num_clients: { self . _num_clients } , strategy: qFedavg\" ) self . _q = q self . _lr = learning_rate self . _params : dict = {} self . _fs : dict = {}","title":"qFedavgServer"},{"location":"zh/api/reference/business/homo/strategy/qfedavg_server/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.norm_grad","text":"normalize the grad. Parameters: Name Type Description Default grad Dict [ str , Dict ] grad required Returns: Name Type Description _type_ the normalized grad Source code in iflearner/business/homo/strategy/qfedavg_server.py 109 110 111 112 113 114 115 116 117 118 119 120 121 def norm_grad ( self , grad : Dict [ str , Dict ]): \"\"\"normalize the grad. Args: grad (Dict[str, Dict]): grad Returns: _type_: the normalized grad \"\"\" sum_grad = 0 for v in grad . values (): sum_grad += np . sum ( np . square ( v )) # type: ignore return sum_grad","title":"norm_grad()"},{"location":"zh/api/reference/business/homo/strategy/qfedavg_server/#iflearner.business.homo.strategy.qfedavg_server.qFedavgServer.step","text":"a optimized step for deltas. Parameters: Name Type Description Default deltas Dict [ str , Dict ] the delta of model parameters required hs Dict [ str , float ] demominator required Returns: Name Type Description _type_ new parameters after optimizing the deltas Source code in iflearner/business/homo/strategy/qfedavg_server.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def step ( self , deltas : Dict [ str , Dict ], hs : Dict [ str , float ]): \"\"\"a optimized step for deltas. Args: deltas (Dict[str, Dict]): the delta of model parameters hs (Dict[str, float]): demominator Returns: _type_: new parameters after optimizing the deltas \"\"\" demominator = sum ( hs . values ()) updates : dict = {} for client_delta in deltas . values (): for param_name , param in client_delta . items (): updates [ param_name ] = updates . get ( param_name , 0 ) + param / demominator new_param = {} for param_name , param in self . _params . items (): new_param [ param_name ] = param . reshape (( - 1 )) - updates [ param_name ] self . _params [ param_name ] = new_param [ param_name ] . reshape ( param . shape ) return new_param","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/","text":"StrategyClient () \u00b6 Bases: ABC Implement the strategy of client. Attributes: Name Type Description _custom_handlers Dict [ str , Any ] custom handlers _trainer_config Dict [ str , Any ] the trainer config of client _current_stage Stage current stage of client _aggregate_result aggregate model parameters result with grpc format _aggregate_result_np aggregate model parameters result with numpy format _aggregate_c None _smpc bool use smpc to start federated learning if _smpc is True _sum_random_value random value using in smpc Source code in iflearner/business/homo/strategy/strategy_client.py 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () self . _current_stage = self . Stage . Waiting self . _aggregate_result = None self . _aggregate_result_np = None self . _aggregate_c = None self . _local_c = None self . _local_c_initial = None self . _smpc = False self . _sum_random_value = 0.0 self . _gradient_suffix = \"_gradient\" Stage \u00b6 Bases: IntEnum Enum the stage of client. aggregate_result () \u00b6 get the aggregated model parameters. Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the aggregated model parameters of grpc format Source code in iflearner/business/homo/strategy/strategy_client.py 76 77 78 79 80 81 82 def aggregate_result ( self ) -> homo_pb2 . AggregateResult : \"\"\"get the aggregated model parameters. Returns: homo_pb2.AggregateResult: the aggregated model parameters of grpc format \"\"\" return self . _aggregate_result_np current_stage () \u00b6 the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Name Type Description Stage Stage the current stage Source code in iflearner/business/homo/strategy/strategy_client.py 150 151 152 153 154 155 156 def current_stage ( self ) -> Stage : \"\"\"the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Stage: the current stage \"\"\" return self . _current_stage generate_registration_info () \u00b6 Generate the message of MSG_REGISTER. Source code in iflearner/business/homo/strategy/strategy_client.py 72 73 74 def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass generate_upload_param ( epoch , data , metrics = None ) \u00b6 Generate the message of MSG_UPLOAD_PARAM. Parameters: Name Type Description Default epoch int Current epoch of number of client training. required data Dict [ Any , Any ] The data that will be uploaded to server. required metrics Dict [ str , Any ] The client metrics. None Returns: Name Type Description Any Any The grpc format data that can be send to server Source code in iflearner/business/homo/strategy/strategy_client.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ], metrics : Dict [ str , float ] = None , ) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM. Args: epoch (int): Current epoch of number of client training. data (Dict[Any, Any]): The data that will be uploaded to server. metrics (Dict[str, Any]): The client metrics. Returns: Any: The grpc format data that can be send to server \"\"\" pb_params = dict () for k , v in data . items (): pb_params [ k ] = homo_pb2 . Parameter ( values = v . ravel (), shape = v . shape ) data = homo_pb2 . UploadParam ( epoch = epoch , parameters = pb_params , metrics = metrics ) return data handler_aggregate_result ( data ) \u00b6 Handle the message of MSG_AGGREGATE_RESULT from the server. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Source code in iflearner/business/homo/strategy/strategy_client.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server. Args: data (homo_pb2.AggregateResult): the aggregated result from server \"\"\" data_m = dict () data_c = dict () for k , v in data . parameters . items (): if k . endswith ( self . _gradient_suffix ): data_c [ k . replace ( self . _gradient_suffix , \"\" )] = homo_pb2 . Parameter ( shape = v . shape ) data_c [ k . replace ( self . _gradient_suffix , \"\" )] . values . extend ( v . values ) else : data_m [ k ] = homo_pb2 . Parameter ( shape = v . shape ) data_m [ k ] . values . extend ( v . values ) self . _aggregate_result = homo_pb2 . AggregateResult ( parameters = data_m ) self . _aggregate_result_np = {} # type: ignore for k , v in data_m . items (): self . _aggregate_result_np [ k ] = np . asarray ( v . values ) . reshape ( v . shape ) # type: ignore self . _aggregate_c = homo_pb2 . AggregateResult ( parameters = data_c ) self . _current_stage = self . Stage . Setting handler_notify_training () \u00b6 Handle the message of MSG_NOTIFY_TRAINING from the server. Source code in iflearner/business/homo/strategy/strategy_client.py 146 147 148 def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" self . _current_stage = self . Stage . Training set_current_stage ( stage ) \u00b6 set current stage. Source code in iflearner/business/homo/strategy/strategy_client.py 158 159 160 161 def set_current_stage ( self , stage : Stage ) -> None : \"\"\"set current stage.\"\"\" self . _current_stage = stage set_global_param ( param ) \u00b6 set global parameters. Parameters: Name Type Description Default param Dict [ str , Any ] parameters required Source code in iflearner/business/homo/strategy/strategy_client.py 163 164 165 166 167 168 169 170 def set_global_param ( self , param : Dict [ str , Any ]) -> None : \"\"\"set global parameters. Args: param (Dict[str, Any]): parameters \"\"\" self . _global_param = param set_trainer_config ( config ) \u00b6 set trainer config. Parameters: Name Type Description Default config Dict [ str , Any ] the config of client Trainer required Source code in iflearner/business/homo/strategy/strategy_client.py 64 65 66 67 68 69 70 def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : \"\"\"set trainer config. Args: config (Dict[str, Any]): the config of client Trainer \"\"\" self . _trainer_config = config update_param ( data ) \u00b6 Update the parameter during training. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the updated result Source code in iflearner/business/homo/strategy/strategy_client.py 108 109 110 111 112 113 114 115 116 117 def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training. Args: data (homo_pb2.AggregateResult): the aggregated result from server Returns: homo_pb2.AggregateResult: the updated result \"\"\" pass","title":"strategy_client"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient","text":"Bases: ABC Implement the strategy of client. Attributes: Name Type Description _custom_handlers Dict [ str , Any ] custom handlers _trainer_config Dict [ str , Any ] the trainer config of client _current_stage Stage current stage of client _aggregate_result aggregate model parameters result with grpc format _aggregate_result_np aggregate model parameters result with numpy format _aggregate_c None _smpc bool use smpc to start federated learning if _smpc is True _sum_random_value random value using in smpc Source code in iflearner/business/homo/strategy/strategy_client.py 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () self . _current_stage = self . Stage . Waiting self . _aggregate_result = None self . _aggregate_result_np = None self . _aggregate_c = None self . _local_c = None self . _local_c_initial = None self . _smpc = False self . _sum_random_value = 0.0 self . _gradient_suffix = \"_gradient\"","title":"StrategyClient"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.Stage","text":"Bases: IntEnum Enum the stage of client.","title":"Stage"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.aggregate_result","text":"get the aggregated model parameters. Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the aggregated model parameters of grpc format Source code in iflearner/business/homo/strategy/strategy_client.py 76 77 78 79 80 81 82 def aggregate_result ( self ) -> homo_pb2 . AggregateResult : \"\"\"get the aggregated model parameters. Returns: homo_pb2.AggregateResult: the aggregated model parameters of grpc format \"\"\" return self . _aggregate_result_np","title":"aggregate_result()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.current_stage","text":"the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Name Type Description Stage Stage the current stage Source code in iflearner/business/homo/strategy/strategy_client.py 150 151 152 153 154 155 156 def current_stage ( self ) -> Stage : \"\"\"the current stage, which is in Waiting, Trainning or Settinh stage. Returns: Stage: the current stage \"\"\" return self . _current_stage","title":"current_stage()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.generate_registration_info","text":"Generate the message of MSG_REGISTER. Source code in iflearner/business/homo/strategy/strategy_client.py 72 73 74 def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass","title":"generate_registration_info()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.generate_upload_param","text":"Generate the message of MSG_UPLOAD_PARAM. Parameters: Name Type Description Default epoch int Current epoch of number of client training. required data Dict [ Any , Any ] The data that will be uploaded to server. required metrics Dict [ str , Any ] The client metrics. None Returns: Name Type Description Any Any The grpc format data that can be send to server Source code in iflearner/business/homo/strategy/strategy_client.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ], metrics : Dict [ str , float ] = None , ) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM. Args: epoch (int): Current epoch of number of client training. data (Dict[Any, Any]): The data that will be uploaded to server. metrics (Dict[str, Any]): The client metrics. Returns: Any: The grpc format data that can be send to server \"\"\" pb_params = dict () for k , v in data . items (): pb_params [ k ] = homo_pb2 . Parameter ( values = v . ravel (), shape = v . shape ) data = homo_pb2 . UploadParam ( epoch = epoch , parameters = pb_params , metrics = metrics ) return data","title":"generate_upload_param()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.handler_aggregate_result","text":"Handle the message of MSG_AGGREGATE_RESULT from the server. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Source code in iflearner/business/homo/strategy/strategy_client.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server. Args: data (homo_pb2.AggregateResult): the aggregated result from server \"\"\" data_m = dict () data_c = dict () for k , v in data . parameters . items (): if k . endswith ( self . _gradient_suffix ): data_c [ k . replace ( self . _gradient_suffix , \"\" )] = homo_pb2 . Parameter ( shape = v . shape ) data_c [ k . replace ( self . _gradient_suffix , \"\" )] . values . extend ( v . values ) else : data_m [ k ] = homo_pb2 . Parameter ( shape = v . shape ) data_m [ k ] . values . extend ( v . values ) self . _aggregate_result = homo_pb2 . AggregateResult ( parameters = data_m ) self . _aggregate_result_np = {} # type: ignore for k , v in data_m . items (): self . _aggregate_result_np [ k ] = np . asarray ( v . values ) . reshape ( v . shape ) # type: ignore self . _aggregate_c = homo_pb2 . AggregateResult ( parameters = data_c ) self . _current_stage = self . Stage . Setting","title":"handler_aggregate_result()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.handler_notify_training","text":"Handle the message of MSG_NOTIFY_TRAINING from the server. Source code in iflearner/business/homo/strategy/strategy_client.py 146 147 148 def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" self . _current_stage = self . Stage . Training","title":"handler_notify_training()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.set_current_stage","text":"set current stage. Source code in iflearner/business/homo/strategy/strategy_client.py 158 159 160 161 def set_current_stage ( self , stage : Stage ) -> None : \"\"\"set current stage.\"\"\" self . _current_stage = stage","title":"set_current_stage()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.set_global_param","text":"set global parameters. Parameters: Name Type Description Default param Dict [ str , Any ] parameters required Source code in iflearner/business/homo/strategy/strategy_client.py 163 164 165 166 167 168 169 170 def set_global_param ( self , param : Dict [ str , Any ]) -> None : \"\"\"set global parameters. Args: param (Dict[str, Any]): parameters \"\"\" self . _global_param = param","title":"set_global_param()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.set_trainer_config","text":"set trainer config. Parameters: Name Type Description Default config Dict [ str , Any ] the config of client Trainer required Source code in iflearner/business/homo/strategy/strategy_client.py 64 65 66 67 68 69 70 def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : \"\"\"set trainer config. Args: config (Dict[str, Any]): the config of client Trainer \"\"\" self . _trainer_config = config","title":"set_trainer_config()"},{"location":"zh/api/reference/business/homo/strategy/strategy_client/#iflearner.business.homo.strategy.strategy_client.StrategyClient.update_param","text":"Update the parameter during training. Parameters: Name Type Description Default data homo_pb2 . AggregateResult the aggregated result from server required Returns: Type Description homo_pb2 . AggregateResult homo_pb2.AggregateResult: the updated result Source code in iflearner/business/homo/strategy/strategy_client.py 108 109 110 111 112 113 114 115 116 117 def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training. Args: data (homo_pb2.AggregateResult): the aggregated result from server Returns: homo_pb2.AggregateResult: the updated result \"\"\" pass","title":"update_param()"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/","text":"StrategyServer ( num_clients , total_epoch ) \u00b6 Bases: ABC Implement the strategy of server. Parameters: Name Type Description Default num_clients int client number required total_epoch int the epoch number of client trainning required Attributes: Name Type Description _num_clients int) client number _total_epoch int the epoch number of client trainning _custom_handlers Dict [ str , Any ] _complete_num int the number of client that has completed federated learning _clients Dict[str, ClientStatus]) a dict storage the client status _training_clients dict) a dict storage the training clients _server_param dict the server model _ready_num int the number of ready client _uploaded_num int the number of client that has uploaded its model parameters _aggregated_num int the number of client that has aggregated its model parameters _on_aggregating bool whether the server is in aggregating stage _params dict the server model parameters Source code in iflearner/business/homo/strategy/strategy_server.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , num_clients , total_epoch ) -> None : self . _num_clients = num_clients self . _total_epoch = total_epoch self . _custom_handlers : Dict [ str , Any ] = dict () self . _complete_num : int = 0 self . _clients : Dict [ str , ClientStatus ] = {} self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _params : dict = {} self . _metric = Metric ( logdir = \"metric\" ) clients_to_json () \u00b6 save clients to json file. Returns: Name Type Description str str json string Source code in iflearner/business/homo/strategy/strategy_server.py 85 86 87 88 89 90 91 92 93 94 95 96 97 def clients_to_json ( self ) -> str : \"\"\"save clients to json file. Returns: str: json string \"\"\" tmp = dict () for k , v in self . _clients . items (): print ( k , v ) tmp [ k ] = v . __dict__ return json . dumps ( tmp ) get_client_notification ( party_name ) \u00b6 Get the notification information of the specified client. Parameters: Name Type Description Default party_name str client name required Returns: Type Description Tuple [ str , Any ] Tuple[str, Any]: the notification message type and notification data Source code in iflearner/business/homo/strategy/strategy_server.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client. Args: party_name (str): client name Returns: Tuple[str, Any]: the notification message type and notification data \"\"\" if party_name in self . _training_clients : if self . _on_aggregating : if not self . _training_clients [ party_name ] . get ( \"aggregating\" , False ): self . _training_clients [ party_name ][ \"aggregating\" ] = True result = homo_pb2 . AggregateResult ( parameters = self . _server_param ) self . _aggregated_num += 1 if self . _aggregated_num == self . _num_clients : self . _aggregated_num = 0 self . _on_aggregating = False self . _training_clients . clear () return message_type . MSG_AGGREGATE_RESULT , result elif not self . _training_clients [ party_name ] . get ( \"training\" , False ): self . _training_clients [ party_name ][ \"training\" ] = True return message_type . MSG_NOTIFY_TRAINING , None return \"\" , None handler_client_ready ( party_name ) \u00b6 Handle the message of MSG_CLIENT_READY from the client. Source code in iflearner/business/homo/strategy/strategy_server.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" logger . info ( f \"Client ready: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _clients [ party_name ] . ready = True self . _clients [ party_name ] . current_epoch += 1 self . _ready_num += 1 if self . _ready_num == self . _num_clients : logger . info ( \"Clients are all ready.\" ) self . _ready_num = 0 for k in self . _clients . keys (): self . _training_clients [ k ] = dict () handler_complete ( party_name ) \u00b6 Handle the message of MSG_COMPLETE from the client. Parameters: Name Type Description Default party_name str client name required Raises: Type Description HomoException if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def handler_complete ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_COMPLETE from the client. Args: party_name (str): client name Raises: HomoException: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client complete: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _complete_num += 1 self . _clients [ party_name ] . complete = True handler_register ( party_name , sample_num = 0 , step_num = 0 ) \u00b6 Handle the message of MSG_REGISTER from the client. Parameters: Name Type Description Default party_name str client name required sample_num Optional [ int ] the total sample number of client party_name . Defaults to 0. 0 step_num int The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. 0 Raises: Type Description HomoException description Returns: Type Description homo_pb2 . RegistrationResponse homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def handler_register ( self , party_name : str , sample_num : Optional [ int ] = 0 , step_num : int = 0 ) -> homo_pb2 . RegistrationResponse : \"\"\"Handle the message of MSG_REGISTER from the client. Args: party_name (str): client name sample_num (Optional[int], optional): the total sample number of client `party_name` . Defaults to 0. step_num (int, optional): The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. Raises: HomoException: _description_ Returns: homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client register: { party_name } \" ) if len ( self . _clients ) >= self . _num_clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Registered clients are full.\" , ) self . _clients [ party_name ] = ClientStatus ( self . _total_epoch ) handler_upload_param ( party_name , data ) \u00b6 Handle the message of MSG_UPLOAD_PARAM from the client. Parameters: Name Type Description Default party_name str client name required data homo_pb2 . UploadParam the data uploaded from party_name , with grpc format required Raises: Type Description HomoException if party_name not in the training_clients list, raise the Forbidden error Source code in iflearner/business/homo/strategy/strategy_server.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client. Args: party_name (str): client name data (homo_pb2.UploadParam): the data uploaded from `party_name`, with grpc format Raises: HomoException: if party_name not in the training_clients list, raise the Forbidden error \"\"\" logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _params is None : self . _params = dict () for param_name , param_info in data . parameters . items (): self . _params [ param_name ] = np . array ( param_info . values ) . reshape ( param_info . shape ) if data . metrics is not None : for k , v in data . metrics . items (): self . _metric . add ( k , party_name , data . epoch , v )","title":"strategy_server"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer","text":"Bases: ABC Implement the strategy of server. Parameters: Name Type Description Default num_clients int client number required total_epoch int the epoch number of client trainning required Attributes: Name Type Description _num_clients int) client number _total_epoch int the epoch number of client trainning _custom_handlers Dict [ str , Any ] _complete_num int the number of client that has completed federated learning _clients Dict[str, ClientStatus]) a dict storage the client status _training_clients dict) a dict storage the training clients _server_param dict the server model _ready_num int the number of ready client _uploaded_num int the number of client that has uploaded its model parameters _aggregated_num int the number of client that has aggregated its model parameters _on_aggregating bool whether the server is in aggregating stage _params dict the server model parameters Source code in iflearner/business/homo/strategy/strategy_server.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , num_clients , total_epoch ) -> None : self . _num_clients = num_clients self . _total_epoch = total_epoch self . _custom_handlers : Dict [ str , Any ] = dict () self . _complete_num : int = 0 self . _clients : Dict [ str , ClientStatus ] = {} self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _params : dict = {} self . _metric = Metric ( logdir = \"metric\" )","title":"StrategyServer"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.clients_to_json","text":"save clients to json file. Returns: Name Type Description str str json string Source code in iflearner/business/homo/strategy/strategy_server.py 85 86 87 88 89 90 91 92 93 94 95 96 97 def clients_to_json ( self ) -> str : \"\"\"save clients to json file. Returns: str: json string \"\"\" tmp = dict () for k , v in self . _clients . items (): print ( k , v ) tmp [ k ] = v . __dict__ return json . dumps ( tmp )","title":"clients_to_json()"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.get_client_notification","text":"Get the notification information of the specified client. Parameters: Name Type Description Default party_name str client name required Returns: Type Description Tuple [ str , Any ] Tuple[str, Any]: the notification message type and notification data Source code in iflearner/business/homo/strategy/strategy_server.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client. Args: party_name (str): client name Returns: Tuple[str, Any]: the notification message type and notification data \"\"\" if party_name in self . _training_clients : if self . _on_aggregating : if not self . _training_clients [ party_name ] . get ( \"aggregating\" , False ): self . _training_clients [ party_name ][ \"aggregating\" ] = True result = homo_pb2 . AggregateResult ( parameters = self . _server_param ) self . _aggregated_num += 1 if self . _aggregated_num == self . _num_clients : self . _aggregated_num = 0 self . _on_aggregating = False self . _training_clients . clear () return message_type . MSG_AGGREGATE_RESULT , result elif not self . _training_clients [ party_name ] . get ( \"training\" , False ): self . _training_clients [ party_name ][ \"training\" ] = True return message_type . MSG_NOTIFY_TRAINING , None return \"\" , None","title":"get_client_notification()"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_client_ready","text":"Handle the message of MSG_CLIENT_READY from the client. Source code in iflearner/business/homo/strategy/strategy_server.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" logger . info ( f \"Client ready: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _clients [ party_name ] . ready = True self . _clients [ party_name ] . current_epoch += 1 self . _ready_num += 1 if self . _ready_num == self . _num_clients : logger . info ( \"Clients are all ready.\" ) self . _ready_num = 0 for k in self . _clients . keys (): self . _training_clients [ k ] = dict ()","title":"handler_client_ready()"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_complete","text":"Handle the message of MSG_COMPLETE from the client. Parameters: Name Type Description Default party_name str client name required Raises: Type Description HomoException if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def handler_complete ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_COMPLETE from the client. Args: party_name (str): client name Raises: HomoException: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client complete: { party_name } \" ) if party_name not in self . _clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Unregistered client.\" ) self . _complete_num += 1 self . _clients [ party_name ] . complete = True","title":"handler_complete()"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_register","text":"Handle the message of MSG_REGISTER from the client. Parameters: Name Type Description Default party_name str client name required sample_num Optional [ int ] the total sample number of client party_name . Defaults to 0. 0 step_num int The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. 0 Raises: Type Description HomoException description Returns: Type Description homo_pb2 . RegistrationResponse homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error Source code in iflearner/business/homo/strategy/strategy_server.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def handler_register ( self , party_name : str , sample_num : Optional [ int ] = 0 , step_num : int = 0 ) -> homo_pb2 . RegistrationResponse : \"\"\"Handle the message of MSG_REGISTER from the client. Args: party_name (str): client name sample_num (Optional[int], optional): the total sample number of client `party_name` . Defaults to 0. step_num (int, optional): The number a client epoch needs to be optimized, always equals to the batch number of client. Defaults to 0. Raises: HomoException: _description_ Returns: homo_pb2.RegistrationResponse: if party_name not in the register list, raise the Unauthorized error \"\"\" logger . info ( f \"Client register: { party_name } \" ) if len ( self . _clients ) >= self . _num_clients : raise HomoException ( HomoException . HomoResponseCode . Unauthorized , \"Registered clients are full.\" , ) self . _clients [ party_name ] = ClientStatus ( self . _total_epoch )","title":"handler_register()"},{"location":"zh/api/reference/business/homo/strategy/strategy_server/#iflearner.business.homo.strategy.strategy_server.StrategyServer.handler_upload_param","text":"Handle the message of MSG_UPLOAD_PARAM from the client. Parameters: Name Type Description Default party_name str client name required data homo_pb2 . UploadParam the data uploaded from party_name , with grpc format required Raises: Type Description HomoException if party_name not in the training_clients list, raise the Forbidden error Source code in iflearner/business/homo/strategy/strategy_server.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client. Args: party_name (str): client name data (homo_pb2.UploadParam): the data uploaded from `party_name`, with grpc format Raises: HomoException: if party_name not in the training_clients list, raise the Forbidden error \"\"\" logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _params is None : self . _params = dict () for param_name , param_info in data . parameters . items (): self . _params [ param_name ] = np . array ( param_info . values ) . reshape ( param_info . shape ) if data . metrics is not None : for k , v in data . metrics . items (): self . _metric . add ( k , party_name , data . epoch , v )","title":"handler_upload_param()"},{"location":"zh/api/reference/business/homo/strategy/opt/","text":"FedAdagrad ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedAdam ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedAvgm ( learning_rate = 1 , momentum = 0.0 ) \u00b6 Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params FedYogi ( learning_rate = 0.1 , betas = ( 0.9 , 0.99 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"Index"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedAdagrad","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdagrad"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedAdam","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdam"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedadam.FedAdam.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedAvgm","text":"Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum","title":"FedAvgm"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedavgm.FedAvgm.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.FedYogi","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedYogi"},{"location":"zh/api/reference/business/homo/strategy/opt/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/fedadagrad/","text":"FedAdagrad ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedadagrad"},{"location":"zh/api/reference/business/homo/strategy/opt/fedadagrad/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdagrad"},{"location":"zh/api/reference/business/homo/strategy/opt/fedadagrad/#iflearner.business.homo.strategy.opt.fedadagrad.FedAdagrad.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadagrad.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] + np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/fedadam/","text":"FedAdam ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedadam"},{"location":"zh/api/reference/business/homo/strategy/opt/fedadam/#iflearner.business.homo.strategy.opt.fedadam.FedAdam","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedadam.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedAdam"},{"location":"zh/api/reference/business/homo/strategy/opt/fedadam/#iflearner.business.homo.strategy.opt.fedadam.FedAdam.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedadam.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" m_t = \u03b2_1m_t\u22121 + (1\u2212\u03b2_1)\u2206_t v_t = \u03b2_2v_t\u22121 + (1\u2212\u03b2_2)\u2206^2_t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _beta2 * self . _v [ key ] + ( 1 - self . _beta2 ) * np . square ( value ) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/fedavgm/","text":"FedAvgm ( learning_rate = 1 , momentum = 0.0 ) \u00b6 Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedavgm"},{"location":"zh/api/reference/business/homo/strategy/opt/fedavgm/#iflearner.business.homo.strategy.opt.fedavgm.FedAvgm","text":"Bases: FedOpt Implementation based on https://arxiv.org/abs/1909.06335 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 1. momentum float momentum factor. Defaults to 0.0. Source code in iflearner/business/homo/strategy/opt/fedavgm.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , learning_rate : float = 1 , momentum : float = 0.0 , ) -> None : super () . __init__ ( learning_rate = learning_rate ) self . _momentum_vector : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _momentum = momentum","title":"FedAvgm"},{"location":"zh/api/reference/business/homo/strategy/opt/fedavgm/#iflearner.business.homo.strategy.opt.fedavgm.FedAvgm.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedavgm.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _momentum_vector is None : self . _momentum_vector = dict () for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = - value else : for key , value in pseudo_gradient . items (): self . _momentum_vector [ key ] = ( self . _momentum * self . _momentum_vector [ key ] - value ) pseudo_gradient = self . _momentum_vector new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) - self . _lr * pseudo_gradient [ key ] self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/fedopt/","text":"FedOpt ( learning_rate = 0.1 , betas = ( 0.9 , 0.999 ), t = 0.001 ) \u00b6 Implementation based on https://arxiv.org/abs/2003.00295 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedopt.py 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t self . _params : dict = {} set_params ( params ) \u00b6 set params to self._params. Parameters: Name Type Description Default params _type_ parameters of server model required Source code in iflearner/business/homo/strategy/opt/fedopt.py 57 58 59 60 61 62 63 def set_params ( self , params ): \"\"\"set params to self._params. Args: params (_type_): parameters of server model \"\"\" self . _params = params step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedopt.py 43 44 45 46 47 48 49 50 51 52 53 54 55 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]], ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" pass","title":"fedopt"},{"location":"zh/api/reference/business/homo/strategy/opt/fedopt/#iflearner.business.homo.strategy.opt.fedopt.FedOpt","text":"Implementation based on https://arxiv.org/abs/2003.00295 . Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedopt.py 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t self . _params : dict = {}","title":"FedOpt"},{"location":"zh/api/reference/business/homo/strategy/opt/fedopt/#iflearner.business.homo.strategy.opt.fedopt.FedOpt.set_params","text":"set params to self._params. Parameters: Name Type Description Default params _type_ parameters of server model required Source code in iflearner/business/homo/strategy/opt/fedopt.py 57 58 59 60 61 62 63 def set_params ( self , params ): \"\"\"set params to self._params. Args: params (_type_): parameters of server model \"\"\" self . _params = params","title":"set_params()"},{"location":"zh/api/reference/business/homo/strategy/opt/fedopt/#iflearner.business.homo.strategy.opt.fedopt.FedOpt.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedopt.py 43 44 45 46 47 48 49 50 51 52 53 54 55 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]], ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" pass","title":"step()"},{"location":"zh/api/reference/business/homo/strategy/opt/fedyogi/","text":"FedYogi ( learning_rate = 0.1 , betas = ( 0.9 , 0.99 ), t = 0.001 ) \u00b6 Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {} step ( pseudo_gradient ) \u00b6 a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"fedyogi"},{"location":"zh/api/reference/business/homo/strategy/opt/fedyogi/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi","text":"Bases: FedOpt Attributes: Name Type Description learning_rate float learning rate. Defaults to 0.1. betas Tuple [ float , float ] coefficients used for computing t float adaptivity parameter. Defaults to 0.001. Source code in iflearner/business/homo/strategy/opt/fedyogi.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.99 ), t : float = 0.001 , ) -> None : super () . __init__ ( learning_rate , betas , t ) self . _m : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _v : Optional [ Dict [ str , npt . NDArray [ np . float32 ]]] = None self . _params : dict = {}","title":"FedYogi"},{"location":"zh/api/reference/business/homo/strategy/opt/fedyogi/#iflearner.business.homo.strategy.opt.fedyogi.FedYogi.step","text":"a step to optimize parameters of server model with pseudo gradient. Parameters: Name Type Description Default pseudo_gradient Dict [ str , npt . NDArray [ np . float32 ]] the pseudo gradient of server model required Returns: Type Description Dict [ str , npt . NDArray [ np . float32 ]] Dict[str, npt.NDArray[np.float32]]: parameters of server model after step Source code in iflearner/business/homo/strategy/opt/fedyogi.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: \"\"\"a step to optimize parameters of server model with pseudo gradient. Args: pseudo_gradient (Dict[str, npt.NDArray[np.float32]]): the pseudo gradient of server model Returns: Dict[str, npt.NDArray[np.float32]]: parameters of server model after step \"\"\" if self . _m is None : self . _m = dict () for key , value in pseudo_gradient . items (): self . _m [ key ] = np . zeros_like ( value ) if self . _v is None : self . _v = dict () for key , value in pseudo_gradient . items (): self . _v [ key ] = np . zeros_like ( value ) \"\"\" mt = \u03b21mt\u22121 + (1 \u2212\u03b21)\u2206t vt = \u03b22vt\u22121 + (1 \u2212\u03b22)\u22062t \"\"\" for key , value in pseudo_gradient . items (): self . _m [ key ] = self . _beta1 * self . _m [ key ] + ( 1 - self . _beta1 ) * value self . _v [ key ] = self . _v [ key ] - ( 1 - self . _beta2 ) * np . square ( value ) * np . sign ( self . _v [ key ] - np . square ( value )) \"\"\"x_t+1 = x_t + \u03b7 mt / (\u221av_t+\u03c4)\"\"\" new_params = dict () for key , value in self . _params . items (): new_params [ key ] = value . reshape (( - 1 )) + self . _lr * self . _m [ key ] / ( np . sqrt ( self . _v [ key ]) + self . _adaptivity ) self . _params [ key ] = new_params [ key ] . reshape ( value . shape ) return new_params","title":"step()"},{"location":"zh/api/reference/business/util/","text":"AccuracyMetric ( metric_name = 'accuracy' , file_dir = '' ) \u00b6 Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , ) BaseMetric ( metric_name , x_label , y_label , file_dir = './' ) \u00b6 Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir add ( x , y , train_type = TrainType . FederatedTrain ) \u00b6 add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore F1Metric ( metric_name = 'f1' , file_dir = '' ) \u00b6 Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir ) LossMetric ( metric_name = 'loss' , file_dir = '' ) \u00b6 Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir ) Metrics ( file_dir = './' ) \u00b6 Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True ) add ( metric ) \u00b6 add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric ) dump () \u00b6 save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f ) load () \u00b6 load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric plot () \u00b6 plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot ()","title":"Index"},{"location":"zh/api/reference/business/util/#iflearner.business.util.AccuracyMetric","text":"Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , )","title":"AccuracyMetric"},{"location":"zh/api/reference/business/util/#iflearner.business.util.BaseMetric","text":"Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir","title":"BaseMetric"},{"location":"zh/api/reference/business/util/#iflearner.business.util.metric_dev.BaseMetric.add","text":"add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore","title":"add()"},{"location":"zh/api/reference/business/util/#iflearner.business.util.F1Metric","text":"Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir )","title":"F1Metric"},{"location":"zh/api/reference/business/util/#iflearner.business.util.LossMetric","text":"Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir )","title":"LossMetric"},{"location":"zh/api/reference/business/util/#iflearner.business.util.Metrics","text":"Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True )","title":"Metrics"},{"location":"zh/api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.add","text":"add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric )","title":"add()"},{"location":"zh/api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.dump","text":"save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f )","title":"dump()"},{"location":"zh/api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.load","text":"load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric","title":"load()"},{"location":"zh/api/reference/business/util/#iflearner.business.util.metric_dev.Metrics.plot","text":"plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot ()","title":"plot()"},{"location":"zh/api/reference/business/util/metric/","text":"Metric ( logdir ) \u00b6 Integrate visualdl to visualize metrics. Source code in iflearner/business/util/metric.py 23 24 25 26 27 28 29 def __init__ ( self , logdir : str ) -> None : \"\"\"Init class with log directory.\"\"\" self . _tag_prefix = \"train\" self . _logdir = logdir self . _writers : Dict [ str , LogWriter ] = dict () self . _figs : Dict [ str , Any ] = dict () add ( name , label , x , y ) \u00b6 Add a point. Parameters: Name Type Description Default name str The name of metric, eg: acc, loss. required label str The label of metric, eg: local learning, federated learning. required x Any The x of point, eg: 1, 2, 3... required y Any The y of point, eg: 95.5, 96.0, 96.5... required Source code in iflearner/business/util/metric.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def add ( self , name : str , label : str , x : Any , y : Any ) -> None : \"\"\"Add a point. Args: name: The name of metric, eg: acc, loss. label: The label of metric, eg: local learning, federated learning. x: The x of point, eg: 1, 2, 3... y: The y of point, eg: 95.5, 96.0, 96.5... \"\"\" if label not in self . _writers : self . _writers [ label ] = LogWriter ( logdir = f \" { self . _logdir } / { label } \" , display_name = label ) self . _writers [ label ] . add_scalar ( f \" { self . _tag_prefix } / { name } \" , y , x )","title":"Metric"},{"location":"zh/api/reference/business/util/metric/#iflearner.business.util.metric.Metric","text":"Integrate visualdl to visualize metrics. Source code in iflearner/business/util/metric.py 23 24 25 26 27 28 29 def __init__ ( self , logdir : str ) -> None : \"\"\"Init class with log directory.\"\"\" self . _tag_prefix = \"train\" self . _logdir = logdir self . _writers : Dict [ str , LogWriter ] = dict () self . _figs : Dict [ str , Any ] = dict ()","title":"Metric"},{"location":"zh/api/reference/business/util/metric/#iflearner.business.util.metric.Metric.add","text":"Add a point. Parameters: Name Type Description Default name str The name of metric, eg: acc, loss. required label str The label of metric, eg: local learning, federated learning. required x Any The x of point, eg: 1, 2, 3... required y Any The y of point, eg: 95.5, 96.0, 96.5... required Source code in iflearner/business/util/metric.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def add ( self , name : str , label : str , x : Any , y : Any ) -> None : \"\"\"Add a point. Args: name: The name of metric, eg: acc, loss. label: The label of metric, eg: local learning, federated learning. x: The x of point, eg: 1, 2, 3... y: The y of point, eg: 95.5, 96.0, 96.5... \"\"\" if label not in self . _writers : self . _writers [ label ] = LogWriter ( logdir = f \" { self . _logdir } / { label } \" , display_name = label ) self . _writers [ label ] . add_scalar ( f \" { self . _tag_prefix } / { name } \" , y , x )","title":"add()"},{"location":"zh/api/reference/business/util/metric_dev/","text":"AccuracyMetric ( metric_name = 'accuracy' , file_dir = '' ) \u00b6 Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , ) BaseMetric ( metric_name , x_label , y_label , file_dir = './' ) \u00b6 Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir add ( x , y , train_type = TrainType . FederatedTrain ) \u00b6 add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore F1Metric ( metric_name = 'f1' , file_dir = '' ) \u00b6 Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir ) LossMetric ( metric_name = 'loss' , file_dir = '' ) \u00b6 Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir ) Metrics ( file_dir = './' ) \u00b6 Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True ) add ( metric ) \u00b6 add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric ) dump () \u00b6 save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f ) load () \u00b6 load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric plot () \u00b6 plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot () TrainType \u00b6 Bases: Enum define the type of train. supported local and federated","title":"Metric dev"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.AccuracyMetric","text":"Bases: BaseMetric accuracy metric class. Source code in iflearner/business/util/metric_dev.py 189 190 191 192 193 194 195 def __init__ ( self , metric_name : str = \"accuracy\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"accuracy\" , file_dir = file_dir , )","title":"AccuracyMetric"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.BaseMetric","text":"Bases: object Base class for metric. x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. Source code in iflearner/business/util/metric_dev.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , metric_name : str , x_label : str , y_label : str , file_dir : str = \"./\" ): \"\"\" Args: metric_name: metric name x_label: x-axis label for drawing y_label: y-axis label for drawing file_dir: The file path to save metic. \"\"\" self . _x_label = x_label self . _y_label = y_label self . _metric_name = metric_name self . _local_x_elements : List [ Scalar ] = [] self . _local_y_elements : List [ Scalar ] = [] self . _federate_x_elements : List [ Scalar ] = [] self . _federate_y_elements : List [ Scalar ] = [] self . _file_dir = file_dir","title":"BaseMetric"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.BaseMetric.add","text":"add scalar to elements. Parameters: Name Type Description Default train_type TrainType support localTrain and federatedTrain. TrainType.FederatedTrain x Union [ Scalar , List [ Scalar ]] x-axis scalar, for example as epoch value required y Union [ Scalar , List [ Scalar ]] y-axis scalar, for example as loss value required Source code in iflearner/business/util/metric_dev.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def add ( self , x : Union [ Scalar , List [ Scalar ]], y : Union [ Scalar , List [ Scalar ]], train_type : TrainType = TrainType . FederatedTrain , ) -> None : \"\"\"add scalar to elements. Args: train_type: support localTrain and federatedTrain. x: x-axis scalar, for example as `epoch` value y: y-axis scalar, for example as `loss` value Returns: None \"\"\" if train_type == TrainType . LocalTrain : if isinstance ( x , list ): self . _local_x_elements . extend ( x ) # type: ignore self . _local_y_elements . extend ( y ) # type: ignore else : self . _local_x_elements . append ( x ) # type: ignore self . _local_y_elements . append ( y ) # type: ignore elif train_type == TrainType . FederatedTrain : if isinstance ( x , list ): self . _federate_x_elements . extend ( x ) # type: ignore self . _federate_y_elements . extend ( y ) # type: ignore else : self . _federate_x_elements . append ( x ) # type: ignore self . _federate_y_elements . append ( y ) # type: ignore","title":"add()"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.F1Metric","text":"Bases: BaseMetric f1 metric class. Source code in iflearner/business/util/metric_dev.py 201 202 203 204 def __init__ ( self , metric_name : str = \"f1\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"f1\" , file_dir = file_dir )","title":"F1Metric"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.LossMetric","text":"Bases: BaseMetric loss metric class. Source code in iflearner/business/util/metric_dev.py 180 181 182 183 def __init__ ( self , metric_name : str = \"loss\" , file_dir : str = \"\" ): super () . __init__ ( metric_name = metric_name , x_label = \"epoch\" , y_label = \"loss\" , file_dir = file_dir )","title":"LossMetric"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics","text":"Statistical metric information, such as loss, accuracy, etc... Source code in iflearner/business/util/metric_dev.py 210 211 212 213 214 215 216 217 218 def __init__ ( self , file_dir : str = \"./\" ) -> None : \"\"\" Args: file_dir: The file path to save metic. \"\"\" self . _figs : Dict [ str , Any ] = dict () self . _metrics : List [ BaseMetric ] = [] self . _file_dir = file_dir Path ( file_dir ) . mkdir ( parents = True , exist_ok = True )","title":"Metrics"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.add","text":"add metric to metrics list. Parameters: Name Type Description Default metric BaseMetric class Metric, for example as LossMetric. required Source code in iflearner/business/util/metric_dev.py 224 225 226 227 228 229 230 231 232 233 def add ( self , metric : BaseMetric ) -> None : \"\"\"add metric to metrics list. Args: metric: class Metric, for example as LossMetric. Returns: None \"\"\" metric . file_dir = self . _file_dir self . _metrics . append ( metric )","title":"add()"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.dump","text":"save metrics data to file. Source code in iflearner/business/util/metric_dev.py 240 241 242 243 def dump ( self ) -> None : \"\"\"save metrics data to file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"wb\" ) as f : pickle . dump ( self , f )","title":"dump()"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.load","text":"load metrics data from file. Source code in iflearner/business/util/metric_dev.py 245 246 247 248 249 def load ( self ): \"\"\"load metrics data from file.\"\"\" with open ( f \" { self . _file_dir } /metrics.pkl\" , \"rb\" ) as f : metric = pickle . load ( f ) return metric","title":"load()"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.Metrics.plot","text":"plot and save to file. Source code in iflearner/business/util/metric_dev.py 235 236 237 238 def plot ( self ) -> None : \"\"\"plot and save to file.\"\"\" for metric in self . metrics : metric . plot ()","title":"plot()"},{"location":"zh/api/reference/business/util/metric_dev/#iflearner.business.util.metric_dev.TrainType","text":"Bases: Enum define the type of train. supported local and federated","title":"TrainType"},{"location":"zh/api/reference/communication/","text":"","title":"Index"},{"location":"zh/api/reference/communication/base/","text":"","title":"Index"},{"location":"zh/api/reference/communication/base/base_client/","text":"BaseClient ( addr , cert_path = None ) \u00b6 Provides methods that implement functionality of base client. Source code in iflearner/communication/base/base_client.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , addr : str , cert_path : str = None ) -> None : options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ] if cert_path is None : channel = grpc . insecure_channel ( addr , options = options ) else : with open ( cert_path , \"rb\" ) as f : cert_bytes = f . read () channel = grpc . secure_channel ( addr , grpc . ssl_channel_credentials ( cert_bytes ), options = options ) self . _stub : base_pb2_grpc . BaseStub = base_pb2_grpc . BaseStub ( channel )","title":"base_client"},{"location":"zh/api/reference/communication/base/base_client/#iflearner.communication.base.base_client.BaseClient","text":"Provides methods that implement functionality of base client. Source code in iflearner/communication/base/base_client.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , addr : str , cert_path : str = None ) -> None : options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ] if cert_path is None : channel = grpc . insecure_channel ( addr , options = options ) else : with open ( cert_path , \"rb\" ) as f : cert_bytes = f . read () channel = grpc . secure_channel ( addr , grpc . ssl_channel_credentials ( cert_bytes ), options = options ) self . _stub : base_pb2_grpc . BaseStub = base_pb2_grpc . BaseStub ( channel )","title":"BaseClient"},{"location":"zh/api/reference/communication/base/base_exception/","text":"BaseException ( code , message ) \u00b6 Bases: Exception Define the base exception class. You can inherit the base class and implement your business logic. Attributes: Name Type Description code error code message error details Source code in iflearner/communication/base/base_exception.py 25 26 27 def __init__ ( self , code : int , message : str ) -> None : self . code = code self . message = message","title":"base_exception"},{"location":"zh/api/reference/communication/base/base_exception/#iflearner.communication.base.base_exception.BaseException","text":"Bases: Exception Define the base exception class. You can inherit the base class and implement your business logic. Attributes: Name Type Description code error code message error details Source code in iflearner/communication/base/base_exception.py 25 26 27 def __init__ ( self , code : int , message : str ) -> None : self . code = code self . message = message","title":"BaseException"},{"location":"zh/api/reference/communication/base/base_pb2/","text":"Generated protocol buffer code.","title":"Base pb2"},{"location":"zh/api/reference/communication/base/base_pb2_grpc/","text":"Client and server classes corresponding to protobuf-defined services. Base \u00b6 Bases: object Missing associated documentation comment in .proto file. BaseServicer \u00b6 Bases: object Missing associated documentation comment in .proto file. callback ( request , context ) \u00b6 Use this function to wait for server information. Source code in iflearner/communication/base/base_pb2_grpc.py 51 52 53 54 55 56 def callback ( self , request , context ): \"\"\"Use this function to wait for server information. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' ) post ( request , context ) \u00b6 Use this function to transport information asynchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 44 45 46 47 48 49 def post ( self , request , context ): \"\"\"Use this function to transport information asynchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' ) send ( request , context ) \u00b6 Use this function to transport information synchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 37 38 39 40 41 42 def send ( self , request , context ): \"\"\"Use this function to transport information synchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' ) BaseStub ( channel ) \u00b6 Bases: object Missing associated documentation comment in .proto file. Parameters: Name Type Description Default channel A grpc.Channel. required Source code in iflearner/communication/base/base_pb2_grpc.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , channel ): \"\"\"Constructor. Args: channel: A grpc.Channel. \"\"\" self . send = channel . unary_unary ( '/Base/send' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . post = channel . unary_unary ( '/Base/post' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . callback = channel . unary_unary ( '/Base/callback' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , )","title":"Base pb2 grpc"},{"location":"zh/api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.Base","text":"Bases: object Missing associated documentation comment in .proto file.","title":"Base"},{"location":"zh/api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer","text":"Bases: object Missing associated documentation comment in .proto file.","title":"BaseServicer"},{"location":"zh/api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer.callback","text":"Use this function to wait for server information. Source code in iflearner/communication/base/base_pb2_grpc.py 51 52 53 54 55 56 def callback ( self , request , context ): \"\"\"Use this function to wait for server information. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' )","title":"callback()"},{"location":"zh/api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer.post","text":"Use this function to transport information asynchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 44 45 46 47 48 49 def post ( self , request , context ): \"\"\"Use this function to transport information asynchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' )","title":"post()"},{"location":"zh/api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseServicer.send","text":"Use this function to transport information synchronously. Source code in iflearner/communication/base/base_pb2_grpc.py 37 38 39 40 41 42 def send ( self , request , context ): \"\"\"Use this function to transport information synchronously. \"\"\" context . set_code ( grpc . StatusCode . UNIMPLEMENTED ) context . set_details ( 'Method not implemented!' ) raise NotImplementedError ( 'Method not implemented!' )","title":"send()"},{"location":"zh/api/reference/communication/base/base_pb2_grpc/#iflearner.communication.base.base_pb2_grpc.BaseStub","text":"Bases: object Missing associated documentation comment in .proto file. Parameters: Name Type Description Default channel A grpc.Channel. required Source code in iflearner/communication/base/base_pb2_grpc.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , channel ): \"\"\"Constructor. Args: channel: A grpc.Channel. \"\"\" self . send = channel . unary_unary ( '/Base/send' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . post = channel . unary_unary ( '/Base/post' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , ) self . callback = channel . unary_unary ( '/Base/callback' , request_serializer = base__pb2 . BaseRequest . SerializeToString , response_deserializer = base__pb2 . BaseResponse . FromString , )","title":"BaseStub"},{"location":"zh/api/reference/communication/base/base_server/","text":"BaseServer \u00b6 Bases: base_pb2_grpc . BaseServicer , ABC Provides methods that implement functionality of base server. start_server ( addr , servicer ) \u00b6 Start server at the address. Source code in iflearner/communication/base/base_server.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def start_server ( addr : str , servicer : BaseServer ) -> None : \"\"\"Start server at the address.\"\"\" server = grpc . server ( futures . ThreadPoolExecutor ( max_workers = 10 ), options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ], ) base_pb2_grpc . add_BaseServicer_to_server ( servicer , server ) server . add_insecure_port ( addr ) server . start () server . wait_for_termination ()","title":"base_server"},{"location":"zh/api/reference/communication/base/base_server/#iflearner.communication.base.base_server.BaseServer","text":"Bases: base_pb2_grpc . BaseServicer , ABC Provides methods that implement functionality of base server.","title":"BaseServer"},{"location":"zh/api/reference/communication/base/base_server/#iflearner.communication.base.base_server.start_server","text":"Start server at the address. Source code in iflearner/communication/base/base_server.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def start_server ( addr : str , servicer : BaseServer ) -> None : \"\"\"Start server at the address.\"\"\" server = grpc . server ( futures . ThreadPoolExecutor ( max_workers = 10 ), options = [ ( \"grpc.max_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_send_message_length\" , constant . MAX_MSG_LENGTH ), ( \"grpc.max_receive_message_length\" , constant . MAX_MSG_LENGTH ), ], ) base_pb2_grpc . add_BaseServicer_to_server ( servicer , server ) server . add_insecure_port ( addr ) server . start () server . wait_for_termination ()","title":"start_server()"},{"location":"zh/api/reference/communication/base/constant/","text":"","title":"Constant"},{"location":"zh/api/reference/communication/hetero/","text":"","title":"Index"},{"location":"zh/api/reference/communication/homo/","text":"","title":"Index"},{"location":"zh/api/reference/communication/homo/homo_client/","text":"HomoClient ( server_addr , party_name , cert_path = None ) \u00b6 Bases: base_client . BaseClient Implement homogeneous client base on base_client.BaseClient. Source code in iflearner/communication/homo/homo_client.py 30 31 32 33 34 35 def __init__ ( self , server_addr : str , party_name : str , cert_path : str = None ) -> None : super () . __init__ ( server_addr , cert_path ) self . _party_name = party_name self . _strategy : strategy_client . StrategyClient = None # type: ignore notice () \u00b6 Receive notifications from the server regularly. Source code in iflearner/communication/homo/homo_client.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def notice ( self ) -> None : \"\"\"Receive notifications from the server regularly.\"\"\" while True : start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name ) resp = self . _callback ( req ) if resp . code != 0 : raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message ) if resp . type == message_type . MSG_AGGREGATE_RESULT : data = homo_pb2 . AggregateResult () data . ParseFromString ( resp . data ) self . _strategy . handler_aggregate_result ( data ) elif resp . type == message_type . MSG_NOTIFY_TRAINING : self . _strategy . handler_notify_training () elif resp . type in self . _strategy . custom_handlers : self . _strategy . custom_handlers [ resp . type ]() # type: ignore if resp . type != \"\" : stop = timeit . default_timer () logger . info ( f \"IN: party: message type: { resp . type } , time: { 1000 * ( stop - start ) } ms\" ) time . sleep ( message_type . MSG_HEARTBEAT_INTERVAL ) transport ( type , data = None ) \u00b6 Transport data to server. Source code in iflearner/communication/homo/homo_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def transport ( self , type : str , data : Any = None ) -> homo_pb2 . RegistrationResponse : \"\"\"Transport data to server.\"\"\" start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = type ) if data is not None : req . data = data . SerializeToString () resp = None if type == message_type . MSG_REGISTER : resp = self . _send ( req ) elif type == message_type . MSG_CLIENT_READY : resp = self . _send ( req ) elif type == message_type . MSG_UPLOAD_PARAM : resp = self . _post ( req ) elif type == message_type . MSG_COMPLETE : resp = self . _send ( req ) stop = timeit . default_timer () logger . info ( f \"OUT: message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp . code != 0 : # type: ignore raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message # type: ignore ) if type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationResponse () data . ParseFromString ( resp . data ) # type: ignore return data","title":"homo_client"},{"location":"zh/api/reference/communication/homo/homo_client/#iflearner.communication.homo.homo_client.HomoClient","text":"Bases: base_client . BaseClient Implement homogeneous client base on base_client.BaseClient. Source code in iflearner/communication/homo/homo_client.py 30 31 32 33 34 35 def __init__ ( self , server_addr : str , party_name : str , cert_path : str = None ) -> None : super () . __init__ ( server_addr , cert_path ) self . _party_name = party_name self . _strategy : strategy_client . StrategyClient = None # type: ignore","title":"HomoClient"},{"location":"zh/api/reference/communication/homo/homo_client/#iflearner.communication.homo.homo_client.HomoClient.notice","text":"Receive notifications from the server regularly. Source code in iflearner/communication/homo/homo_client.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def notice ( self ) -> None : \"\"\"Receive notifications from the server regularly.\"\"\" while True : start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name ) resp = self . _callback ( req ) if resp . code != 0 : raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message ) if resp . type == message_type . MSG_AGGREGATE_RESULT : data = homo_pb2 . AggregateResult () data . ParseFromString ( resp . data ) self . _strategy . handler_aggregate_result ( data ) elif resp . type == message_type . MSG_NOTIFY_TRAINING : self . _strategy . handler_notify_training () elif resp . type in self . _strategy . custom_handlers : self . _strategy . custom_handlers [ resp . type ]() # type: ignore if resp . type != \"\" : stop = timeit . default_timer () logger . info ( f \"IN: party: message type: { resp . type } , time: { 1000 * ( stop - start ) } ms\" ) time . sleep ( message_type . MSG_HEARTBEAT_INTERVAL )","title":"notice()"},{"location":"zh/api/reference/communication/homo/homo_client/#iflearner.communication.homo.homo_client.HomoClient.transport","text":"Transport data to server. Source code in iflearner/communication/homo/homo_client.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def transport ( self , type : str , data : Any = None ) -> homo_pb2 . RegistrationResponse : \"\"\"Transport data to server.\"\"\" start = timeit . default_timer () req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = type ) if data is not None : req . data = data . SerializeToString () resp = None if type == message_type . MSG_REGISTER : resp = self . _send ( req ) elif type == message_type . MSG_CLIENT_READY : resp = self . _send ( req ) elif type == message_type . MSG_UPLOAD_PARAM : resp = self . _post ( req ) elif type == message_type . MSG_COMPLETE : resp = self . _send ( req ) stop = timeit . default_timer () logger . info ( f \"OUT: message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp . code != 0 : # type: ignore raise HomoException ( code = HomoException . HomoResponseCode ( resp . code ), message = resp . message # type: ignore ) if type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationResponse () data . ParseFromString ( resp . data ) # type: ignore return data","title":"transport()"},{"location":"zh/api/reference/communication/homo/homo_exception/","text":"HomoException ( code , message ) \u00b6 Bases: base_exception . BaseException Source code in iflearner/communication/homo/homo_exception.py 30 31 32 33 def __init__ ( self , code : HomoResponseCode , message : str ) -> None : super () . __init__ ( code . value , f \" { code . __class__ . __name__ } . { code . name } - { message } \" ) HomoResponseCode \u00b6 Bases: IntEnum Define response code","title":"homo_exception"},{"location":"zh/api/reference/communication/homo/homo_exception/#iflearner.communication.homo.homo_exception.HomoException","text":"Bases: base_exception . BaseException Source code in iflearner/communication/homo/homo_exception.py 30 31 32 33 def __init__ ( self , code : HomoResponseCode , message : str ) -> None : super () . __init__ ( code . value , f \" { code . __class__ . __name__ } . { code . name } - { message } \" )","title":"HomoException"},{"location":"zh/api/reference/communication/homo/homo_exception/#iflearner.communication.homo.homo_exception.HomoException.HomoResponseCode","text":"Bases: IntEnum Define response code","title":"HomoResponseCode"},{"location":"zh/api/reference/communication/homo/homo_pb2/","text":"Generated protocol buffer code.","title":"Homo pb2"},{"location":"zh/api/reference/communication/homo/homo_pb2_grpc/","text":"Client and server classes corresponding to protobuf-defined services.","title":"Homo pb2 grpc"},{"location":"zh/api/reference/communication/homo/homo_server/","text":"HomoServer ( strategy ) \u00b6 Bases: base_server . BaseServer Implement homogeneous server base on base_server.BaseServer. Source code in iflearner/communication/homo/homo_server.py 29 30 31 def __init__ ( self , strategy : strategy_server . StrategyServer ) -> None : self . _callback_messages : dict = dict () self . _strategy = strategy callback ( request , context ) \u00b6 The channel of pushing message to clients initiatively. Source code in iflearner/communication/homo/homo_server.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def callback ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"The channel of pushing message to clients initiatively.\"\"\" start = timeit . default_timer () type , resp_data = self . _strategy . get_client_notification ( request . party_name ) if type is not None : stop = timeit . default_timer () logger . info ( f \"OUT: party: { request . party_name } , message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp_data is None : return base_pb2 . BaseResponse ( type = type ) return base_pb2 . BaseResponse ( type = type , data = resp_data . SerializeToString ()) post ( request , context ) \u00b6 Handle client requests asynchronously. Source code in iflearner/communication/homo/homo_server.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def post ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests asynchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_UPLOAD_PARAM : req_data = homo_pb2 . UploadParam () req_data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_upload_param ( request . party_name , req_data ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" ) send ( request , context ) \u00b6 Handle client requests synchronously. Source code in iflearner/communication/homo/homo_server.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests synchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationInfo () data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_register ( request . party_name , data . sample_num , data . step_num ) elif request . type == message_type . MSG_CLIENT_READY : resp_data = self . _strategy . handler_client_ready ( request . party_name ) # type: ignore elif request . type == message_type . MSG_COMPLETE : resp_data = self . _strategy . handler_complete ( request . party_name ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . party_name , request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" )","title":"homo_server"},{"location":"zh/api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer","text":"Bases: base_server . BaseServer Implement homogeneous server base on base_server.BaseServer. Source code in iflearner/communication/homo/homo_server.py 29 30 31 def __init__ ( self , strategy : strategy_server . StrategyServer ) -> None : self . _callback_messages : dict = dict () self . _strategy = strategy","title":"HomoServer"},{"location":"zh/api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer.callback","text":"The channel of pushing message to clients initiatively. Source code in iflearner/communication/homo/homo_server.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def callback ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"The channel of pushing message to clients initiatively.\"\"\" start = timeit . default_timer () type , resp_data = self . _strategy . get_client_notification ( request . party_name ) if type is not None : stop = timeit . default_timer () logger . info ( f \"OUT: party: { request . party_name } , message type: { type } , time: { 1000 * ( stop - start ) } ms\" ) if resp_data is None : return base_pb2 . BaseResponse ( type = type ) return base_pb2 . BaseResponse ( type = type , data = resp_data . SerializeToString ())","title":"callback()"},{"location":"zh/api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer.post","text":"Handle client requests asynchronously. Source code in iflearner/communication/homo/homo_server.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def post ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests asynchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_UPLOAD_PARAM : req_data = homo_pb2 . UploadParam () req_data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_upload_param ( request . party_name , req_data ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" )","title":"post()"},{"location":"zh/api/reference/communication/homo/homo_server/#iflearner.communication.homo.homo_server.HomoServer.send","text":"Handle client requests synchronously. Source code in iflearner/communication/homo/homo_server.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Handle client requests synchronously.\"\"\" try : start = timeit . default_timer () resp_data = None if request . type == message_type . MSG_REGISTER : data = homo_pb2 . RegistrationInfo () data . ParseFromString ( request . data ) resp_data = self . _strategy . handler_register ( request . party_name , data . sample_num , data . step_num ) elif request . type == message_type . MSG_CLIENT_READY : resp_data = self . _strategy . handler_client_ready ( request . party_name ) # type: ignore elif request . type == message_type . MSG_COMPLETE : resp_data = self . _strategy . handler_complete ( request . party_name ) # type: ignore elif request . type in self . _strategy . custom_handlers : resp_data = self . _strategy . custom_handlers [ request . type ]( request . party_name , request . data ) except HomoException as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = e . code , message = e . message ) except Exception as e : logger . info ( e ) return base_pb2 . BaseResponse ( code = HomoException . HomoResponseCode . InternalError , message = str ( e ) ) else : if resp_data is None : return base_pb2 . BaseResponse () return base_pb2 . BaseResponse ( data = resp_data . SerializeToString ()) # type: ignore finally : stop = timeit . default_timer () logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } , time: { 1000 * ( stop - start ) } ms\" )","title":"send()"},{"location":"zh/api/reference/communication/homo/message_type/","text":"Define the heartbeat interval between client and server. MSG_COMPLETE = 'msg_complete' module-attribute \u00b6 Define the message type for communication. (From server to client) MSG_HEARTBEAT_INTERVAL = 1 module-attribute \u00b6 Define the message type for communication. (From client to server) MSG_NOTIFY_TRAINING = 'msg_notify_training' module-attribute \u00b6 Define the name of strategy.","title":"message_type"},{"location":"zh/api/reference/communication/homo/message_type/#iflearner.communication.homo.message_type.MSG_COMPLETE","text":"Define the message type for communication. (From server to client)","title":"MSG_COMPLETE"},{"location":"zh/api/reference/communication/homo/message_type/#iflearner.communication.homo.message_type.MSG_HEARTBEAT_INTERVAL","text":"Define the message type for communication. (From client to server)","title":"MSG_HEARTBEAT_INTERVAL"},{"location":"zh/api/reference/communication/homo/message_type/#iflearner.communication.homo.message_type.MSG_NOTIFY_TRAINING","text":"Define the name of strategy.","title":"MSG_NOTIFY_TRAINING"},{"location":"zh/api/reference/communication/peer/","text":"","title":"Index"},{"location":"zh/api/reference/communication/peer/aes/","text":"AESCipher ( key ) \u00b6 Bases: object Source code in iflearner/communication/peer/aes.py 10 11 12 def __init__ ( self , key : str ) -> None : self . bs = AES . block_size self . key = hashlib . sha256 ( key . encode ()) . digest () decrypt ( enc ) \u00b6 Decrypt data Source code in iflearner/communication/peer/aes.py 22 23 24 25 26 27 28 def decrypt ( self , enc : List ) -> Any : \"\"\"Decrypt data\"\"\" enc = base64 . b64decode ( enc ) iv = enc [: AES . block_size ] cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return self . _unpad ( cipher . decrypt ( enc [ AES . block_size :])) . decode ( \"utf-8\" ) encrypt ( raw ) \u00b6 Encrypt string to bytes Source code in iflearner/communication/peer/aes.py 14 15 16 17 18 19 20 def encrypt ( self , raw : str ) -> bytes : \"\"\"Encrypt string to bytes\"\"\" raw = self . _pad ( raw ) iv = Random . new () . read ( AES . block_size ) cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return base64 . b64encode ( iv + cipher . encrypt ( raw . encode ()))","title":"aes"},{"location":"zh/api/reference/communication/peer/aes/#iflearner.communication.peer.aes.AESCipher","text":"Bases: object Source code in iflearner/communication/peer/aes.py 10 11 12 def __init__ ( self , key : str ) -> None : self . bs = AES . block_size self . key = hashlib . sha256 ( key . encode ()) . digest ()","title":"AESCipher"},{"location":"zh/api/reference/communication/peer/aes/#iflearner.communication.peer.aes.AESCipher.decrypt","text":"Decrypt data Source code in iflearner/communication/peer/aes.py 22 23 24 25 26 27 28 def decrypt ( self , enc : List ) -> Any : \"\"\"Decrypt data\"\"\" enc = base64 . b64decode ( enc ) iv = enc [: AES . block_size ] cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return self . _unpad ( cipher . decrypt ( enc [ AES . block_size :])) . decode ( \"utf-8\" )","title":"decrypt()"},{"location":"zh/api/reference/communication/peer/aes/#iflearner.communication.peer.aes.AESCipher.encrypt","text":"Encrypt string to bytes Source code in iflearner/communication/peer/aes.py 14 15 16 17 18 19 20 def encrypt ( self , raw : str ) -> bytes : \"\"\"Encrypt string to bytes\"\"\" raw = self . _pad ( raw ) iv = Random . new () . read ( AES . block_size ) cipher = AES . new ( self . key , AES . MODE_CBC , iv ) return base64 . b64encode ( iv + cipher . encrypt ( raw . encode ()))","title":"encrypt()"},{"location":"zh/api/reference/communication/peer/diffie_hellman/","text":"DiffieHellman \u00b6 Bases: object key_pair ( num_bits = 1024 , pair_name = None ) staticmethod \u00b6 Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator Source code in iflearner/communication/peer/diffie_hellman.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @staticmethod def key_pair ( num_bits = 1024 , pair_name = None ): \"\"\" Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator \"\"\" if pair_name is None : if num_bits : return DiffieHellman . _key_pair ( num_bits ) else : return DiffieHellman . _additional_group_1024_160 () assert pair_name in { \"additional_group_1024_160\" , \"additional_group_2048_224\" , \"additional_group_2048_256\" , \"oakley_group_768_768\" , \"oakley_group_1024_1024\" , }, \"unsupported pair name: {0} \" . format ( pair_name ) if pair_name == \"additional_group_1024_160\" : return DiffieHellman . _additional_group_1024_160 () if pair_name == \"additional_group_2048_224\" : return DiffieHellman . _additional_group_2048_224 () if pair_name == \"additional_group_2048_256\" : return DiffieHellman . _additional_group_2048_256 () if pair_name == \"oakley_group_768_768\" : return DiffieHellman . _oakley_group_768_768 () if pair_name == \"oakley_group_1024_1024\" : return DiffieHellman . _oakley_group_1024_1024 ()","title":"diffie_hellman"},{"location":"zh/api/reference/communication/peer/diffie_hellman/#iflearner.communication.peer.diffie_hellman.DiffieHellman","text":"Bases: object","title":"DiffieHellman"},{"location":"zh/api/reference/communication/peer/diffie_hellman/#iflearner.communication.peer.diffie_hellman.DiffieHellman.key_pair","text":"Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator Source code in iflearner/communication/peer/diffie_hellman.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @staticmethod def key_pair ( num_bits = 1024 , pair_name = None ): \"\"\" Generate a primitive root for a big prime number is really slow! Notice the fact that: 1. we don't need the generator to be a primitive element of the group but the one generates a large prime order. 2. There is no security issue with Diffie-Hellman if you reuse previously generated \ud835\udc5d and \ud835\udc54. We simply use key pairs from RFC 5114 and RFC 2409 @:param pair_name: one of \"additional_group_1024_160\", \"additional_group_2048_224\", \"additional_group_2048_256\", \"oakley_group_768_768\", \"oakley_group_1024_1024\" use additional_group_1024_160 as default @:param num_bits: specify size of p @:return p, g, where p is a prime number, g is a generator \"\"\" if pair_name is None : if num_bits : return DiffieHellman . _key_pair ( num_bits ) else : return DiffieHellman . _additional_group_1024_160 () assert pair_name in { \"additional_group_1024_160\" , \"additional_group_2048_224\" , \"additional_group_2048_256\" , \"oakley_group_768_768\" , \"oakley_group_1024_1024\" , }, \"unsupported pair name: {0} \" . format ( pair_name ) if pair_name == \"additional_group_1024_160\" : return DiffieHellman . _additional_group_1024_160 () if pair_name == \"additional_group_2048_224\" : return DiffieHellman . _additional_group_2048_224 () if pair_name == \"additional_group_2048_256\" : return DiffieHellman . _additional_group_2048_256 () if pair_name == \"oakley_group_768_768\" : return DiffieHellman . _oakley_group_768_768 () if pair_name == \"oakley_group_1024_1024\" : return DiffieHellman . _oakley_group_1024_1024 ()","title":"key_pair()"},{"location":"zh/api/reference/communication/peer/diffie_hellman_inst/","text":"DiffieHellmanInst \u00b6 Bases: object The Diffie-Hellman instance for generating public key and secret.","title":"diffie_hellman_inst"},{"location":"zh/api/reference/communication/peer/diffie_hellman_inst/#iflearner.communication.peer.diffie_hellman_inst.DiffieHellmanInst","text":"Bases: object The Diffie-Hellman instance for generating public key and secret.","title":"DiffieHellmanInst"},{"location":"zh/api/reference/communication/peer/message_type/","text":"The public key of diffie hellman . MSG_DH_PUBLIC_KEY = 'msg_dh_public_key' module-attribute \u00b6 The random key in SMPC.","title":"Message type"},{"location":"zh/api/reference/communication/peer/message_type/#iflearner.communication.peer.message_type.MSG_DH_PUBLIC_KEY","text":"The random key in SMPC.","title":"MSG_DH_PUBLIC_KEY"},{"location":"zh/api/reference/communication/peer/peer_client/","text":"PeerClient ( server_addr , party_name , peer_cert = None ) \u00b6 Bases: base_client . BaseClient The client for peer party communication. Source code in iflearner/communication/peer/peer_client.py 29 30 31 def __init__ ( self , server_addr : str , party_name : str , peer_cert : str = None ) -> None : super () . __init__ ( server_addr , peer_cert ) self . _party_name = party_name get_DH_public_key () \u00b6 Get Diffie-Hellman public key from specified server. Returns: Type Description List The public key. Source code in iflearner/communication/peer/peer_client.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def get_DH_public_key ( self ) -> List : \"\"\"Get Diffie-Hellman public key from specified server. Returns: The public key. \"\"\" while True : try : req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_DH_PUBLIC_KEY , data = DiffieHellmanInst . generate_public_key (), ) resp = self . _send ( req ) public_key = resp . data logger . info ( f \"Public key: { public_key } \" ) return public_key except Exception as e : logger . info ( e ) time . sleep ( 3 ) get_SMPC_random_key ( key ) \u00b6 Get random value from the other party. Returns: Type Description float A float value. Source code in iflearner/communication/peer/peer_client.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_SMPC_random_key ( self , key : str ) -> float : \"\"\"Get random value from the other party. Returns: A float value. \"\"\" req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_SMPC_RANDOM_KEY ) resp = self . _send ( req ) random_float = float ( aes . AESCipher ( key ) . decrypt ( resp . data )) logger . info ( f \"Random float: { random_float } \" ) return random_float","title":"peer_client"},{"location":"zh/api/reference/communication/peer/peer_client/#iflearner.communication.peer.peer_client.PeerClient","text":"Bases: base_client . BaseClient The client for peer party communication. Source code in iflearner/communication/peer/peer_client.py 29 30 31 def __init__ ( self , server_addr : str , party_name : str , peer_cert : str = None ) -> None : super () . __init__ ( server_addr , peer_cert ) self . _party_name = party_name","title":"PeerClient"},{"location":"zh/api/reference/communication/peer/peer_client/#iflearner.communication.peer.peer_client.PeerClient.get_DH_public_key","text":"Get Diffie-Hellman public key from specified server. Returns: Type Description List The public key. Source code in iflearner/communication/peer/peer_client.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def get_DH_public_key ( self ) -> List : \"\"\"Get Diffie-Hellman public key from specified server. Returns: The public key. \"\"\" while True : try : req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_DH_PUBLIC_KEY , data = DiffieHellmanInst . generate_public_key (), ) resp = self . _send ( req ) public_key = resp . data logger . info ( f \"Public key: { public_key } \" ) return public_key except Exception as e : logger . info ( e ) time . sleep ( 3 )","title":"get_DH_public_key()"},{"location":"zh/api/reference/communication/peer/peer_client/#iflearner.communication.peer.peer_client.PeerClient.get_SMPC_random_key","text":"Get random value from the other party. Returns: Type Description float A float value. Source code in iflearner/communication/peer/peer_client.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_SMPC_random_key ( self , key : str ) -> float : \"\"\"Get random value from the other party. Returns: A float value. \"\"\" req = base_pb2 . BaseRequest ( party_name = self . _party_name , type = message_type . MSG_SMPC_RANDOM_KEY ) resp = self . _send ( req ) random_float = float ( aes . AESCipher ( key ) . decrypt ( resp . data )) logger . info ( f \"Random float: { random_float } \" ) return random_float","title":"get_SMPC_random_key()"},{"location":"zh/api/reference/communication/peer/peer_server/","text":"PeerServer ( peer_num ) \u00b6 Bases: base_server . BaseServer The server for peer party communication. Source code in iflearner/communication/peer/peer_server.py 29 30 31 32 33 34 def __init__ ( self , peer_num : int ) -> None : super () . __init__ () self . _parties_secret = dict () self . _parties_random_value = dict () self . _peer_num = peer_num send ( request , context ) \u00b6 Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY. Source code in iflearner/communication/peer/peer_server.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY.\"\"\" logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } \" ) if request . type == message_type . MSG_DH_PUBLIC_KEY : self . _parties_secret [ request . party_name ] = DiffieHellmanInst () . generate_secret ( request . data ) return base_pb2 . BaseResponse ( data = DiffieHellmanInst () . generate_public_key ()) elif request . type == message_type . MSG_SMPC_RANDOM_KEY : random_float = random . uniform ( 0.1 , 1.0 ) logger . info ( f \"Party: { request . party_name } , Random float: { random_float } \" ) self . _parties_random_value [ request . party_name ] = - random_float # return base_pb2.BaseResponse(data=bytearray(struct.pack('f', random_float))) return base_pb2 . BaseResponse ( data = aes . AESCipher ( self . _parties_secret [ request . party_name ]) . encrypt ( str ( random_float ) ) ) sum_parties_random_value () \u00b6 When the values from each party are received, we add all the values together. Source code in iflearner/communication/peer/peer_server.py 36 37 38 39 40 41 42 43 def sum_parties_random_value ( self ) -> float : \"\"\"When the values from each party are received, we add all the values together.\"\"\" while True : if self . _peer_num == len ( self . _parties_random_value . values ()): return sum ( self . _parties_random_value . values ()) time . sleep ( 1 )","title":"peer_server"},{"location":"zh/api/reference/communication/peer/peer_server/#iflearner.communication.peer.peer_server.PeerServer","text":"Bases: base_server . BaseServer The server for peer party communication. Source code in iflearner/communication/peer/peer_server.py 29 30 31 32 33 34 def __init__ ( self , peer_num : int ) -> None : super () . __init__ () self . _parties_secret = dict () self . _parties_random_value = dict () self . _peer_num = peer_num","title":"PeerServer"},{"location":"zh/api/reference/communication/peer/peer_server/#iflearner.communication.peer.peer_server.PeerServer.send","text":"Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY. Source code in iflearner/communication/peer/peer_server.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def send ( self , request : base_pb2 . BaseRequest , context : Any ) -> base_pb2 . BaseResponse : \"\"\"Send two types of requests to server, including MSG_DH_PUBLIC_KEY and MSG_SMPC_RANDOM_KEY.\"\"\" logger . info ( f \"IN: party: { request . party_name } , message type: { request . type } \" ) if request . type == message_type . MSG_DH_PUBLIC_KEY : self . _parties_secret [ request . party_name ] = DiffieHellmanInst () . generate_secret ( request . data ) return base_pb2 . BaseResponse ( data = DiffieHellmanInst () . generate_public_key ()) elif request . type == message_type . MSG_SMPC_RANDOM_KEY : random_float = random . uniform ( 0.1 , 1.0 ) logger . info ( f \"Party: { request . party_name } , Random float: { random_float } \" ) self . _parties_random_value [ request . party_name ] = - random_float # return base_pb2.BaseResponse(data=bytearray(struct.pack('f', random_float))) return base_pb2 . BaseResponse ( data = aes . AESCipher ( self . _parties_secret [ request . party_name ]) . encrypt ( str ( random_float ) ) )","title":"send()"},{"location":"zh/api/reference/communication/peer/peer_server/#iflearner.communication.peer.peer_server.PeerServer.sum_parties_random_value","text":"When the values from each party are received, we add all the values together. Source code in iflearner/communication/peer/peer_server.py 36 37 38 39 40 41 42 43 def sum_parties_random_value ( self ) -> float : \"\"\"When the values from each party are received, we add all the values together.\"\"\" while True : if self . _peer_num == len ( self . _parties_random_value . values ()): return sum ( self . _parties_random_value . values ()) time . sleep ( 1 )","title":"sum_parties_random_value()"},{"location":"zh/api/reference/datasets/","text":"","title":"Index"},{"location":"zh/api/reference/datasets/cifar/","text":"CIFAR10 ( root , download = False ) \u00b6 Bases: FLDateset CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True. required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. False Source code in iflearner/datasets/cifar.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , root : str , download : bool = False , ) -> None : super ( CIFAR10 , self ) . __init__ () self . root = root if download : self . download () if not self . _check_integrity (): raise RuntimeError ( \"Dataset not found or corrupted.\" + \" You can use download=True to download it\" ) self . train_x = [] self . test_x = [] self . train_targets = [] self . test_targets = [] # now load the picked numpy arrays for file_name , checksum in self . train_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . train_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . train_targets . extend ( entry [ \"labels\" ]) else : self . train_targets . extend ( entry [ \"fine_labels\" ]) self . train_x = np . vstack ( self . train_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . train_x = self . train_x . transpose (( 0 , 2 , 3 , 1 )) # convert to HWC for file_name , checksum in self . test_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . test_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . test_targets . extend ( entry [ \"labels\" ]) else : self . test_targets . extend ( entry [ \"fine_labels\" ]) self . test_x = np . vstack ( self . test_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . test_x = self . test_x . transpose (( 0 , 2 , 3 , 1 )) self . _load_meta () CIFAR100 \u00b6 Bases: CIFAR10 CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. This is a subclass of the CIFAR10 Dataset.","title":"Cifar"},{"location":"zh/api/reference/datasets/cifar/#iflearner.datasets.cifar.CIFAR10","text":"Bases: FLDateset CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True. required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. False Source code in iflearner/datasets/cifar.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , root : str , download : bool = False , ) -> None : super ( CIFAR10 , self ) . __init__ () self . root = root if download : self . download () if not self . _check_integrity (): raise RuntimeError ( \"Dataset not found or corrupted.\" + \" You can use download=True to download it\" ) self . train_x = [] self . test_x = [] self . train_targets = [] self . test_targets = [] # now load the picked numpy arrays for file_name , checksum in self . train_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . train_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . train_targets . extend ( entry [ \"labels\" ]) else : self . train_targets . extend ( entry [ \"fine_labels\" ]) self . train_x = np . vstack ( self . train_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . train_x = self . train_x . transpose (( 0 , 2 , 3 , 1 )) # convert to HWC for file_name , checksum in self . test_list : file_path = os . path . join ( self . root , self . base_folder , file_name ) with open ( file_path , \"rb\" ) as f : entry = pickle . load ( f , encoding = \"latin1\" ) self . test_x . append ( entry [ \"data\" ]) if \"labels\" in entry : self . test_targets . extend ( entry [ \"labels\" ]) else : self . test_targets . extend ( entry [ \"fine_labels\" ]) self . test_x = np . vstack ( self . test_x ) . reshape ( - 1 , 3 , 32 , 32 ) self . test_x = self . test_x . transpose (( 0 , 2 , 3 , 1 )) self . _load_meta ()","title":"CIFAR10"},{"location":"zh/api/reference/datasets/cifar/#iflearner.datasets.cifar.CIFAR100","text":"Bases: CIFAR10 CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html> _ Dataset. This is a subclass of the CIFAR10 Dataset.","title":"CIFAR100"},{"location":"zh/api/reference/datasets/fl_dataset/","text":"","title":"Fl dataset"},{"location":"zh/api/reference/datasets/mnist/","text":"EMNIST ( root , split = 'mnist' , ** kwargs ) \u00b6 Bases: MNIST EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_researc h/emnist> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where EMNIST/processed/training.pt and EMNIST/processed/test.pt exist. required split string The dataset has 6 different splits: byclass , bymerge , balanced , letters , digits and mnist . This argument specifies which one to use. 'mnist' train bool If True, creates dataset from training.pt , otherwise from test.pt . required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. required transform callable A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop required target_transform callable A function/transform that takes in the target and transforms it. required Source code in iflearner/datasets/mnist.py 209 210 211 212 213 214 def __init__ ( self , root : str , split : str = \"mnist\" , ** kwargs : Any ) -> None : self . split = verify_str_arg ( split , \"split\" , self . splits ) self . training_file = self . _training_file ( split ) self . test_file = self . _test_file ( split ) super ( EMNIST , self ) . __init__ ( root , ** kwargs ) self . classes = self . classes_split_dict [ self . split ] download () \u00b6 Download the EMNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def download ( self ) -> None : \"\"\"Download the EMNIST data if it doesn't exist in processed_folder already.\"\"\" import shutil if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files print ( \"Downloading and extracting zip archive\" ) download_and_extract_archive ( self . url , download_root = self . raw_folder , filename = \"emnist.zip\" , remove_finished = True , md5 = self . md5 , ) gzip_folder = os . path . join ( self . raw_folder , \"gzip\" ) for gzip_file in os . listdir ( gzip_folder ): if gzip_file . endswith ( \".gz\" ): extract_archive ( os . path . join ( gzip_folder , gzip_file ), gzip_folder ) # process and save as torch files for split in self . splits : print ( \"Processing \" + split ) training_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -train-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -train-labels-idx1-ubyte\" . format ( split ) ) ), ) test_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -test-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -test-labels-idx1-ubyte\" . format ( split ) ) ), ) with open ( os . path . join ( self . processed_folder , self . _training_file ( split )), \"wb\" ) as f : pickle . dump ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . _test_file ( split )), \"wb\" ) as f : pickle . dump ( test_set , f ) shutil . rmtree ( gzip_folder ) print ( \"Done!\" ) MNIST ( root , download = False ) \u00b6 Bases: FLDateset Source code in iflearner/datasets/mnist.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , root : str , download : bool = False ): super () . __init__ () self . root = root self . raw_folder = os . path . join ( self . root , self . __class__ . __name__ , \"raw\" ) self . processed_folder = os . path . join ( self . root , self . __class__ . __name__ , \"processed\" ) if download : self . download () with open ( os . path . join ( self . processed_folder , self . training_file ), \"rb\" ) as f : self . train_x , self . train_targets = pickle . load ( f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"rb\" ) as f : self . test_x , self . test_targets = pickle . load ( f ) download () \u00b6 Download the MNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download ( self ) -> None : \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\" if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files for url , md5 in self . resources : filename = url . rpartition ( \"/\" )[ 2 ] download_and_extract_archive ( url , download_root = self . raw_folder , filename = filename , md5 = md5 ) train_set = ( read_image_file ( os . path . join ( self . raw_folder , \"train-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"train-labels-idx1-ubyte\" )), ) test_set = ( read_image_file ( os . path . join ( self . raw_folder , \"t10k-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"t10k-labels-idx1-ubyte\" )), ) with open ( os . path . join ( self . processed_folder , self . training_file ), \"wb\" ) as f : pickle . dump ( train_set , f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"wb\" ) as f : pickle . dump ( test_set , f )","title":"Mnist"},{"location":"zh/api/reference/datasets/mnist/#iflearner.datasets.mnist.EMNIST","text":"Bases: MNIST EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_researc h/emnist> _ Dataset. Parameters: Name Type Description Default root string Root directory of dataset where EMNIST/processed/training.pt and EMNIST/processed/test.pt exist. required split string The dataset has 6 different splits: byclass , bymerge , balanced , letters , digits and mnist . This argument specifies which one to use. 'mnist' train bool If True, creates dataset from training.pt , otherwise from test.pt . required download bool If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. required transform callable A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop required target_transform callable A function/transform that takes in the target and transforms it. required Source code in iflearner/datasets/mnist.py 209 210 211 212 213 214 def __init__ ( self , root : str , split : str = \"mnist\" , ** kwargs : Any ) -> None : self . split = verify_str_arg ( split , \"split\" , self . splits ) self . training_file = self . _training_file ( split ) self . test_file = self . _test_file ( split ) super ( EMNIST , self ) . __init__ ( root , ** kwargs ) self . classes = self . classes_split_dict [ self . split ]","title":"EMNIST"},{"location":"zh/api/reference/datasets/mnist/#iflearner.datasets.mnist.EMNIST.download","text":"Download the EMNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def download ( self ) -> None : \"\"\"Download the EMNIST data if it doesn't exist in processed_folder already.\"\"\" import shutil if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files print ( \"Downloading and extracting zip archive\" ) download_and_extract_archive ( self . url , download_root = self . raw_folder , filename = \"emnist.zip\" , remove_finished = True , md5 = self . md5 , ) gzip_folder = os . path . join ( self . raw_folder , \"gzip\" ) for gzip_file in os . listdir ( gzip_folder ): if gzip_file . endswith ( \".gz\" ): extract_archive ( os . path . join ( gzip_folder , gzip_file ), gzip_folder ) # process and save as torch files for split in self . splits : print ( \"Processing \" + split ) training_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -train-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -train-labels-idx1-ubyte\" . format ( split ) ) ), ) test_set = ( read_image_file ( os . path . join ( gzip_folder , \"emnist- {} -test-images-idx3-ubyte\" . format ( split ) ) ), read_label_file ( os . path . join ( gzip_folder , \"emnist- {} -test-labels-idx1-ubyte\" . format ( split ) ) ), ) with open ( os . path . join ( self . processed_folder , self . _training_file ( split )), \"wb\" ) as f : pickle . dump ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . _test_file ( split )), \"wb\" ) as f : pickle . dump ( test_set , f ) shutil . rmtree ( gzip_folder ) print ( \"Done!\" )","title":"download()"},{"location":"zh/api/reference/datasets/mnist/#iflearner.datasets.mnist.MNIST","text":"Bases: FLDateset Source code in iflearner/datasets/mnist.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , root : str , download : bool = False ): super () . __init__ () self . root = root self . raw_folder = os . path . join ( self . root , self . __class__ . __name__ , \"raw\" ) self . processed_folder = os . path . join ( self . root , self . __class__ . __name__ , \"processed\" ) if download : self . download () with open ( os . path . join ( self . processed_folder , self . training_file ), \"rb\" ) as f : self . train_x , self . train_targets = pickle . load ( f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"rb\" ) as f : self . test_x , self . test_targets = pickle . load ( f )","title":"MNIST"},{"location":"zh/api/reference/datasets/mnist/#iflearner.datasets.mnist.MNIST.download","text":"Download the MNIST data if it doesn't exist in processed_folder already. Source code in iflearner/datasets/mnist.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download ( self ) -> None : \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\" if self . _check_exists (): return os . makedirs ( self . raw_folder , exist_ok = True ) os . makedirs ( self . processed_folder , exist_ok = True ) # download files for url , md5 in self . resources : filename = url . rpartition ( \"/\" )[ 2 ] download_and_extract_archive ( url , download_root = self . raw_folder , filename = filename , md5 = md5 ) train_set = ( read_image_file ( os . path . join ( self . raw_folder , \"train-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"train-labels-idx1-ubyte\" )), ) test_set = ( read_image_file ( os . path . join ( self . raw_folder , \"t10k-images-idx3-ubyte\" )), read_label_file ( os . path . join ( self . raw_folder , \"t10k-labels-idx1-ubyte\" )), ) with open ( os . path . join ( self . processed_folder , self . training_file ), \"wb\" ) as f : pickle . dump ( train_set , f ) with open ( os . path . join ( self . processed_folder , self . test_file ), \"wb\" ) as f : pickle . dump ( test_set , f )","title":"download()"},{"location":"zh/api/reference/datasets/sampler/","text":"","title":"Sampler"},{"location":"zh/api/reference/datasets/split_dataset/","text":"","title":"Split dataset"},{"location":"zh/api/reference/datasets/utils/","text":"download_file_from_google_drive ( file_id , root , filename = None , md5 = None ) \u00b6 Download a Google Drive file from and place it in root. Parameters: Name Type Description Default file_id str id of file to be downloaded required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the id of the file. None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def download_file_from_google_drive ( file_id : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ): \"\"\"Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url import requests url = \"https://docs.google.com/uc?export=download\" root = os . path . expanduser ( root ) if not filename : filename = file_id fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : session = requests . Session () response = session . get ( url , params = { \"id\" : file_id }, stream = True ) token = _get_confirm_token ( response ) if token : params = { \"id\" : file_id , \"confirm\" : token } response = session . get ( url , params = params , stream = True ) if _quota_exceeded ( response ): msg = ( f \"The daily quota of the file { filename } is exceeded and it \" f \"can't be downloaded. This is a limitation of Google Drive \" f \"and can only be overcome by trying again later.\" ) raise RuntimeError ( msg ) _save_response_content ( response , fpath ) download_url ( url , root , filename = None , md5 = None ) \u00b6 Download a file from a url and place it in root. Parameters: Name Type Description Default url str URL to download file from required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the basename of the URL None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download_url ( url : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ) -> None : \"\"\"Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" import urllib root = os . path . expanduser ( root ) if not filename : filename = os . path . basename ( url ) fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) # check if file is already present locally if check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : # download the file try : print ( \"Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) # type: ignore[attr-defined] except ( urllib . error . URLError , IOError ) as e : if url [: 5 ] == \"https\" : url = url . replace ( \"https:\" , \"http:\" ) print ( \"Failed download. Trying https -> http instead.\" \" Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) else : raise e # check integrity of downloaded file if not check_integrity ( fpath , md5 ): raise RuntimeError ( \"File not found or corrupted.\" ) list_dir ( root , prefix = False ) \u00b6 List all directories at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required prefix bool If true, prepends the path to each result, otherwise only returns the name of the directories found False Source code in iflearner/datasets/utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def list_dir ( root : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all directories at a given root. Args: root (str): Path to directory whose folders need to be listed prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the directories found \"\"\" root = os . path . expanduser ( root ) directories = [ p for p in os . listdir ( root ) if os . path . isdir ( os . path . join ( root , p ))] if prefix is True : directories = [ os . path . join ( root , d ) for d in directories ] return directories list_files ( root , suffix , prefix = False ) \u00b6 List all files ending with a suffix at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required suffix str or tuple Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly required prefix bool If true, prepends the path to each result, otherwise only returns the name of the files found False Source code in iflearner/datasets/utils.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def list_files ( root : str , suffix : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all files ending with a suffix at a given root. Args: root (str): Path to directory whose folders need to be listed suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the files found \"\"\" root = os . path . expanduser ( root ) files = [ p for p in os . listdir ( root ) if os . path . isfile ( os . path . join ( root , p )) and p . endswith ( suffix ) ] if prefix is True : files = [ os . path . join ( root , d ) for d in files ] return files open_maybe_compressed_file ( path ) \u00b6 Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument path is a string and ends with '.gz' or '.xz'. Source code in iflearner/datasets/utils.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def open_maybe_compressed_file ( path : Union [ str , IO ]) -> Union [ IO , gzip . GzipFile ]: \"\"\"Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument `path` is a string and ends with '.gz' or '.xz'. \"\"\" import torch if not isinstance ( path , ( str , bytes )): return path if path . endswith ( \".gz\" ): return gzip . open ( path , \"rb\" ) if path . endswith ( \".xz\" ): return lzma . open ( path , \"rb\" ) return open ( path , \"rb\" ) read_sn3_pascalvincent_tensor ( path , strict = True ) \u00b6 Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. Source code in iflearner/datasets/utils.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def read_sn3_pascalvincent_tensor ( path : Union [ str , IO ], strict : bool = True ) -> np . ndarray : \"\"\"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. \"\"\" # read with open_maybe_compressed_file ( path ) as f : data = f . read () # parse magic = get_int ( data [ 0 : 4 ]) nd = magic % 256 ty = magic // 256 assert nd >= 1 and nd <= 3 assert ty >= 8 and ty <= 14 m = SN3_PASCALVINCENT_TYPEMAP [ ty ] s = [ get_int ( data [ 4 * ( i + 1 ): 4 * ( i + 2 )]) for i in range ( nd )] parsed = np . frombuffer ( data , dtype = m [ 1 ], offset = ( 4 * ( nd + 1 ))) assert parsed . shape [ 0 ] == np . prod ( s ) or not strict return parsed . astype ( m [ 1 ]) . reshape ( s )","title":"Utils"},{"location":"zh/api/reference/datasets/utils/#iflearner.datasets.utils.download_file_from_google_drive","text":"Download a Google Drive file from and place it in root. Parameters: Name Type Description Default file_id str id of file to be downloaded required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the id of the file. None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def download_file_from_google_drive ( file_id : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ): \"\"\"Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url import requests url = \"https://docs.google.com/uc?export=download\" root = os . path . expanduser ( root ) if not filename : filename = file_id fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : session = requests . Session () response = session . get ( url , params = { \"id\" : file_id }, stream = True ) token = _get_confirm_token ( response ) if token : params = { \"id\" : file_id , \"confirm\" : token } response = session . get ( url , params = params , stream = True ) if _quota_exceeded ( response ): msg = ( f \"The daily quota of the file { filename } is exceeded and it \" f \"can't be downloaded. This is a limitation of Google Drive \" f \"and can only be overcome by trying again later.\" ) raise RuntimeError ( msg ) _save_response_content ( response , fpath )","title":"download_file_from_google_drive()"},{"location":"zh/api/reference/datasets/utils/#iflearner.datasets.utils.download_url","text":"Download a file from a url and place it in root. Parameters: Name Type Description Default url str URL to download file from required root str Directory to place downloaded file in required filename str Name to save the file under. If None, use the basename of the URL None md5 str MD5 checksum of the download. If None, do not check None Source code in iflearner/datasets/utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def download_url ( url : str , root : str , filename : Optional [ str ] = None , md5 : Optional [ str ] = None ) -> None : \"\"\"Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check \"\"\" import urllib root = os . path . expanduser ( root ) if not filename : filename = os . path . basename ( url ) fpath = os . path . join ( root , filename ) os . makedirs ( root , exist_ok = True ) # check if file is already present locally if check_integrity ( fpath , md5 ): print ( \"Using downloaded and verified file: \" + fpath ) else : # download the file try : print ( \"Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) # type: ignore[attr-defined] except ( urllib . error . URLError , IOError ) as e : if url [: 5 ] == \"https\" : url = url . replace ( \"https:\" , \"http:\" ) print ( \"Failed download. Trying https -> http instead.\" \" Downloading \" + url + \" to \" + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ()) else : raise e # check integrity of downloaded file if not check_integrity ( fpath , md5 ): raise RuntimeError ( \"File not found or corrupted.\" )","title":"download_url()"},{"location":"zh/api/reference/datasets/utils/#iflearner.datasets.utils.list_dir","text":"List all directories at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required prefix bool If true, prepends the path to each result, otherwise only returns the name of the directories found False Source code in iflearner/datasets/utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def list_dir ( root : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all directories at a given root. Args: root (str): Path to directory whose folders need to be listed prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the directories found \"\"\" root = os . path . expanduser ( root ) directories = [ p for p in os . listdir ( root ) if os . path . isdir ( os . path . join ( root , p ))] if prefix is True : directories = [ os . path . join ( root , d ) for d in directories ] return directories","title":"list_dir()"},{"location":"zh/api/reference/datasets/utils/#iflearner.datasets.utils.list_files","text":"List all files ending with a suffix at a given root. Parameters: Name Type Description Default root str Path to directory whose folders need to be listed required suffix str or tuple Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly required prefix bool If true, prepends the path to each result, otherwise only returns the name of the files found False Source code in iflearner/datasets/utils.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def list_files ( root : str , suffix : str , prefix : bool = False ) -> List [ str ]: \"\"\"List all files ending with a suffix at a given root. Args: root (str): Path to directory whose folders need to be listed suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python \"str.endswith\" method and is passed directly prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the files found \"\"\" root = os . path . expanduser ( root ) files = [ p for p in os . listdir ( root ) if os . path . isfile ( os . path . join ( root , p )) and p . endswith ( suffix ) ] if prefix is True : files = [ os . path . join ( root , d ) for d in files ] return files","title":"list_files()"},{"location":"zh/api/reference/datasets/utils/#iflearner.datasets.utils.open_maybe_compressed_file","text":"Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument path is a string and ends with '.gz' or '.xz'. Source code in iflearner/datasets/utils.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def open_maybe_compressed_file ( path : Union [ str , IO ]) -> Union [ IO , gzip . GzipFile ]: \"\"\"Return a file object that possibly decompresses 'path' on the fly. Decompression occurs when argument `path` is a string and ends with '.gz' or '.xz'. \"\"\" import torch if not isinstance ( path , ( str , bytes )): return path if path . endswith ( \".gz\" ): return gzip . open ( path , \"rb\" ) if path . endswith ( \".xz\" ): return lzma . open ( path , \"rb\" ) return open ( path , \"rb\" )","title":"open_maybe_compressed_file()"},{"location":"zh/api/reference/datasets/utils/#iflearner.datasets.utils.read_sn3_pascalvincent_tensor","text":"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. Source code in iflearner/datasets/utils.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def read_sn3_pascalvincent_tensor ( path : Union [ str , IO ], strict : bool = True ) -> np . ndarray : \"\"\"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx- io.lsh'). Argument may be a filename, compressed filename, or file object. \"\"\" # read with open_maybe_compressed_file ( path ) as f : data = f . read () # parse magic = get_int ( data [ 0 : 4 ]) nd = magic % 256 ty = magic // 256 assert nd >= 1 and nd <= 3 assert ty >= 8 and ty <= 14 m = SN3_PASCALVINCENT_TYPEMAP [ ty ] s = [ get_int ( data [ 4 * ( i + 1 ): 4 * ( i + 2 )]) for i in range ( nd )] parsed = np . frombuffer ( data , dtype = m [ 1 ], offset = ( 4 * ( nd + 1 ))) assert parsed . shape [ 0 ] == np . prod ( s ) or not strict return parsed . astype ( m [ 1 ]) . reshape ( s )","title":"read_sn3_pascalvincent_tensor()"},{"location":"zh/about/changelog/","text":"\u7248\u672c\u65e5\u5fd7 \u00b6 \u6b64\u9879\u76ee\u7684\u6240\u6709\u663e\u7740\u66f4\u6539\u90fd\u5c06\u8bb0\u5f55\u5728\u6b64\u6587\u4ef6\u4e2d\u3002 iflearner \u00b6 [Unreleased] \u00b6 Add \u00b6 \u652f\u6301GRPC\u5e95\u5c42\u901a\u4fe1\u534f\u8bae\uff0c\u5e76\u5b8c\u6210\u4e0a\u5c42\u534f\u8bae\u62bd\u8c61\u5316 \u652f\u6301Tensorflow\u3001Pytorch\u3001Mxnet\u3001Keras\u7b49\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u63a5\u5165 \u652f\u6301\u5e38\u89c1\u7684\u805a\u5408\u7b56\u7565\uff0c\u5e76\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u5b9e\u73b0\u81ea\u5df1\u7684\u805a\u5408\u7b56\u7565 \u652f\u6301SMPC\u3001\u5dee\u5206\u9690\u79c1\u7684\u5b89\u5168\u52a0\u5bc6\u7b56\u7565","title":"ChangeLog"},{"location":"zh/about/changelog/#_1","text":"\u6b64\u9879\u76ee\u7684\u6240\u6709\u663e\u7740\u66f4\u6539\u90fd\u5c06\u8bb0\u5f55\u5728\u6b64\u6587\u4ef6\u4e2d\u3002","title":"\u7248\u672c\u65e5\u5fd7"},{"location":"zh/about/changelog/#iflearner","text":"","title":"iflearner"},{"location":"zh/about/changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"zh/about/changelog/#add","text":"\u652f\u6301GRPC\u5e95\u5c42\u901a\u4fe1\u534f\u8bae\uff0c\u5e76\u5b8c\u6210\u4e0a\u5c42\u534f\u8bae\u62bd\u8c61\u5316 \u652f\u6301Tensorflow\u3001Pytorch\u3001Mxnet\u3001Keras\u7b49\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u63a5\u5165 \u652f\u6301\u5e38\u89c1\u7684\u805a\u5408\u7b56\u7565\uff0c\u5e76\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u5b9e\u73b0\u81ea\u5df1\u7684\u805a\u5408\u7b56\u7565 \u652f\u6301SMPC\u3001\u5dee\u5206\u9690\u79c1\u7684\u5b89\u5168\u52a0\u5bc6\u7b56\u7565","title":"Add"},{"location":"zh/about/contact/","text":"\u8054\u7cfb\u6211\u4eec \u00b6 \u5982\u679c\u60a8\u6709\u4efb\u4f55\u95ee\u9898\u60f3\u89e3\u51b3\u6216\u8005\u60f3\u53c2\u4e0e\u6211\u4eec\u4e00\u8d77\u5171\u540c\u6784\u5efa\u9879\u76ee, \u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u8054\u7cfb\u9014\u5f84\u8054\u7cfb\u5230\u6211\u4eec\uff0c \u6211\u4eec\u975e\u5e38\u6b22\u8fce\u60a8\u7684\u52a0\u5165\u3002 \u8054\u7cfb\u8def\u5f84 \u00b6 \u53ef\u4ee5\u52a0\u5165\u6211\u4eec\u7684\u5fae\u4fe1\u7fa4\u3002 \u5bf9\u4e8e\u5e38\u89c1\u95ee\u9898, \u6211\u4eec\u4e3a\u60a8\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684 FAQ \u6587\u6863\u3002 \u8bf7\u4f7f\u7528 issues \u63d0\u4ea4BUG\u3002 \u8bf7\u4f7f\u7528 pull requests \u63d0\u4ea4\u3001\u8d21\u732e\u4ee3\u7801\u3002","title":"Contact Us"},{"location":"zh/about/contact/#_1","text":"\u5982\u679c\u60a8\u6709\u4efb\u4f55\u95ee\u9898\u60f3\u89e3\u51b3\u6216\u8005\u60f3\u53c2\u4e0e\u6211\u4eec\u4e00\u8d77\u5171\u540c\u6784\u5efa\u9879\u76ee, \u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u8054\u7cfb\u9014\u5f84\u8054\u7cfb\u5230\u6211\u4eec\uff0c \u6211\u4eec\u975e\u5e38\u6b22\u8fce\u60a8\u7684\u52a0\u5165\u3002","title":"\u8054\u7cfb\u6211\u4eec"},{"location":"zh/about/contact/#_2","text":"\u53ef\u4ee5\u52a0\u5165\u6211\u4eec\u7684\u5fae\u4fe1\u7fa4\u3002 \u5bf9\u4e8e\u5e38\u89c1\u95ee\u9898, \u6211\u4eec\u4e3a\u60a8\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684 FAQ \u6587\u6863\u3002 \u8bf7\u4f7f\u7528 issues \u63d0\u4ea4BUG\u3002 \u8bf7\u4f7f\u7528 pull requests \u63d0\u4ea4\u3001\u8d21\u732e\u4ee3\u7801\u3002","title":"\u8054\u7cfb\u8def\u5f84"},{"location":"zh/api/api_reference/","text":"API Reference \u00b6 Trainer \u00b6 1. class iflearner.business.homo.trainer.Trainer \u00b6 \u8fd9\u662f\u60a8\u5b9e\u73b0\u81ea\u5df1\u7684\u5ba2\u6237\u7aef\u65f6\u8981\u7ee7\u627f\u7684\u57fa\u672c\u7c7b\uff0c\u5b83\u6709\u56db\u4e2a\u62bd\u8c61\u51fd\u6570\uff0c\u5176\u4e2d\u5305\u542b get , set , fit \u548c evaluate . def get ( self , param_type = ParameterType . ParameterModel ) -> dict : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement how to get the parameters or gradients from your model, and we call this function to get your model information. Then, we will send it to server for aggregating with other clients. IN: param_type: which one you want to get, parameter or gradient - ParameterType.ParameterModel (default) - ParameterType.ParameterGradient OUT: dict: k: str (the parameter name), v: np.ndarray (the parameter value) def set(self, parameters: dict, param_type=ParameterType.ParameterModel) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement how to set the parameters or gradients to your model , and we will call this function when we got aggregating result from server. IN : dict : the same as the return of `` get `` function param_type : which one you want to set , parameter or gradient - ParameterType . ParameterModel ( default ) - ParameterType . ParameterGradient OUT : none def fit ( self , epoch : int ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement the training process on one epoch, and we will call this function per epoch. IN: epoch: the index of epoch OUT: none def evaluate(self, epoch: int) -> dict: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement the evaluating process for model and return the metrics . We will call this function after called `` fit `` function at every time . IN : epoch : the index of epoch OUT : dict : k : str ( metric name ), v : float ( metric value ) 2. class iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u00b6 \u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 PyTorch \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002 3. class iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer \u00b6 \u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 Tensorflow \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002 4. class iflearner.business.homo.mxnet_trainer.MxnetTrainer \u00b6 \u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 Mxnet \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002 5. class iflearner.business.homo.keras_trainer.KerasTrainer \u00b6 \u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 Keras \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002 Command Arguments \u00b6 iflearner.business.homo.argument.parser \u6211\u4eec\u9884\u5148\u63d0\u4f9b\u4e86\u4e00\u4e9b\u547d\u4ee4\u53c2\u6570\uff0c\u60a8\u9700\u8981\u5728\u7a0b\u5e8f\u5f00\u59cb\u65f6\u8c03\u7528 parser.parse_args \u51fd\u6570\u3002 \u5f53\u7136\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5728\u89e3\u6790\u547d\u4ee4\u53c2\u6570\u4e4b\u524d\u8c03\u7528 parser.add_argument \u51fd\u6570\u6765\u6dfb\u52a0\u81ea\u5df1\u7684\u547d\u4ee4\u53c2\u6570\u3002 \u6709\u5982\u4e0b\u9ed8\u8ba4\u53c2\u6570\uff1a name \u60a8\u53ef\u4ee5\u6307\u5b9a\u5ba2\u6237\u7aef\u7684\u540d\u79f0\uff0c\u540d\u79f0\u5fc5\u987b\u662f\u552f\u4e00\u7684\uff0c\u4e0d\u80fd\u4e0e\u5176\u4ed6\u5ba2\u6237\u7aef\u76f8\u540c\u3002 epochs \u60a8\u53ef\u4ee5\u6307\u5b9a\u8bad\u7ec3\u7684\u603b\u8f6e\u6570\uff0c\u5f53\u8bad\u7ec3\u5b8c\u6210\u65f6\uff0c\u5ba2\u6237\u7aef\u5c06\u81ea\u52a8\u9000\u51fa\u3002 server \u60a8\u53ef\u4ee5\u6307\u5b9a\u670d\u52a1\u7aef\u94fe\u63a5\u5730\u5740\uff0c\u4f8b\u5982\uff1a\"192.168.0.1:50001\"\u3002 enable-ll \u6211\u4eec\u540c\u65f6\u63d0\u4f9b\u672c\u5730\u57f9\u8bad\u65b9\u5f0f\uff0c\u53ea\u4f7f\u7528\u60a8\u81ea\u5df1\u7684\u6570\u636e\uff0c\u56e0\u6b64\u60a8\u53ef\u4ee5\u6bd4\u8f83\u8054\u90a6\u57f9\u8bad\u4e0e\u672c\u5730\u57f9\u8bad\u7684\u7ed3\u679c\u3002 \u53c2\u6570\u503c\u4e3a 1\uff08\u542f\u7528\uff09\u6216 0\uff08\u7981\u7528\uff0c\u9ed8\u8ba4\u503c\uff09\u3002 peers \u6211\u4eec\u4e3a\u5b89\u5168\u805a\u5408\u63d0\u4f9b SMPC\uff0c\u60a8\u53ef\u4ee5\u6307\u5b9a\u5730\u5740\u6765\u542f\u7528\u6b64\u529f\u80fd\u3002 \u4f8b\u5982\uff1a'192.168.0.1:50010;192.168.0.2:50010;192.168.0.3:50010'. \u7b2c\u4e00\u4e2a\u5730\u5740\u662f\u60a8\u81ea\u5df1\u7684\u5730\u5740\uff0c\u540e\u9762\u7684\u5176\u4ed6\u5730\u5740\u662f\u5176\u4ed6\u5ba2\u6237\u7684\u5730\u5740\u3002 \u6240\u6709\u5730\u5740\u90fd\u4f7f\u7528\u5206\u53f7\u5206\u9694\u3002 Controller \u00b6 class iflearner.business.homo.train_client.Controller \u8be5\u7c7b\u662f\u5ba2\u6237\u7aef\u7684\u9a71\u52a8\u7a0b\u5e8f\u5e76\u63a7\u5236\u6574\u4e2a\u8fc7\u7a0b\uff0c\u56e0\u6b64\u60a8\u5c06\u5b9e\u4f8b\u5316\u8be5\u7c7b\u4ee5\u542f\u52a8\u60a8\u7684\u5ba2\u6237\u7aef\u3002 def __init__ ( self , args , trainer : Trainer ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" class initialization function IN: args: the return of ``iflearner.business.homo.argument.parser.parse_args`` trainer: the instance of ``iflearner.business.homo.trainer.Trainer`` OUT: none def run(self) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" You would call this function after your client has ready , and this function will block until training process has been completed .","title":"API REFERENCE"},{"location":"zh/api/api_reference/#api-reference","text":"","title":"API Reference"},{"location":"zh/api/api_reference/#trainer","text":"","title":"Trainer"},{"location":"zh/api/api_reference/#1-class-iflearnerbusinesshomotrainertrainer","text":"\u8fd9\u662f\u60a8\u5b9e\u73b0\u81ea\u5df1\u7684\u5ba2\u6237\u7aef\u65f6\u8981\u7ee7\u627f\u7684\u57fa\u672c\u7c7b\uff0c\u5b83\u6709\u56db\u4e2a\u62bd\u8c61\u51fd\u6570\uff0c\u5176\u4e2d\u5305\u542b get , set , fit \u548c evaluate . def get ( self , param_type = ParameterType . ParameterModel ) -> dict : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement how to get the parameters or gradients from your model, and we call this function to get your model information. Then, we will send it to server for aggregating with other clients. IN: param_type: which one you want to get, parameter or gradient - ParameterType.ParameterModel (default) - ParameterType.ParameterGradient OUT: dict: k: str (the parameter name), v: np.ndarray (the parameter value) def set(self, parameters: dict, param_type=ParameterType.ParameterModel) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement how to set the parameters or gradients to your model , and we will call this function when we got aggregating result from server. IN : dict : the same as the return of `` get `` function param_type : which one you want to set , parameter or gradient - ParameterType . ParameterModel ( default ) - ParameterType . ParameterGradient OUT : none def fit ( self , epoch : int ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function, you would implement the training process on one epoch, and we will call this function per epoch. IN: epoch: the index of epoch OUT: none def evaluate(self, epoch: int) -> dict: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" In this function , you would implement the evaluating process for model and return the metrics . We will call this function after called `` fit `` function at every time . IN : epoch : the index of epoch OUT : dict : k : str ( metric name ), v : float ( metric value )","title":"1. class iflearner.business.homo.trainer.Trainer"},{"location":"zh/api/api_reference/#2-class-iflearnerbusinesshomopytorch_trainerpytorchtrainer","text":"\u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 PyTorch \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002","title":"2. class iflearner.business.homo.pytorch_trainer.PyTorchTrainer"},{"location":"zh/api/api_reference/#3-class-iflearnerbusinesshomotensorflow_trainertensorflowtrainer","text":"\u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 Tensorflow \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002","title":"3. class iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer"},{"location":"zh/api/api_reference/#4-class-iflearnerbusinesshomomxnet_trainermxnettrainer","text":"\u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 Mxnet \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002","title":"4. class iflearner.business.homo.mxnet_trainer.MxnetTrainer"},{"location":"zh/api/api_reference/#5-class-iflearnerbusinesshomokeras_trainerkerastrainer","text":"\u8fd9\u4e2a\u7c7b\u7ee7\u627f\u81ea iflearner.business.homo.trainer.Trainer \u5e76\u5b9e\u73b0\u4e86 get \u548c set \u8fd9\u56db\u4e2a\u51fd\u6570\u4e2d\u7684\u4e24\u4e2a\u3002 \u5f53\u4f60\u4f7f\u7528 Keras \u6846\u67b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u9009\u62e9\u7ee7\u627f\u8fd9\u4e2a\u7c7b\uff0c\u800c\u4e0d\u662f iflearner.business.homo.trainer.Trainer \u3002 \u7136\u540e\uff0c\u60a8\u53ea\u9700\u8981\u5b9e\u73b0 fit \u548c evaluate \u51fd\u6570\u3002","title":"5. class iflearner.business.homo.keras_trainer.KerasTrainer"},{"location":"zh/api/api_reference/#command-arguments","text":"iflearner.business.homo.argument.parser \u6211\u4eec\u9884\u5148\u63d0\u4f9b\u4e86\u4e00\u4e9b\u547d\u4ee4\u53c2\u6570\uff0c\u60a8\u9700\u8981\u5728\u7a0b\u5e8f\u5f00\u59cb\u65f6\u8c03\u7528 parser.parse_args \u51fd\u6570\u3002 \u5f53\u7136\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5728\u89e3\u6790\u547d\u4ee4\u53c2\u6570\u4e4b\u524d\u8c03\u7528 parser.add_argument \u51fd\u6570\u6765\u6dfb\u52a0\u81ea\u5df1\u7684\u547d\u4ee4\u53c2\u6570\u3002 \u6709\u5982\u4e0b\u9ed8\u8ba4\u53c2\u6570\uff1a name \u60a8\u53ef\u4ee5\u6307\u5b9a\u5ba2\u6237\u7aef\u7684\u540d\u79f0\uff0c\u540d\u79f0\u5fc5\u987b\u662f\u552f\u4e00\u7684\uff0c\u4e0d\u80fd\u4e0e\u5176\u4ed6\u5ba2\u6237\u7aef\u76f8\u540c\u3002 epochs \u60a8\u53ef\u4ee5\u6307\u5b9a\u8bad\u7ec3\u7684\u603b\u8f6e\u6570\uff0c\u5f53\u8bad\u7ec3\u5b8c\u6210\u65f6\uff0c\u5ba2\u6237\u7aef\u5c06\u81ea\u52a8\u9000\u51fa\u3002 server \u60a8\u53ef\u4ee5\u6307\u5b9a\u670d\u52a1\u7aef\u94fe\u63a5\u5730\u5740\uff0c\u4f8b\u5982\uff1a\"192.168.0.1:50001\"\u3002 enable-ll \u6211\u4eec\u540c\u65f6\u63d0\u4f9b\u672c\u5730\u57f9\u8bad\u65b9\u5f0f\uff0c\u53ea\u4f7f\u7528\u60a8\u81ea\u5df1\u7684\u6570\u636e\uff0c\u56e0\u6b64\u60a8\u53ef\u4ee5\u6bd4\u8f83\u8054\u90a6\u57f9\u8bad\u4e0e\u672c\u5730\u57f9\u8bad\u7684\u7ed3\u679c\u3002 \u53c2\u6570\u503c\u4e3a 1\uff08\u542f\u7528\uff09\u6216 0\uff08\u7981\u7528\uff0c\u9ed8\u8ba4\u503c\uff09\u3002 peers \u6211\u4eec\u4e3a\u5b89\u5168\u805a\u5408\u63d0\u4f9b SMPC\uff0c\u60a8\u53ef\u4ee5\u6307\u5b9a\u5730\u5740\u6765\u542f\u7528\u6b64\u529f\u80fd\u3002 \u4f8b\u5982\uff1a'192.168.0.1:50010;192.168.0.2:50010;192.168.0.3:50010'. \u7b2c\u4e00\u4e2a\u5730\u5740\u662f\u60a8\u81ea\u5df1\u7684\u5730\u5740\uff0c\u540e\u9762\u7684\u5176\u4ed6\u5730\u5740\u662f\u5176\u4ed6\u5ba2\u6237\u7684\u5730\u5740\u3002 \u6240\u6709\u5730\u5740\u90fd\u4f7f\u7528\u5206\u53f7\u5206\u9694\u3002","title":"Command Arguments"},{"location":"zh/api/api_reference/#controller","text":"class iflearner.business.homo.train_client.Controller \u8be5\u7c7b\u662f\u5ba2\u6237\u7aef\u7684\u9a71\u52a8\u7a0b\u5e8f\u5e76\u63a7\u5236\u6574\u4e2a\u8fc7\u7a0b\uff0c\u56e0\u6b64\u60a8\u5c06\u5b9e\u4f8b\u5316\u8be5\u7c7b\u4ee5\u542f\u52a8\u60a8\u7684\u5ba2\u6237\u7aef\u3002 def __init__ ( self , args , trainer : Trainer ) -> None : \"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" class initialization function IN: args: the return of ``iflearner.business.homo.argument.parser.parse_args`` trainer: the instance of ``iflearner.business.homo.trainer.Trainer`` OUT: none def run(self) -> None: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" You would call this function after your client has ready , and this function will block until training process has been completed .","title":"Controller"},{"location":"zh/faq/faq/","text":"FAQ \u00b6","title":"Deploy"},{"location":"zh/faq/faq/#faq","text":"","title":"FAQ"},{"location":"zh/quick_start/installation/","text":"\u5b89\u88c5IFLearner \u00b6 Python\u7248\u672c \u00b6 IFLearner \u9700\u8981 Python 3.7 \u6216\u66f4\u9ad8\u7248\u672c\u3002 \u5b89\u88c5 \u00b6 \u60a8\u53ef\u4ee5\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5feb\u901f\u5b89\u88c5\uff1a pip install iflearner \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u662f\u5426\u5df2\u6210\u529f\u5b89\u88c5\uff1a python -c \"import iflearner;print(iflearner.__version__)\"","title":"Installation"},{"location":"zh/quick_start/installation/#iflearner","text":"","title":"\u5b89\u88c5IFLearner"},{"location":"zh/quick_start/installation/#python","text":"IFLearner \u9700\u8981 Python 3.7 \u6216\u66f4\u9ad8\u7248\u672c\u3002","title":"Python\u7248\u672c"},{"location":"zh/quick_start/installation/#_1","text":"\u60a8\u53ef\u4ee5\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5feb\u901f\u5b89\u88c5\uff1a pip install iflearner \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u662f\u5426\u5df2\u6210\u529f\u5b89\u88c5\uff1a python -c \"import iflearner;print(iflearner.__version__)\"","title":"\u5b89\u88c5"},{"location":"zh/quick_start/quickstart_keras/","text":"\u5feb\u901f\u5f00\u59cb (Keras) \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Keras \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 Keras \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 Keras \u5e93: pip install keras == 2 .9.0 Ifleaner Server \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3 Ifleaner Client \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_keras.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c: 1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784 \u00b6 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from typing import Any , Tuple from keras.layers import Activation , Dense from keras.models import Sequential from keras.optimizers import RMSprop from keras.utils import np_utils # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 ) 2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b \u00b6 \u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.keras_trainer.KerasTrainer \u7c7b\u3002KerasTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: class Mnist ( KerasTrainer ): def __init__ ( self ): # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 ) self . _model = model super ( Mnist , self ) . __init__ ( model = model ) ( x_train , y_train ), ( x_test , y_test ) = self . _load_data () self . _x_train = x_train self . _y_train = y_train self . _x_test = x_test self . _y_test = y_test @staticmethod def _load_data () -> Dataset : # \u4e0b\u8f7dMNIST\u6570\u636e # X shape(60000, 28*28) y shape(10000, ) ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # \u6570\u636e\u9884\u5904\u7406 x_train = x_train . reshape ( x_train . shape [ 0 ], - 1 ) / 255 # normalize x_test = x_test . reshape ( x_test . shape [ 0 ], - 1 ) / 255 # normalize # \u5c06\u7c7b\u5411\u91cf\u8f6c\u5316\u4e3a\u7c7b\u77e9\u9635 \u6570\u5b57 5 \u8f6c\u6362\u4e3a 0 0 0 0 0 1 0 0 0 0 \u77e9\u9635 y_train = np_utils . to_categorical ( y_train , num_classes = 10 ) y_test = np_utils . to_categorical ( y_test , num_classes = 10 ) return ( x_train , y_train ), ( x_test , y_test ) def fit ( self , epoch : int ): self . _model . fit ( self . _x_train , self . _y_train , epochs = 1 , batch_size = 32 ) def evaluate ( self , epoch : int ) -> dict : loss , accuracy = self . _model . evaluate ( self . _x_test , self . _y_test ) print ( f \"epoch: { epoch } | accuracy: { accuracy } | loss: { loss } \" ) return { \"loss\" : loss , \"accuracy\" : accuracy } 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u00b6 \u6700\u540e\uff0c\u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a main \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef\u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c: if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_keras.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_keras.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms (iflearner) yiyezhiqiu:quickstart_mxnet lucky$ cd ../quickstart_keras/ (iflearner) yiyezhiqiu:quickstart_keras lucky$ python quickstart_keras.py --name client01 --epochs 2 Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:28:25.569565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(RMSprop, self).__init__(name, **kwargs) 2022-08-03 18:28:27.137 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:28:27.384 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 246.81410300000016ms 2022-08-03 18:28:27.385 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:28:27.386 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:28:27.391 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 4.523920999998765ms 2022-08-03 18:28:54.529 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4368589999946835ms 2022-08-03 18:28:55.466 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 6s 2ms/step - loss: 0.3668 - accuracy: 0.8968 2022-08-03 18:29:01.852 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 1s 2ms/step - loss: 0.2343 - accuracy: 0.9348 epoch:1 | accuracy:0.9348000288009644 | loss:0.23433993756771088 2022-08-03 18:29:02.782 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:29:02.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 4.293857000000401ms 2022-08-03 18:29:03.773 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 13.262433999997825ms 2022-08-03 18:29:03.795 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:29:03.797 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.148008999997785ms 2022-08-03 18:29:04.778 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4469219999995175ms 2022-08-03 18:29:04.800 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2399 - accuracy: 0.9317 2022-08-03 18:29:09.112 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.9448 epoch:2 | accuracy:0.9448000192642212 | loss:0.18558283150196075 2022-08-03 18:29:09.686 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.23433993756771088, 0.18558283150196075]) label: LT, points: ([1], [0.23433993756771088]) label: FT, points: ([1, 2], [0.9348000288009644, 0.9448000192642212]) label: LT, points: ([1], [0.9348000288009644]) 2022-08-03 18:29:10.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.3958279999997103ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 Quickstart_Keras \u3002","title":"Quickstart Keras"},{"location":"zh/quick_start/quickstart_keras/#keras","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Keras \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 Keras \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 Keras \u5e93: pip install keras == 2 .9.0","title":"\u5feb\u901f\u5f00\u59cb (Keras)"},{"location":"zh/quick_start/quickstart_keras/#ifleaner-server","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3","title":"Ifleaner Server"},{"location":"zh/quick_start/quickstart_keras/#ifleaner-client","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_keras.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c:","title":"Ifleaner Client"},{"location":"zh/quick_start/quickstart_keras/#1","text":"\u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from typing import Any , Tuple from keras.layers import Activation , Dense from keras.models import Sequential from keras.optimizers import RMSprop from keras.utils import np_utils # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 )","title":"1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784"},{"location":"zh/quick_start/quickstart_keras/#2-trainer","text":"\u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.keras_trainer.KerasTrainer \u7c7b\u3002KerasTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: class Mnist ( KerasTrainer ): def __init__ ( self ): # Another way to build your neural net model : Any = Sequential ( [ Dense ( 32 , input_dim = 784 ), # \u8f93\u5165\u503c784(28*28) => \u8f93\u51fa\u503c32 Activation ( \"relu\" ), # \u6fc0\u52b1\u51fd\u6570 \u8f6c\u6362\u6210\u975e\u7ebf\u6027\u6570\u636e Dense ( 10 ), # \u8f93\u51fa\u4e3a10\u4e2a\u5355\u4f4d\u7684\u7ed3\u679c Activation ( \"softmax\" ), # \u6fc0\u52b1\u51fd\u6570 \u8c03\u7528softmax\u8fdb\u884c\u5206\u7c7b ] ) # Another way to define your optimizer rmsprop = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # \u5b66\u4e60\u7387lr # We add metrics to get more results you want to see # \u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc model . compile ( optimizer = rmsprop , # \u52a0\u901f\u795e\u7ecf\u7f51\u7edc loss = \"categorical_crossentropy\" , # \u635f\u5931\u51fd\u6570 metrics = [ \"accuracy\" ], # \u8ba1\u7b97\u8bef\u5dee\u6216\u51c6\u786e\u7387 ) self . _model = model super ( Mnist , self ) . __init__ ( model = model ) ( x_train , y_train ), ( x_test , y_test ) = self . _load_data () self . _x_train = x_train self . _y_train = y_train self . _x_test = x_test self . _y_test = y_test @staticmethod def _load_data () -> Dataset : # \u4e0b\u8f7dMNIST\u6570\u636e # X shape(60000, 28*28) y shape(10000, ) ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # \u6570\u636e\u9884\u5904\u7406 x_train = x_train . reshape ( x_train . shape [ 0 ], - 1 ) / 255 # normalize x_test = x_test . reshape ( x_test . shape [ 0 ], - 1 ) / 255 # normalize # \u5c06\u7c7b\u5411\u91cf\u8f6c\u5316\u4e3a\u7c7b\u77e9\u9635 \u6570\u5b57 5 \u8f6c\u6362\u4e3a 0 0 0 0 0 1 0 0 0 0 \u77e9\u9635 y_train = np_utils . to_categorical ( y_train , num_classes = 10 ) y_test = np_utils . to_categorical ( y_test , num_classes = 10 ) return ( x_train , y_train ), ( x_test , y_test ) def fit ( self , epoch : int ): self . _model . fit ( self . _x_train , self . _y_train , epochs = 1 , batch_size = 32 ) def evaluate ( self , epoch : int ) -> dict : loss , accuracy = self . _model . evaluate ( self . _x_test , self . _y_test ) print ( f \"epoch: { epoch } | accuracy: { accuracy } | loss: { loss } \" ) return { \"loss\" : loss , \"accuracy\" : accuracy }","title":"2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b"},{"location":"zh/quick_start/quickstart_keras/#3-iflearner","text":"\u6700\u540e\uff0c\u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a main \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef\u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c: if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_keras.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_keras.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms (iflearner) yiyezhiqiu:quickstart_mxnet lucky$ cd ../quickstart_keras/ (iflearner) yiyezhiqiu:quickstart_keras lucky$ python quickstart_keras.py --name client01 --epochs 2 Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:28:25.569565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(RMSprop, self).__init__(name, **kwargs) 2022-08-03 18:28:27.137 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:28:27.384 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 246.81410300000016ms 2022-08-03 18:28:27.385 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:28:27.386 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:28:27.391 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 4.523920999998765ms 2022-08-03 18:28:54.529 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4368589999946835ms 2022-08-03 18:28:55.466 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 6s 2ms/step - loss: 0.3668 - accuracy: 0.8968 2022-08-03 18:29:01.852 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 1s 2ms/step - loss: 0.2343 - accuracy: 0.9348 epoch:1 | accuracy:0.9348000288009644 | loss:0.23433993756771088 2022-08-03 18:29:02.782 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:29:02.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 4.293857000000401ms 2022-08-03 18:29:03.773 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 13.262433999997825ms 2022-08-03 18:29:03.795 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:29:03.797 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.148008999997785ms 2022-08-03 18:29:04.778 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.4469219999995175ms 2022-08-03 18:29:04.800 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2399 - accuracy: 0.9317 2022-08-03 18:29:09.112 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- 313/313 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.9448 epoch:2 | accuracy:0.9448000192642212 | loss:0.18558283150196075 2022-08-03 18:29:09.686 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.23433993756771088, 0.18558283150196075]) label: LT, points: ([1], [0.23433993756771088]) label: FT, points: ([1, 2], [0.9348000288009644, 0.9448000192642212]) label: LT, points: ([1], [0.9348000288009644]) 2022-08-03 18:29:10.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.3958279999997103ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 Quickstart_Keras \u3002","title":"3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef"},{"location":"zh/quick_start/quickstart_mxnet/","text":"\u5feb\u901f\u5f00\u59cb (Mxnet) \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Mxnet \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 Mxnet \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 Mxnet \u5e93: pip install mxnet == 1 .9.1 Ifleaner Server \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3 Ifleaner Client \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_mxnet.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c: 1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784 \u00b6 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn def model (): net = nn . Sequential () net . add ( nn . Dense ( 256 , activation = \"relu\" )) net . add ( nn . Dense ( 64 , activation = \"relu\" )) net . add ( nn . Dense ( 10 )) net . collect_params () . initialize () return net 2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b \u00b6 \u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.mxnet_trainer.MxnetTrainer \u7c7b\u3002MxnetTrainer iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: from typing import Any , Tuple import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn from iflearner.business.homo.argument import parser from iflearner.business.homo.mxnet_trainer import MxnetTrainer from iflearner.business.homo.train_client import Controller class Mnist ( MxnetTrainer ): def __init__ ( self ): self . _model = model () init = nd . random . uniform ( shape = ( 2 , 784 )) self . _model ( init ) super () . __init__ ( model = self . _model ) self . _train_data , self . _val_data = self . _load_data () self . _DEVICE = [ mx . gpu () if mx . test_utils . list_gpus () else mx . cpu ()] @staticmethod def _load_data () -> Tuple [ Any , Any ]: print ( \"Download Dataset\" ) mnist = mx . test_utils . get_mnist () batch_size = 100 train_data = mx . io . NDArrayIter ( mnist [ \"train_data\" ], mnist [ \"train_label\" ], batch_size , shuffle = True ) val_data = mx . io . NDArrayIter ( mnist [ \"test_data\" ], mnist [ \"test_label\" ], batch_size ) return train_data , val_data def fit ( self , epoch : int ): trainer = gluon . Trainer ( self . _model . collect_params (), \"sgd\" , { \"learning_rate\" : 0.01 } ) accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) softmax_cross_entropy_loss = gluon . loss . SoftmaxCrossEntropyLoss () self . _train_data . reset () num_examples = 0 for batch in self . _train_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] with ag . record (): for x , y in zip ( data , label ): z = self . _model ( x ) loss = softmax_cross_entropy_loss ( z , y ) loss . backward () outputs . append ( z . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) trainer . step ( batch . data [ 0 ] . shape [ 0 ]) trainings_metric = metrics . get_name_value () [ accuracy , loss ] = trainings_metric print ( f \"epoch : { epoch } : accuracy: { float ( accuracy [ 1 ]) } loss\uff1a { float ( loss [ 1 ]) } \" ) def evaluate ( self , epoch : int ) -> dict : accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) self . _val_data . reset () num_examples = 0 for batch in self . _val_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] for x in data : outputs . append ( self . _model ( x ) . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) metrics . update ( label , outputs ) [ accuracy , loss ] = metrics . get_name_value () print ( f \"Evaluation accuracy: { accuracy } loss: { loss } \" ) return { \"loss\" : float ( loss [ 1 ]), \"accuracy\" : float ( accuracy [ 1 ])} 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u00b6 \u6700\u540e\uff0c\u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a main \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef\u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c: from iflearner.business.homo.argument import parser if __name__ == \"__main__\" : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_mxnet.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_mxnet.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 Quickstart_Mxnet \u3002","title":"Quickstart Mxnet"},{"location":"zh/quick_start/quickstart_mxnet/#mxnet","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Mxnet \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 Mxnet \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 Mxnet \u5e93: pip install mxnet == 1 .9.1","title":"\u5feb\u901f\u5f00\u59cb (Mxnet)"},{"location":"zh/quick_start/quickstart_mxnet/#ifleaner-server","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3","title":"Ifleaner Server"},{"location":"zh/quick_start/quickstart_mxnet/#ifleaner-client","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_mxnet.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c:","title":"Ifleaner Client"},{"location":"zh/quick_start/quickstart_mxnet/#1","text":"\u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn def model (): net = nn . Sequential () net . add ( nn . Dense ( 256 , activation = \"relu\" )) net . add ( nn . Dense ( 64 , activation = \"relu\" )) net . add ( nn . Dense ( 10 )) net . collect_params () . initialize () return net","title":"1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784"},{"location":"zh/quick_start/quickstart_mxnet/#2-trainer","text":"\u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.mxnet_trainer.MxnetTrainer \u7c7b\u3002MxnetTrainer iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: from typing import Any , Tuple import mxnet as mx from mxnet import autograd as ag from mxnet import gluon , nd from mxnet.gluon import nn from iflearner.business.homo.argument import parser from iflearner.business.homo.mxnet_trainer import MxnetTrainer from iflearner.business.homo.train_client import Controller class Mnist ( MxnetTrainer ): def __init__ ( self ): self . _model = model () init = nd . random . uniform ( shape = ( 2 , 784 )) self . _model ( init ) super () . __init__ ( model = self . _model ) self . _train_data , self . _val_data = self . _load_data () self . _DEVICE = [ mx . gpu () if mx . test_utils . list_gpus () else mx . cpu ()] @staticmethod def _load_data () -> Tuple [ Any , Any ]: print ( \"Download Dataset\" ) mnist = mx . test_utils . get_mnist () batch_size = 100 train_data = mx . io . NDArrayIter ( mnist [ \"train_data\" ], mnist [ \"train_label\" ], batch_size , shuffle = True ) val_data = mx . io . NDArrayIter ( mnist [ \"test_data\" ], mnist [ \"test_label\" ], batch_size ) return train_data , val_data def fit ( self , epoch : int ): trainer = gluon . Trainer ( self . _model . collect_params (), \"sgd\" , { \"learning_rate\" : 0.01 } ) accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) softmax_cross_entropy_loss = gluon . loss . SoftmaxCrossEntropyLoss () self . _train_data . reset () num_examples = 0 for batch in self . _train_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] with ag . record (): for x , y in zip ( data , label ): z = self . _model ( x ) loss = softmax_cross_entropy_loss ( z , y ) loss . backward () outputs . append ( z . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) trainer . step ( batch . data [ 0 ] . shape [ 0 ]) trainings_metric = metrics . get_name_value () [ accuracy , loss ] = trainings_metric print ( f \"epoch : { epoch } : accuracy: { float ( accuracy [ 1 ]) } loss\uff1a { float ( loss [ 1 ]) } \" ) def evaluate ( self , epoch : int ) -> dict : accuracy_metric = mx . metric . Accuracy () loss_metric = mx . metric . CrossEntropy () metrics = mx . metric . CompositeEvalMetric () for child_metric in [ accuracy_metric , loss_metric ]: metrics . add ( child_metric ) self . _val_data . reset () num_examples = 0 for batch in self . _val_data : data = gluon . utils . split_and_load ( batch . data [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) label = gluon . utils . split_and_load ( batch . label [ 0 ], ctx_list = self . _DEVICE , batch_axis = 0 ) outputs = [] for x in data : outputs . append ( self . _model ( x ) . softmax ()) num_examples += len ( x ) metrics . update ( label , outputs ) metrics . update ( label , outputs ) [ accuracy , loss ] = metrics . get_name_value () print ( f \"Evaluation accuracy: { accuracy } loss: { loss } \" ) return { \"loss\" : float ( loss [ 1 ]), \"accuracy\" : float ( accuracy [ 1 ])}","title":"2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b"},{"location":"zh/quick_start/quickstart_mxnet/#3-iflearner","text":"\u6700\u540e\uff0c\u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a main \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef\u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c: from iflearner.business.homo.argument import parser if __name__ == \"__main__\" : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_mxnet.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_mxnet.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) Download Dataset 2022-08-03 18:20:44.788 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:20:44.827 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 38.734709000000755ms 2022-08-03 18:20:44.830 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:20:44.832 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:20:44.836 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.204193999999937ms 2022-08-03 18:22:39.393 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.843368999999484ms 2022-08-03 18:22:40.203 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :1: accuracy:0.4393666666666667 loss\uff1a2.1208519152323406 2022-08-03 18:22:45.960 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.7123762376237623) loss:('cross-entropy', 1.6562039970171334) 2022-08-03 18:22:46.386 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:22:46.469 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.984263000002443ms 2022-08-03 18:22:47.532 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 79.32561299999463ms 2022-08-03 18:22:48.486 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:22:48.491 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 3.8189600000180235ms 2022-08-03 18:22:48.538 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 4.523907999981702ms 2022-08-03 18:22:49.495 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- epoch :2: accuracy:0.7846166666666666 loss\uff1a1.017146420733134 2022-08-03 18:22:54.082 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Evaluation accuracy:('accuracy', 0.8396039603960396) loss:('cross-entropy', 0.633656327464793) 2022-08-03 18:22:54.298 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [1.6562039970171334, 0.633656327464793]) label: LT, points: ([1], [1.6562039970171334]) label: FT, points: ([1, 2], [0.7123762376237623, 0.8396039603960396]) label: LT, points: ([1], [0.7123762376237623]) 2022-08-03 18:22:55.326 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.322818999994979ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 Quickstart_Mxnet \u3002","title":"3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef"},{"location":"zh/quick_start/quickstart_opacus/","text":"\u5feb\u901f\u5f00\u59cb (Opacus) \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u5c06\u5e2e\u52a9\u60a8\u4e86\u89e3\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u52a0\u5bc6\u6280\u672f\uff0c \u4f7f\u7528Opacus\u5e93\uff0c\u5728 pytorch \u4e0b\u8fd0\u884c\u8054\u90a6\u4efb\u52a1 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 PyTorch \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5Opacus\u3001 PyTorch \u548c torchvision \u5e93: pip install opacus == 1 .1.3 torch == 1 .8.1 torchvision == 0 .9.1 Ifleaner Server \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3 Ifleaner Client \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_pytorch.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c: 1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784 \u00b6 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) 2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b \u00b6 \u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u7c7b\u3002PyTorchTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u540c\u65f6\uff0c\u6211\u4eec\u96c6\u6210\u4e5f\u96c6\u6210\u4e86Opacus\u5dee\u5206\u9690\u79c1\u5e93\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 , delta = 1e-5 ) -> None : self . _lr = lr self . _delta = delta self . _device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) print ( f \"device: { self . _device } \" ) model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( model ) optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))] ) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) self . _privacy_engine = PrivacyEngine () self . _model , self . _optimizer , self . _train_data = self . _privacy_engine . make_private ( module = model , optimizer = optimizer , data_loader = train_data , noise_multiplier = 1.1 , max_grad_norm = 1.0 , ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) losses = [] for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () losses . append ( loss . item ()) epsilon , best_alpha = self . _privacy_engine . accountant . get_privacy_spent ( delta = self . _delta ) print ( f \"Train Epoch: { epoch } \\t \" f \"Loss: { np . mean ( losses ) : .6f } \" f \"(\u03b5 = { epsilon : .2f } , \u03b4 = { self . _delta } ) for \u03b1 = { best_alpha } \" ) def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = \"sum\" ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( \"Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)\" . format ( test_loss , correct , len ( self . _test_data . dataset ), 100.0 * correct / len ( self . _test_data . dataset ), ) ) return { \"loss\" : test_loss , \"acc\" : correct / len ( self . _test_data . dataset )} ``` #### 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u6700\u540e \uff0c \u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a ` main ` \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef \u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on. warnings.warn( 2022-08-08 20:54:51.294 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 20:54:51.308 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 13.392697000000009ms 2022-08-08 20:54:51.308 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 20:54:51.309 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 20:54:51.311 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5273650000002803ms 2022-08-08 20:54:53.322 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.9921759999999011ms 2022-08-08 20:54:54.325 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" Train Epoch: 1 Loss: 1.963445 (\u03b5 = 0.53, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:55:49.100 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6190, Accuracy: 7907/10000 (79.07%) 2022-08-08 20:55:51.779 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 20:55:51.785 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.787130000003856ms 2022-08-08 20:55:52.656 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.798496000001933ms 2022-08-08 20:55:52.789 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 20:55:52.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5915189999944346ms 2022-08-08 20:55:53.659 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.2224549999947385ms 2022-08-08 20:55:53.799 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 Train Epoch: 2 Loss: 1.975834 (\u03b5 = 0.55, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:56:41.185 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6178, Accuracy: 8213/10000 (82.13%) 2022-08-08 20:56:44.589 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.6190427913188934, 0.617782280254364]) label: LT, points: ([1], [0.6190427913188934]) label: FT, points: ([1, 2], [0.7907, 0.8213]) label: LT, points: ([1], [0.7907]) 2022-08-08 20:56:45.487 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.8401949999997669ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 Quickstart_Pytorch \u3002","title":"Quickstart Opacus"},{"location":"zh/quick_start/quickstart_opacus/#opacus","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u5c06\u5e2e\u52a9\u60a8\u4e86\u89e3\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u52a0\u5bc6\u6280\u672f\uff0c \u4f7f\u7528Opacus\u5e93\uff0c\u5728 pytorch \u4e0b\u8fd0\u884c\u8054\u90a6\u4efb\u52a1 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 PyTorch \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5Opacus\u3001 PyTorch \u548c torchvision \u5e93: pip install opacus == 1 .1.3 torch == 1 .8.1 torchvision == 0 .9.1","title":"\u5feb\u901f\u5f00\u59cb (Opacus)"},{"location":"zh/quick_start/quickstart_opacus/#ifleaner-server","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3","title":"Ifleaner Server"},{"location":"zh/quick_start/quickstart_opacus/#ifleaner-client","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_pytorch.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c:","title":"Ifleaner Client"},{"location":"zh/quick_start/quickstart_opacus/#1","text":"\u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 )","title":"1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784"},{"location":"zh/quick_start/quickstart_opacus/#2-trainer","text":"\u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u7c7b\u3002PyTorchTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u540c\u65f6\uff0c\u6211\u4eec\u96c6\u6210\u4e5f\u96c6\u6210\u4e86Opacus\u5dee\u5206\u9690\u79c1\u5e93\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 , delta = 1e-5 ) -> None : self . _lr = lr self . _delta = delta self . _device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) print ( f \"device: { self . _device } \" ) model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( model ) optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))] ) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) self . _privacy_engine = PrivacyEngine () self . _model , self . _optimizer , self . _train_data = self . _privacy_engine . make_private ( module = model , optimizer = optimizer , data_loader = train_data , noise_multiplier = 1.1 , max_grad_norm = 1.0 , ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) losses = [] for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () losses . append ( loss . item ()) epsilon , best_alpha = self . _privacy_engine . accountant . get_privacy_spent ( delta = self . _delta ) print ( f \"Train Epoch: { epoch } \\t \" f \"Loss: { np . mean ( losses ) : .6f } \" f \"(\u03b5 = { epsilon : .2f } , \u03b4 = { self . _delta } ) for \u03b1 = { best_alpha } \" ) def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = \"sum\" ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( \"Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)\" . format ( test_loss , correct , len ( self . _test_data . dataset ), 100.0 * correct / len ( self . _test_data . dataset ), ) ) return { \"loss\" : test_loss , \"acc\" : correct / len ( self . _test_data . dataset )} ``` #### 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u6700\u540e \uff0c \u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a ` main ` \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef \u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on. warnings.warn( 2022-08-08 20:54:51.294 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 20:54:51.308 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 13.392697000000009ms 2022-08-08 20:54:51.308 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 20:54:51.309 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 20:54:51.311 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5273650000002803ms 2022-08-08 20:54:53.322 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.9921759999999011ms 2022-08-08 20:54:54.325 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 /Users/lucky/opt/anaconda3/envs/iflearner/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \" Train Epoch: 1 Loss: 1.963445 (\u03b5 = 0.53, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:55:49.100 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6190, Accuracy: 7907/10000 (79.07%) 2022-08-08 20:55:51.779 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 20:55:51.785 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.787130000003856ms 2022-08-08 20:55:52.656 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.798496000001933ms 2022-08-08 20:55:52.789 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 20:55:52.794 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5915189999944346ms 2022-08-08 20:55:53.659 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.2224549999947385ms 2022-08-08 20:55:53.799 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 Train Epoch: 2 Loss: 1.975834 (\u03b5 = 0.55, \u03b4 = 1e-05) for \u03b1 = 16.0 2022-08-08 20:56:41.185 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.6178, Accuracy: 8213/10000 (82.13%) 2022-08-08 20:56:44.589 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.6190427913188934, 0.617782280254364]) label: LT, points: ([1], [0.6190427913188934]) label: FT, points: ([1, 2], [0.7907, 0.8213]) label: LT, points: ([1], [0.7907]) 2022-08-08 20:56:45.487 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.8401949999997669ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 Quickstart_Pytorch \u3002","title":"2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b"},{"location":"zh/quick_start/quickstart_pytorch/","text":"\u5feb\u901f\u5f00\u59cb (PyTorch) \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 PyTorch \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 PyTorch \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 PyTorch \u548c torchvision \u5e93: pip install torch == 1 .7.1 torchvision == 0 .8.2 Ifleaner Server \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3 Ifleaner Client \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_pytorch.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c: 1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784 \u00b6 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) 2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b \u00b6 \u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u7c7b\u3002PyTorchTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u6700\u540e \uff0c \u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a ` main ` \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef \u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu 2022-08-03 17:33:49.148 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 17:33:49.165 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 16.620276000000047ms 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 17:33:49.170 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.700055999999895ms 2022-08-03 17:33:54.192 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.3457160000003299ms 2022-08-03 17:33:55.188 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-03 17:34:43.583 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1023, Accuracy: 9696/10000 (96.96%) 2022-08-03 17:34:48.140 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 17:34:48.354 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 210.4355039999959ms 2022-08-03 17:34:48.426 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 15.4244869999971ms 2022-08-03 17:34:49.359 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 17:34:49.362 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.6616899999988277ms 2022-08-03 17:34:50.437 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.947831000002509ms 2022-08-03 17:34:51.367 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-03 17:35:38.518 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0833, Accuracy: 9758/10000 (97.58%) 2022-08-03 17:35:43.808 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.10231972066191956, 0.08325759547855704]) label: LT, points: ([1], [0.10231972066191956]) label: FT, points: ([1, 2], [0.9696, 0.9758]) label: LT, points: ([1], [0.9696]) 2022-08-03 17:35:44.596 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.4475409999903377ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 Quickstart_Pytorch \u3002","title":"Quickstart Pytorch"},{"location":"zh/quick_start/quickstart_pytorch/#pytorch","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 PyTorch \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 PyTorch \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 PyTorch \u548c torchvision \u5e93: pip install torch == 1 .7.1 torchvision == 0 .8.2","title":"\u5feb\u901f\u5f00\u59cb (PyTorch)"},{"location":"zh/quick_start/quickstart_pytorch/#ifleaner-server","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3","title":"Ifleaner Server"},{"location":"zh/quick_start/quickstart_pytorch/#ifleaner-client","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_pytorch.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c:","title":"Ifleaner Client"},{"location":"zh/quick_start/quickstart_pytorch/#1","text":"\u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 )","title":"1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784"},{"location":"zh/quick_start/quickstart_pytorch/#2-trainer","text":"\u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u7c7b\u3002PyTorchTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u6700\u540e \uff0c \u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a ` main ` \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef \u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) device: cpu 2022-08-03 17:33:49.148 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 17:33:49.165 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 16.620276000000047ms 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 17:33:49.166 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 17:33:49.170 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.700055999999895ms 2022-08-03 17:33:54.192 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.3457160000003299ms 2022-08-03 17:33:55.188 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-03 17:34:43.583 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1023, Accuracy: 9696/10000 (96.96%) 2022-08-03 17:34:48.140 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 17:34:48.354 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 210.4355039999959ms 2022-08-03 17:34:48.426 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 15.4244869999971ms 2022-08-03 17:34:49.359 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 17:34:49.362 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.6616899999988277ms 2022-08-03 17:34:50.437 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.947831000002509ms 2022-08-03 17:34:51.367 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-03 17:35:38.518 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0833, Accuracy: 9758/10000 (97.58%) 2022-08-03 17:35:43.808 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [0.10231972066191956, 0.08325759547855704]) label: LT, points: ([1], [0.10231972066191956]) label: FT, points: ([1, 2], [0.9696, 0.9758]) label: LT, points: ([1], [0.9696]) 2022-08-03 17:35:44.596 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.4475409999903377ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 Quickstart_Pytorch \u3002","title":"2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b"},{"location":"zh/quick_start/quickstart_smpc/","text":"\u5feb\u901f\u5f00\u59cb (SMPC) \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u7ed3\u5408 SMPC \u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 PyTorch \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 PyTorch \u548c torchvision \u5e93: pip install torch == 1 .7.1 torchvision == 0 .8.2 Ifleaner Server \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3 Ifleaner Client \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_pytorch.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c: 1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784 \u00b6 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) 2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b \u00b6 \u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u7c7b\u3002PyTorchTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u6700\u540e \uff0c \u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a ` main ` \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef \u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client01 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50012;0.0.0.0:50013\" \u914d\u7f6epeers\u5373\u4f7f\u7528smpc, peers\u914d\u7f6e\u4e3a\u6240\u6709\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740, \u7b2c\u4e00\u4e2a\u5730\u5740\u4e3a\u8be5\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client02 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50013;0.0.0.0:50012\" \u914d\u7f6epeers\u5373\u4f7f\u7528smpc, peers\u914d\u7f6e\u4e3a\u6240\u6709\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740, \u7b2c\u4e00\u4e2a\u5730\u5740\u4e3a\u8be5\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client1', epochs=10, server='0.0.0.0:50001', enable_ll=0, peers='0.0.0.0:50012;0.0.0.0:50013', cert=None) device: cpu 2022-08-08 19:39:37.971 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 19:39:37.976 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 4.347216000000209ms 2022-08-08 19:39:37.977 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 19:39:37.980 | INFO | iflearner.communication.peer.peer_client:get_DH_public_key:43 - Public key: b'\\x01\\x01\\xd0\\xf2\\xa5\\xc0-\\x7f\\x1b\\x88\\xcb\\xc8\\\\\\x91Ra,4\\n\\xd4]\\x97\\x99zs\\xae7\\x1cK]]\\x0c\\x06\\x85\\xa1\\xb5\\x82\\x03.\\x9a\\xe0m\\xa3>#\\xf7(\\xb3x\\x89m\\xfa\\xfbu\\x9ca\\x95\\xf4\\x80GA\\xd8z\\x8fKs\\xe0\\x98\\xe3\\x7fX.\\xe2Ej\\x04c\\x08\\xcf\\xdeF\\'\\xcc(\"@q[\\xa5\\xdf\\xb4#\\x1c\\xd6\\xd8\\xd1\\x05?\\x06tO\\xfa~Z\\x12\\x14\\x1e\\xba\\xbe\\xaa\\xe5/}\\xb1Y\\xde]\\xd8\\\\\\x17\\x9cE\\xf3Z\\xae(\\xbfDsf' 2022-08-08 19:39:37.983 | INFO | iflearner.business.homo.train_client:do_smpc:73 - secret: 122099796455175621216112096188958830464477667871351715488133066776583905683428683953037290811910449486061004359077608812182806437003480898524406977052511968565063977753455848242565116696939311359584774021461025748485006418803112297959930199282888061745382056940467943791248215701671956530776589875636959329985, type: <class 'str'> 2022-08-08 19:39:37.988 | INFO | iflearner.communication.peer.peer_client:get_SMPC_random_key:56 - Random float: 0.6339090663897411 2022-08-08 19:39:37.988 | INFO | iflearner.business.homo.train_client:do_smpc:77 - random value: 0.6339090663897411 2022-08-08 19:39:48.296 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_dh_public_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_smpc_random_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:56 - Party: client2, Random float: 0.3922334767649399 2022-08-08 19:39:49.023 | INFO | iflearner.business.homo.train_client:do_smpc:80 - sum all random values: 0.24167558962480118 2022-08-08 19:39:49.025 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 19:39:49.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.0064270000013096ms 2022-08-08 19:39:50.030 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.7326500000010014ms 2022-08-08 19:39:51.033 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-08 19:40:20.664 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1058, Accuracy: 9694/10000 (96.94%) 2022-08-08 19:40:23.830 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:40:23.858 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.50900600000449ms 2022-08-08 19:40:24.168 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 10.504099000002043ms 2022-08-08 19:40:24.862 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:40:24.865 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2220939999991742ms 2022-08-08 19:40:25.173 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.2624359999975354ms 2022-08-08 19:40:25.871 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:00.230 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0998, Accuracy: 9726/10000 (97.26%) 2022-08-08 19:41:03.992 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:04.020 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.794709000000353ms 2022-08-08 19:41:04.367 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.483094000003689ms 2022-08-08 19:41:05.024 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:05.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.9964000000101123ms 2022-08-08 19:41:05.374 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6533630000026278ms 2022-08-08 19:41:06.032 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 3, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:33.934 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0871, Accuracy: 9743/10000 (97.43%) 2022-08-08 19:41:37.425 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:37.492 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 18.108728000001406ms 2022-08-08 19:41:37.514 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.546628999994255ms 2022-08-08 19:41:38.496 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:38.498 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2796400000070207ms 2022-08-08 19:41:38.519 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 0.9813249999979234ms 2022-08-08 19:41:39.503 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 4, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:12.085 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0989, Accuracy: 9705/10000 (97.05%) 2022-08-08 19:42:15.499 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:15.513 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.581355999978996ms 2022-08-08 19:42:15.694 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.077353999993875ms 2022-08-08 19:42:16.517 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:16.519 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3156910000020616ms 2022-08-08 19:42:16.701 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.0472559999973328ms 2022-08-08 19:42:17.524 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 5, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:53.300 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0639, Accuracy: 9806/10000 (98.06%) 2022-08-08 19:42:56.654 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:56.667 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.813790000028348ms 2022-08-08 19:42:56.892 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.262933999982124ms 2022-08-08 19:42:57.672 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:57.675 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5553380000028483ms 2022-08-08 19:42:57.898 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6520599999978458ms 2022-08-08 19:42:58.679 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 6, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:26.257 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0753, Accuracy: 9787/10000 (97.87%) 2022-08-08 19:43:30.128 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:43:30.143 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.0744939999985945ms 2022-08-08 19:43:31.048 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.83907899998826ms 2022-08-08 19:43:31.148 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:43:31.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.054303000022628ms 2022-08-08 19:43:32.055 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.705947000004926ms 2022-08-08 19:43:32.153 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 7, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:58.396 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0736, Accuracy: 9768/10000 (97.68%) 2022-08-08 19:44:01.113 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:01.125 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.890878999972756ms 2022-08-08 19:44:02.184 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.515048000025672ms 2022-08-08 19:44:03.132 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:03.135 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.91251899997269ms 2022-08-08 19:44:03.188 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6274970000154099ms 2022-08-08 19:44:04.140 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 8, the size of training dataset: 60000, batch size: 938 2022-08-08 19:44:34.161 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0646, Accuracy: 9801/10000 (98.01%) 2022-08-08 19:44:37.132 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:37.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.143503999993754ms 2022-08-08 19:44:37.328 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.2212239999771555ms 2022-08-08 19:44:38.153 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:38.156 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.8954930000063541ms 2022-08-08 19:44:38.335 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.6670950000493576ms 2022-08-08 19:44:39.161 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 9, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:04.166 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0694, Accuracy: 9781/10000 (97.81%) 2022-08-08 19:45:06.821 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:45:06.841 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.79627700000674ms 2022-08-08 19:45:07.453 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.497581000005084ms 2022-08-08 19:45:07.846 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:45:07.848 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.1865850000276623ms 2022-08-08 19:45:09.465 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6427019999696313ms 2022-08-08 19:45:09.858 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 10, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:48.696 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0615, Accuracy: 9814/10000 (98.14%) 2022-08-08 19:45:53.514 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.10584523560330272, 0.09984834497272968, 0.0871084996862337, 0.09891319364532829, 0.063905619766374, 0.07528823107918725, 0.07361029261836957, 0.06460160582875542, 0.0694242621988058, 0.06149101790403947]) label: LT, points: ([1], [0.10584523560330272]) label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.9694, 0.9726, 0.9743, 0.9705, 0.9806, 0.9787, 0.9768, 0.9801, 0.9781, 0.9814]) label: LT, points: ([1], [0.9694]) 2022-08-08 19:45:54.253 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 2.060518000007505ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 Quickstart_SMPC \u3002","title":"Quickstart SMPC"},{"location":"zh/quick_start/quickstart_smpc/#smpc","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u7ed3\u5408 SMPC \u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 PyTorch \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 PyTorch \u548c torchvision \u5e93: pip install torch == 1 .7.1 torchvision == 0 .8.2","title":"\u5feb\u901f\u5f00\u59cb (SMPC)"},{"location":"zh/quick_start/quickstart_smpc/#ifleaner-server","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3","title":"Ifleaner Server"},{"location":"zh/quick_start/quickstart_smpc/#ifleaner-client","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_pytorch.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c:","title":"Ifleaner Client"},{"location":"zh/quick_start/quickstart_smpc/#1","text":"\u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from torch import nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , num_channels , num_classes ): super () . __init__ () self . conv1 = nn . Conv2d ( num_channels , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , num_classes ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , x . shape [ 1 ] * x . shape [ 2 ] * x . shape [ 3 ]) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 )","title":"1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784"},{"location":"zh/quick_start/quickstart_smpc/#2-trainer","text":"\u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.pytorch_trainer.PyTorchTrainer \u7c7b\u3002PyTorchTrainer\u4ece iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: import torch import torch.optim as optim import torch.nn.functional as F from torchvision import datasets , transforms from iflearner.business.homo.train_client import Controller from iflearner.business.homo.pytorch_trainer import PyTorchTrainer class Mnist ( PyTorchTrainer ): def __init__ ( self , lr = 0.15 , momentum = 0.5 ) -> None : self . _lr = lr self . _device = torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) print ( f 'device: { self . _device } ' ) self . _model = Model ( num_channels = 1 , num_classes = 10 ) . to ( self . _device ) super () . __init__ ( self . _model ) self . _optimizer = optim . SGD ( self . _model . parameters (), lr = lr , momentum = momentum ) self . _loss = F . nll_loss apply_transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,))]) train_dataset = datasets . MNIST ( \"./data\" , train = True , download = True , transform = apply_transform ) test_dataset = datasets . MNIST ( \"./data\" , train = False , download = True , transform = apply_transform ) self . _train_data = torch . utils . data . DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) self . _test_data = torch . utils . data . DataLoader ( test_dataset , batch_size = 64 , shuffle = False ) def fit ( self , epoch ): self . _model . to ( self . _device ) self . _model . train () print ( f \"Epoch: { epoch } , the size of training dataset: { len ( self . _train_data . dataset ) } , batch size: { len ( self . _train_data ) } \" ) for batch_idx , ( data , target ) in enumerate ( self . _train_data ): data , target = data . to ( self . _device ), target . to ( self . _device ) self . _optimizer . zero_grad () output = self . _model ( data ) loss = self . _loss ( output , target ) loss . backward () self . _optimizer . step () def evaluate ( self , epoch ): self . _model . to ( self . _device ) self . _model . eval () test_loss = 0 correct = 0 print ( f \"The size of testing dataset: { len ( self . _test_data . dataset ) } \" ) with torch . no_grad (): for data , target in self . _test_data : data , target = data . to ( self . _device ), target . to ( self . _device ) output = self . _model ( data ) test_loss += self . _loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( self . _test_data . dataset ) print ( 'Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.2f} %)' . format ( test_loss , correct , len ( self . _test_data . dataset ), 1. * correct / len ( self . _test_data . dataset ))) return { 'loss' : test_loss , 'acc' : correct } ``` #### 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u6700\u540e \uff0c \u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a ` main ` \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef \u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c : ``` python from iflearner.business.homo.argument import parser if __name__ == '__main__' : args = parser . parse_args () print ( args ) mnist = Mnist () controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client01 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50012;0.0.0.0:50013\" \u914d\u7f6epeers\u5373\u4f7f\u7528smpc, peers\u914d\u7f6e\u4e3a\u6240\u6709\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740, \u7b2c\u4e00\u4e2a\u5730\u5740\u4e3a\u8be5\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_pytorch.py --name client02 --epochs 2 --server \"0.0.0.0:50001\" --peers \"0.0.0.0:50013;0.0.0.0:50012\" \u914d\u7f6epeers\u5373\u4f7f\u7528smpc, peers\u914d\u7f6e\u4e3a\u6240\u6709\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740, \u7b2c\u4e00\u4e2a\u5730\u5740\u4e3a\u8be5\u5ba2\u6237\u7aef\u7684\u76d1\u542c\u5730\u5740 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: Namespace(name='client1', epochs=10, server='0.0.0.0:50001', enable_ll=0, peers='0.0.0.0:50012;0.0.0.0:50013', cert=None) device: cpu 2022-08-08 19:39:37.971 | INFO | iflearner.business.homo.train_client:run:89 - register to server 2022-08-08 19:39:37.976 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 4.347216000000209ms 2022-08-08 19:39:37.977 | INFO | iflearner.business.homo.train_client:run:106 - use strategy: FedAvg 2022-08-08 19:39:37.980 | INFO | iflearner.communication.peer.peer_client:get_DH_public_key:43 - Public key: b'\\x01\\x01\\xd0\\xf2\\xa5\\xc0-\\x7f\\x1b\\x88\\xcb\\xc8\\\\\\x91Ra,4\\n\\xd4]\\x97\\x99zs\\xae7\\x1cK]]\\x0c\\x06\\x85\\xa1\\xb5\\x82\\x03.\\x9a\\xe0m\\xa3>#\\xf7(\\xb3x\\x89m\\xfa\\xfbu\\x9ca\\x95\\xf4\\x80GA\\xd8z\\x8fKs\\xe0\\x98\\xe3\\x7fX.\\xe2Ej\\x04c\\x08\\xcf\\xdeF\\'\\xcc(\"@q[\\xa5\\xdf\\xb4#\\x1c\\xd6\\xd8\\xd1\\x05?\\x06tO\\xfa~Z\\x12\\x14\\x1e\\xba\\xbe\\xaa\\xe5/}\\xb1Y\\xde]\\xd8\\\\\\x17\\x9cE\\xf3Z\\xae(\\xbfDsf' 2022-08-08 19:39:37.983 | INFO | iflearner.business.homo.train_client:do_smpc:73 - secret: 122099796455175621216112096188958830464477667871351715488133066776583905683428683953037290811910449486061004359077608812182806437003480898524406977052511968565063977753455848242565116696939311359584774021461025748485006418803112297959930199282888061745382056940467943791248215701671956530776589875636959329985, type: <class 'str'> 2022-08-08 19:39:37.988 | INFO | iflearner.communication.peer.peer_client:get_SMPC_random_key:56 - Random float: 0.6339090663897411 2022-08-08 19:39:37.988 | INFO | iflearner.business.homo.train_client:do_smpc:77 - random value: 0.6339090663897411 2022-08-08 19:39:48.296 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_dh_public_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:46 - IN: party: client2, message type: msg_smpc_random_key 2022-08-08 19:39:48.298 | INFO | iflearner.communication.peer.peer_server:send:56 - Party: client2, Random float: 0.3922334767649399 2022-08-08 19:39:49.023 | INFO | iflearner.business.homo.train_client:do_smpc:80 - sum all random values: 0.24167558962480118 2022-08-08 19:39:49.025 | INFO | iflearner.business.homo.train_client:run:139 - report client ready 2022-08-08 19:39:49.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.0064270000013096ms 2022-08-08 19:39:50.030 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.7326500000010014ms 2022-08-08 19:39:51.033 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 1, the size of training dataset: 60000, batch size: 938 2022-08-08 19:40:20.664 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.1058, Accuracy: 9694/10000 (96.94%) 2022-08-08 19:40:23.830 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:40:23.858 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.50900600000449ms 2022-08-08 19:40:24.168 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 10.504099000002043ms 2022-08-08 19:40:24.862 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:40:24.865 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2220939999991742ms 2022-08-08 19:40:25.173 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.2624359999975354ms 2022-08-08 19:40:25.871 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 2, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:00.230 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0998, Accuracy: 9726/10000 (97.26%) 2022-08-08 19:41:03.992 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:04.020 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 17.794709000000353ms 2022-08-08 19:41:04.367 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.483094000003689ms 2022-08-08 19:41:05.024 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:05.027 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.9964000000101123ms 2022-08-08 19:41:05.374 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6533630000026278ms 2022-08-08 19:41:06.032 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 3, the size of training dataset: 60000, batch size: 938 2022-08-08 19:41:33.934 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0871, Accuracy: 9743/10000 (97.43%) 2022-08-08 19:41:37.425 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:41:37.492 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 18.108728000001406ms 2022-08-08 19:41:37.514 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.546628999994255ms 2022-08-08 19:41:38.496 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:41:38.498 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.2796400000070207ms 2022-08-08 19:41:38.519 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 0.9813249999979234ms 2022-08-08 19:41:39.503 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 4, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:12.085 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0989, Accuracy: 9705/10000 (97.05%) 2022-08-08 19:42:15.499 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:15.513 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.581355999978996ms 2022-08-08 19:42:15.694 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 8.077353999993875ms 2022-08-08 19:42:16.517 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:16.519 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3156910000020616ms 2022-08-08 19:42:16.701 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.0472559999973328ms 2022-08-08 19:42:17.524 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 5, the size of training dataset: 60000, batch size: 938 2022-08-08 19:42:53.300 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0639, Accuracy: 9806/10000 (98.06%) 2022-08-08 19:42:56.654 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:42:56.667 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.813790000028348ms 2022-08-08 19:42:56.892 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.262933999982124ms 2022-08-08 19:42:57.672 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:42:57.675 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.5553380000028483ms 2022-08-08 19:42:57.898 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6520599999978458ms 2022-08-08 19:42:58.679 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 6, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:26.257 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0753, Accuracy: 9787/10000 (97.87%) 2022-08-08 19:43:30.128 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:43:30.143 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 2.0744939999985945ms 2022-08-08 19:43:31.048 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.83907899998826ms 2022-08-08 19:43:31.148 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:43:31.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 2.054303000022628ms 2022-08-08 19:43:32.055 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.705947000004926ms 2022-08-08 19:43:32.153 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 7, the size of training dataset: 60000, batch size: 938 2022-08-08 19:43:58.396 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0736, Accuracy: 9768/10000 (97.68%) 2022-08-08 19:44:01.113 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:01.125 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 1.890878999972756ms 2022-08-08 19:44:02.184 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.515048000025672ms 2022-08-08 19:44:03.132 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:03.135 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.91251899997269ms 2022-08-08 19:44:03.188 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6274970000154099ms 2022-08-08 19:44:04.140 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 8, the size of training dataset: 60000, batch size: 938 2022-08-08 19:44:34.161 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0646, Accuracy: 9801/10000 (98.01%) 2022-08-08 19:44:37.132 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:44:37.151 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.143503999993754ms 2022-08-08 19:44:37.328 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 7.2212239999771555ms 2022-08-08 19:44:38.153 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:44:38.156 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.8954930000063541ms 2022-08-08 19:44:38.335 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 2.6670950000493576ms 2022-08-08 19:44:39.161 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 9, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:04.166 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0694, Accuracy: 9781/10000 (97.81%) 2022-08-08 19:45:06.821 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- 2022-08-08 19:45:06.841 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 12.79627700000674ms 2022-08-08 19:45:07.453 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 9.497581000005084ms 2022-08-08 19:45:07.846 | INFO | iflearner.business.homo.train_client:run:221 - ----- set ----- 2022-08-08 19:45:07.848 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.1865850000276623ms 2022-08-08 19:45:09.465 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.6427019999696313ms 2022-08-08 19:45:09.858 | INFO | iflearner.business.homo.train_client:run:149 - ----- fit <FT> ----- Epoch: 10, the size of training dataset: 60000, batch size: 938 2022-08-08 19:45:48.696 | INFO | iflearner.business.homo.train_client:run:167 - ----- evaluate <FT> ----- The size of testing dataset: 10000 Test set: Average loss: 0.0615, Accuracy: 9814/10000 (98.14%) 2022-08-08 19:45:53.514 | INFO | iflearner.business.homo.train_client:run:178 - ----- get <FT> ----- label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.10584523560330272, 0.09984834497272968, 0.0871084996862337, 0.09891319364532829, 0.063905619766374, 0.07528823107918725, 0.07361029261836957, 0.06460160582875542, 0.0694242621988058, 0.06149101790403947]) label: LT, points: ([1], [0.10584523560330272]) label: FT, points: ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.9694, 0.9726, 0.9743, 0.9705, 0.9806, 0.9787, 0.9768, 0.9801, 0.9781, 0.9814]) label: LT, points: ([1], [0.9694]) 2022-08-08 19:45:54.253 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 2.060518000007505ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 Quickstart_SMPC \u3002","title":"2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b"},{"location":"zh/quick_start/quickstart_tensorflow/","text":"\u5feb\u901f\u5f00\u59cb (TensorFlow) \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Tensorflow \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 Tensorflow \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 Tensorflow \u5e93: pip install tensorflow == 2 .9.1 Ifleaner Server \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3 Ifleaner Client \u00b6 \u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_tensorflow.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c: 1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784 \u00b6 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from tensorflow.keras.layers import Dense , Flatten , Conv2D from tensorflow.keras import Model class MyModel ( Model ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = Conv2D ( 32 , 3 , activation = 'relu' ) self . flatten = Flatten () self . d1 = Dense ( 128 , activation = 'relu' ) self . d2 = Dense ( 10 ) def call ( self , x ): x = self . conv1 ( x ) x = self . flatten ( x ) x = self . d1 ( x ) return self . d2 ( x ) 2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b \u00b6 \u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer \u7c7b\u3002TensorFlowTrainer iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: import tensorflow as tf from iflearner.business.homo.tensorflow_trainer import TensorFlowTrainer from iflearner.business.homo.train_client import Controller mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Add a channels dimension x_train = x_train [ ... , tf . newaxis ] . astype ( \"float32\" ) x_test = x_test [ ... , tf . newaxis ] . astype ( \"float32\" ) train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train )) . shuffle ( 10000 ) . batch ( 32 ) test_ds = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) . batch ( 32 ) loss_object = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) optimizer = tf . keras . optimizers . Adam () train_loss = tf . keras . metrics . Mean ( name = 'train_loss' ) train_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'train_accuracy' ) test_loss = tf . keras . metrics . Mean ( name = 'test_loss' ) test_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'test_accuracy' ) class Mnist ( TensorFlowTrainer ): def __init__ ( self , model ) -> None : super () . __init__ ( model ) train_loss . reset_states () train_accuracy . reset_states () test_loss . reset_states () test_accuracy . reset_states () def fit ( self , epoch ): for images , labels in train_ds : # train_step(images, labels) self . _fit ( images , labels ) @tf . function def _fit ( self , images , labels ): with tf . GradientTape () as tape : # training=True is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = True ) loss = loss_object ( labels , predictions ) gradients = tape . gradient ( loss , model . trainable_variables ) optimizer . apply_gradients ( zip ( gradients , model . trainable_variables )) train_loss ( loss ) train_accuracy ( labels , predictions ) def evaluate ( self , epoch ): for test_images , test_labels in test_ds : # test_step(test_images, test_labels) self . _evaluate ( test_images , test_labels ) print ( f 'Epoch { epoch } , ' f 'Loss: { train_loss . result () } , ' f 'Accuracy: { train_accuracy . result () * 100 } , ' f 'Test Loss: { test_loss . result () } , ' f 'Test Accuracy: { test_accuracy . result () * 100 } ' ) return { 'Accuracy' : train_accuracy . result () * 100 } @tf . function def _evaluate ( self , images , labels ): # training=False is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = False ) t_loss = loss_object ( labels , predictions ) test_loss ( t_loss ) 3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef \u00b6 \u6700\u540e\uff0c\u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a main \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef\u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c: if __name__ == '__main__' : args = parser . parse_args () print ( args ) model = MyModel () mnist = Mnist ( model ) controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_tensorflow.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_tensorflow.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: 2022-08-03 18:07:07.406604: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:07:07.456 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:07:07.479 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 22.46353199999973ms 2022-08-03 18:07:07.479 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:07:07.480 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:07:07.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3502309999999795ms 2022-08-03 18:07:11.500 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.1013739999992112ms 2022-08-03 18:07:12.497 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:08:19.804 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 1, Loss: 0.1414850801229477, Accuracy: 95.73500061035156, Test Loss: 0.0603780597448349, Test Accuracy: 0.0 2022-08-03 18:08:22.759 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:08:24.130 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 149.0828160000035ms 2022-08-03 18:08:31.445 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 3241.557815999997ms 2022-08-03 18:08:32.446 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:08:32.474 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.617487000004303ms 2022-08-03 18:08:33.469 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 8.709660999997482ms 2022-08-03 18:08:33.479 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:09:46.374 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 2, Loss: 0.11132627725601196, Accuracy: 96.65583038330078, Test Loss: 0.0679144412279129, Test Accuracy: 0.0 2022-08-03 18:09:49.340 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>, <tf.Tensor: shape=(), dtype=float32, numpy=96.65583>]) label: LT, points: ([1], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>]) 2022-08-03 18:09:51.374 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.561914999996361ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 Quickstart_Tensorflow \u3002","title":"Quickstart Tensorflow"},{"location":"zh/quick_start/quickstart_tensorflow/#tensorflow","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Tensorflow \u6846\u67b6\u4e0b\u4f7f\u7528 IFLeaner \u5728 MNIST \u6570\u636e\u96c6\u4e0b\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u8054\u90a6\u8bad\u7ec3\u3002 \u6211\u4eec\u8fd9\u4e2a\u793a\u4f8b\u9ed8\u8ba4\u662f\u5305\u542b\u4e86\u4e24\u4e2a\u5ba2\u6237\u7aef\u548c\u4e00\u4e2a\u670d\u52a1\u7aef\u3002\u6bcf\u4e00\u8f6e\u8bad\u7ec3\uff0c\u5ba2\u6237\u7aef \u8d1f\u8d23\u8bad\u7ec3\u5e76\u4e0a\u4f20\u6a21\u578b \u53c2\u6570\u5230\u670d\u52a1\u7aef\uff0c\u670d\u52a1\u7aef\u8fdb\u884c\u805a\u5408\uff0c \u5e76\u4e0b\u53d1\u805a\u5408\u540e\u7684\u5168\u5c40\u6a21\u578b\u53c2\u6570\u7ed9\u6bcf\u4e2a\u5ba2\u6237\u7aef\uff0c\u7136\u540e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u66f4\u65b0\u805a\u5408\u540e\u7684 \u6a21\u578b\u53c2\u6570\uff0c\u8fd9\u5c06\u91cd\u590d\u591a\u8f6e\u3002 \u9996\u5148\uff0c\u6211\u4eec\u6781\u5176\u63a8\u8350\u5148\u521b\u5efa\u4e00\u4e2apython\u865a\u62df\u73af\u5883\u6765\u8fd0\u884c\uff0c\u53ef\u4ee5\u901a\u8fc7virtualenv, pyenv, conda\u7b49\u7b49\u865a\u62df\u5de5\u5177\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u5feb\u901f\u5b89\u88c5IFLearner\u5e93: pip install iflearner \u53e6\u5916\uff0c\u56e0\u4e3a\u6211\u4eec\u60f3\u4f7f\u7528 Tensorflow \u6765\u5b8c\u6210\u5728 MNIST \u6570\u636e\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u7eed\u5b89\u88c5 Tensorflow \u5e93: pip install tensorflow == 2 .9.1","title":"\u5feb\u901f\u5f00\u59cb (TensorFlow)"},{"location":"zh/quick_start/quickstart_tensorflow/#ifleaner-server","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb server.py \u7684\u65b0\u6587\u4ef6, \u5f15\u5165iflearner\u5e93: from iflearner.business.homo.aggregate_server import main if __name__ == \"__main__\" : main () \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u542f\u52a8Server\u8fdb\u7a0b: python server.py -n 2 -n 2: \u63a5\u6536\u4e24\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u8054\u90a6\u8bad\u7ec3","title":"Ifleaner Server"},{"location":"zh/quick_start/quickstart_tensorflow/#ifleaner-client","text":"\u521b\u5efa\u4e00\u4e2a\u540d\u53eb quickstart_tensorflow.py \u7684\u6587\u4ef6\uff0c\u7136\u540e\u6309\u7167\u4e0b\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c:","title":"Ifleaner Client"},{"location":"zh/quick_start/quickstart_tensorflow/#1","text":"\u9996\u5148\uff0c\u60a8\u9700\u8981\u5728keras\u4e0a\u5b9a\u4e49\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u6a21\u578b\u3002 from tensorflow.keras.layers import Dense , Flatten , Conv2D from tensorflow.keras import Model class MyModel ( Model ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = Conv2D ( 32 , 3 , activation = 'relu' ) self . flatten = Flatten () self . d1 = Dense ( 128 , activation = 'relu' ) self . d2 = Dense ( 10 ) def call ( self , x ): x = self . conv1 ( x ) x = self . flatten ( x ) x = self . d1 ( x ) return self . d2 ( x )","title":"1. \u5b9a\u4e49\u6a21\u578b\u7f51\u7edc\u7ed3\u6784"},{"location":"zh/quick_start/quickstart_tensorflow/#2-trainer","text":"\u5176\u6b21\uff0c\u60a8\u9700\u8981\u5b9e\u73b0\u60a8\u7684trainer\u7c7b\uff0c\u7ee7\u627f iflearner.business.homo.trainer.Trainer \u3002\u8be5\u7c7b\u9700\u8981\u5b9e\u73b0\u56db\u4e2a\u51fd\u6570\uff0c \u5b83\u4eec\u662f get \u3001 set \u3001 fit \u548c evaluate \u51fd\u6570\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86 iflearner.business.homo.tensorflow_trainer.TensorFlowTrainer \u7c7b\u3002TensorFlowTrainer iflearner.business.homo.trainer.Trainer \u7ee7\u627f\u800c\u6765\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e38\u89c1\u7684 get \u548c set \u51fd\u6570\u3002 \u60a8\u53ef\u4ee5\u7ee7\u627f\u5b9e\u73b0\u8be5\u7c7b\u5982\u4e0b\u6240\u793a: import tensorflow as tf from iflearner.business.homo.tensorflow_trainer import TensorFlowTrainer from iflearner.business.homo.train_client import Controller mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Add a channels dimension x_train = x_train [ ... , tf . newaxis ] . astype ( \"float32\" ) x_test = x_test [ ... , tf . newaxis ] . astype ( \"float32\" ) train_ds = tf . data . Dataset . from_tensor_slices ( ( x_train , y_train )) . shuffle ( 10000 ) . batch ( 32 ) test_ds = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) . batch ( 32 ) loss_object = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) optimizer = tf . keras . optimizers . Adam () train_loss = tf . keras . metrics . Mean ( name = 'train_loss' ) train_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'train_accuracy' ) test_loss = tf . keras . metrics . Mean ( name = 'test_loss' ) test_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'test_accuracy' ) class Mnist ( TensorFlowTrainer ): def __init__ ( self , model ) -> None : super () . __init__ ( model ) train_loss . reset_states () train_accuracy . reset_states () test_loss . reset_states () test_accuracy . reset_states () def fit ( self , epoch ): for images , labels in train_ds : # train_step(images, labels) self . _fit ( images , labels ) @tf . function def _fit ( self , images , labels ): with tf . GradientTape () as tape : # training=True is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = True ) loss = loss_object ( labels , predictions ) gradients = tape . gradient ( loss , model . trainable_variables ) optimizer . apply_gradients ( zip ( gradients , model . trainable_variables )) train_loss ( loss ) train_accuracy ( labels , predictions ) def evaluate ( self , epoch ): for test_images , test_labels in test_ds : # test_step(test_images, test_labels) self . _evaluate ( test_images , test_labels ) print ( f 'Epoch { epoch } , ' f 'Loss: { train_loss . result () } , ' f 'Accuracy: { train_accuracy . result () * 100 } , ' f 'Test Loss: { test_loss . result () } , ' f 'Test Accuracy: { test_accuracy . result () * 100 } ' ) return { 'Accuracy' : train_accuracy . result () * 100 } @tf . function def _evaluate ( self , images , labels ): # training=False is only needed if there are layers with different # behavior during training versus inference (e.g. Dropout). predictions = model ( images , training = False ) t_loss = loss_object ( labels , predictions ) test_loss ( t_loss )","title":"2. \u7ee7\u627f\u5b9e\u73b0Trainer\u7c7b"},{"location":"zh/quick_start/quickstart_tensorflow/#3-iflearner","text":"\u6700\u540e\uff0c\u60a8\u9700\u8981\u7f16\u5199\u4e00\u4e2a main \u51fd\u6570\u6765\u542f\u52a8\u5ba2\u6237\u7aef\u3002 \u60a8\u53ef\u4ee5\u6309\u4ee5\u4e0b\u65b9\u5f0f\u6267\u884c: if __name__ == '__main__' : args = parser . parse_args () print ( args ) model = MyModel () mnist = Mnist ( model ) controller = Controller ( args , mnist ) controller . run () \u5728 main \u51fd\u6570\u4e2d\uff0c\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8ff0\u547d\u4ee4\u6765\u542f\u52a8\u60a8\u7684\u7b2c\u4e00\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_tensorflow.py --name client01 --epochs 2 \u6253\u5f00\u53e6\u4e00\u4e2a\u7ec8\u7aef\uff0c\u5e76\u4e14\u542f\u52a8\u7b2c\u4e8c\u4e2a\u5ba2\u6237\u7aef\u8fdb\u7a0b: python quickstart_tensorflow.py --name client02 --epochs 2 \u4e24\u4e2a\u5ba2\u6237\u7aef\u90fd\u5c31\u7eea\u5e76\u542f\u52a8\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4efb\u610f\u4e00\u4e2a\u5ba2\u6237\u7aef\u7ec8\u7aef\u4e0a\u770b\u5230\u7c7b\u4f3c\u4e0b\u8ff0\u7684\u65e5\u5fd7\u4fe1\u606f: 2022-08-03 18:07:07.406604: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Namespace(name='client01', epochs=2, server='localhost:50001', enable_ll=0, peers=None, cert=None) 2022-08-03 18:07:07.456 | INFO | iflearner.business.homo.train_client:run:90 - register to server 2022-08-03 18:07:07.479 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_register, time: 22.46353199999973ms 2022-08-03 18:07:07.479 | INFO | iflearner.business.homo.train_client:run:107 - use strategy: FedAvg 2022-08-03 18:07:07.480 | INFO | iflearner.business.homo.train_client:run:140 - report client ready 2022-08-03 18:07:07.482 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.3502309999999795ms 2022-08-03 18:07:11.500 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 1.1013739999992112ms 2022-08-03 18:07:12.497 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:08:19.804 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 1, Loss: 0.1414850801229477, Accuracy: 95.73500061035156, Test Loss: 0.0603780597448349, Test Accuracy: 0.0 2022-08-03 18:08:22.759 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- 2022-08-03 18:08:24.130 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_upload_param, time: 149.0828160000035ms 2022-08-03 18:08:31.445 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_aggregate_result, time: 3241.557815999997ms 2022-08-03 18:08:32.446 | INFO | iflearner.business.homo.train_client:run:222 - ----- set ----- 2022-08-03 18:08:32.474 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_client_ready, time: 1.617487000004303ms 2022-08-03 18:08:33.469 | INFO | iflearner.communication.homo.homo_client:notice:94 - IN: party: message type: msg_notify_training, time: 8.709660999997482ms 2022-08-03 18:08:33.479 | INFO | iflearner.business.homo.train_client:run:150 - ----- fit <FT> ----- 2022-08-03 18:09:46.374 | INFO | iflearner.business.homo.train_client:run:168 - ----- evaluate <FT> ----- Epoch 2, Loss: 0.11132627725601196, Accuracy: 96.65583038330078, Test Loss: 0.0679144412279129, Test Accuracy: 0.0 2022-08-03 18:09:49.340 | INFO | iflearner.business.homo.train_client:run:179 - ----- get <FT> ----- label: FT, points: ([1, 2], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>, <tf.Tensor: shape=(), dtype=float32, numpy=96.65583>]) label: LT, points: ([1], [<tf.Tensor: shape=(), dtype=float32, numpy=95.735>]) 2022-08-03 18:09:51.374 | INFO | iflearner.communication.homo.homo_client:transport:59 - OUT: message type: msg_complete, time: 1.561914999996361ms \u606d\u559c\u60a8\uff01\u60a8\u5df2\u6210\u529f\u6784\u5efa\u5e76\u8fd0\u884c\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002\u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 Quickstart_Tensorflow \u3002","title":"3. \u542f\u52a8Iflearner\u7684\u5ba2\u6237\u7aef"},{"location":"zh/tutorial/argument/","text":"\u542f\u52a8\u53c2\u6570 \u00b6 \u5ba2\u6237\u7aef \u00b6 \u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u4e0b\u9762\u662f\u5ba2\u6237\u7aef\u9ed8\u8ba4\u7684\u53c2\u6570: option type describe default name str \u5ba2\u6237\u7aef\u540d\u79f0(\u5fc5\u987b\u552f\u4e00) client epochs int \u603b\u7684\u8bad\u7ec3\u8f6e\u6570 10 server str \u94fe\u63a5\u805a\u5408\u670d\u52a1\u7aef\u7684\u5730\u5740 localhost:50001 enable-ll int \u542f\u52a8\u672c\u5730\u8bad\u7ec3\u8fdb\u884c\u5bf9\u6bd4 (1 \u3001 0), 1\u4ee3\u8868\u5f00\u542f 0 peers str \u5982\u679c\u53c2\u6570\u5df2\u6307\u5b9a\uff0c\u5219\u542f\u7528 SMPC\u3002 \u6240\u6709\u5ba2\u6237\u7aef\u7684\u5730\u5740\u5e76\u4f7f\u7528\u5206\u53f7\u5206\u9694\u6240\u6709\u5730\u5740\u3002 \u7b2c\u4e00\u4e2a\u662f\u4f60\u81ea\u5df1\u7684\u5730\u5740\u3002 cert str \u670d\u52a1\u5668 SSL \u8bc1\u4e66\u7684\u8def\u5f84\u3002 \u5982\u679c\u6307\u5b9a\uff0c\u5219\u4f7f\u7528\u5b89\u5168\u901a\u9053\u8fde\u63a5\u5230\u670d\u52a1\u5668 \u670d\u52a1\u7aef \u00b6 \u670d\u52a1\u7aef\u53c2\u6570\u5217\u8868\u5982\u4e0b: \u9009\u9879 \u7c7b\u578b \u63cf\u8ff0 \u9ed8\u8ba4\u503c num int \u5ba2\u6237\u7aef\u6570\u76ee 0 epochs int \u603b\u7684\u805a\u5408\u8f6e\u6570 addr str \u805a\u5408\u670d\u52a1\u7aef\u672c\u8eab\u76d1\u542c\u5730\u5740(\u7528\u4e8e\u5ba2\u6237\u7aef\u94fe\u63a5) \"0.0.0.0:50001\" http_addr str \u8054\u90a6\u8bad\u7ec3\u72b6\u6001\u76d1\u542c\u5730\u5740(\u7528\u4e8e\u67e5\u770b\u8054\u90a6\u8bad\u7ec3\u72b6\u6001) \"0.0.0.0:50002\" strategy str \u805a\u5408\u7b56\u7565 (FedAvg\u3001Scaffold\u3001FedOpt\u3001qFedAvg\u3001FedNova) FedAvg strategy_params dict \u805a\u5408\u7b56\u7565\u53c2\u6570 {}","title":"Startup Option"},{"location":"zh/tutorial/argument/#_1","text":"","title":"\u542f\u52a8\u53c2\u6570"},{"location":"zh/tutorial/argument/#_2","text":"\u60a8\u9700\u8981\u4ece iflearner.business.homo \u5bfc\u5165 parser \uff0c \u7136\u540e\u8c03\u7528 parser.parse_args \uff0c \u56e0\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9700\u8981\u89e3\u6790\u7684\u5e38\u89c1\u53c2\u6570\u3002 \u5982\u679c\u60a8\u81ea\u5df1\u6dfb\u52a0\u5176\u4ed6\u53c2\u6570\uff0c\u53ef\u4ee5\u8c03\u7528 parser.add_argument \u5c06\u5176\u6dfb\u52a0\u5230 parser.parse_args \u4e4b\u524d\u3002\u5728\u89e3\u6790\u53c2\u6570\u540e\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u4e4b\u524d\u5b9e\u73b0\u7684\u7c7b\u521b\u5efatrainer\u5b9e\u4f8b\uff0c\u5e76\u5c06\u5176\u4e0e args \u4f20\u9012\u5230 train_client.Controller \u51fd\u6570\u4e2d\u3002\u6700\u540e\uff0c\u4f60\u53ea\u9700\u8981\u8c03\u7528 controller.run \u6765\u542f\u52a8\u4f60\u7684\u5ba2\u6237\u7aef\u8fdb\u7a0b\u3002 \u4e0b\u9762\u662f\u5ba2\u6237\u7aef\u9ed8\u8ba4\u7684\u53c2\u6570: option type describe default name str \u5ba2\u6237\u7aef\u540d\u79f0(\u5fc5\u987b\u552f\u4e00) client epochs int \u603b\u7684\u8bad\u7ec3\u8f6e\u6570 10 server str \u94fe\u63a5\u805a\u5408\u670d\u52a1\u7aef\u7684\u5730\u5740 localhost:50001 enable-ll int \u542f\u52a8\u672c\u5730\u8bad\u7ec3\u8fdb\u884c\u5bf9\u6bd4 (1 \u3001 0), 1\u4ee3\u8868\u5f00\u542f 0 peers str \u5982\u679c\u53c2\u6570\u5df2\u6307\u5b9a\uff0c\u5219\u542f\u7528 SMPC\u3002 \u6240\u6709\u5ba2\u6237\u7aef\u7684\u5730\u5740\u5e76\u4f7f\u7528\u5206\u53f7\u5206\u9694\u6240\u6709\u5730\u5740\u3002 \u7b2c\u4e00\u4e2a\u662f\u4f60\u81ea\u5df1\u7684\u5730\u5740\u3002 cert str \u670d\u52a1\u5668 SSL \u8bc1\u4e66\u7684\u8def\u5f84\u3002 \u5982\u679c\u6307\u5b9a\uff0c\u5219\u4f7f\u7528\u5b89\u5168\u901a\u9053\u8fde\u63a5\u5230\u670d\u52a1\u5668","title":"\u5ba2\u6237\u7aef"},{"location":"zh/tutorial/argument/#_3","text":"\u670d\u52a1\u7aef\u53c2\u6570\u5217\u8868\u5982\u4e0b: \u9009\u9879 \u7c7b\u578b \u63cf\u8ff0 \u9ed8\u8ba4\u503c num int \u5ba2\u6237\u7aef\u6570\u76ee 0 epochs int \u603b\u7684\u805a\u5408\u8f6e\u6570 addr str \u805a\u5408\u670d\u52a1\u7aef\u672c\u8eab\u76d1\u542c\u5730\u5740(\u7528\u4e8e\u5ba2\u6237\u7aef\u94fe\u63a5) \"0.0.0.0:50001\" http_addr str \u8054\u90a6\u8bad\u7ec3\u72b6\u6001\u76d1\u542c\u5730\u5740(\u7528\u4e8e\u67e5\u770b\u8054\u90a6\u8bad\u7ec3\u72b6\u6001) \"0.0.0.0:50002\" strategy str \u805a\u5408\u7b56\u7565 (FedAvg\u3001Scaffold\u3001FedOpt\u3001qFedAvg\u3001FedNova) FedAvg strategy_params dict \u805a\u5408\u7b56\u7565\u53c2\u6570 {}","title":"\u670d\u52a1\u7aef"},{"location":"zh/tutorial/contributor_guide/","text":"\u8d21\u732e\u8005\u6307\u5357 \u00b6 \u5148\u51b3\u6761\u4ef6 \u00b6 Python 3.7 \u6216 \u4ee5\u4e0a \u5f00\u53d1\u73af\u5883\u8bbe\u7f6e \u00b6 \u9996\u5148\uff0c\u4ece GitHub \u514b\u9686 IFLearner \u5e93\uff1a $ git clone https://github.com/iflytek/iflearner.git $ cd iflearner \u7136\u540e\uff0c\u60a8\u9700\u8981\u4f7f\u7528 conda\u3001pyenv \u7b49\u865a\u62df\u5de5\u5177\u521b\u5efa\u4e00\u4e2a python \u865a\u62df\u73af\u5883\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528 conda \u521b\u5efa\u865a\u62df\u73af\u5883\u7684\u793a\u4f8b\uff1a $ conda create -n iflearner python == 3 .9 $ conda activate iflearner \u6700\u540e\uff0c\u9700\u8981\u5b89\u88c5iflearner\u6240\u9700\u7684\u4f9d\u8d56\uff1a $ pip install -r requirements.txt \u5f00\u53d1\u811a\u672c \u00b6 \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5f00\u53d1\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u5728 ./dev \u76ee\u5f55\u4e2d\u627e\u5230\u5b83\u4eec\u3002 \u4ee3\u7801\u81ea\u52a8\u683c\u5f0f\u5316\u548c\u81ea\u52a8\u6d4b\u8bd5 \u00b6 \u9996\u5148\uff0c\u6267\u884c\u811a\u672c\u81ea\u52a8\u683c\u5f0f\u5316\uff1a $ ./dev/format.sh \u5176\u6b21\uff0c\u6267\u884c\u6d4b\u8bd5\u811a\u672c\u3002\u7136\u540e\uff0c\u60a8\u5e94\u8be5\u9075\u5faa\u4ee3\u7801\u89c4\u8303\uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u4ee3\u7801\u8c03\u6574\u3002 $ ./dev/test.sh \u6784\u5efa\u6587\u6863 \u00b6 IFLearner \u4f7f\u7528 mkdocs \u6784\u5efa\u6587\u6863\uff0c\u60a8\u53ef\u4ee5\u8fdb\u5165 ./doc \u76ee\u5f55\u5e76\u6309\u7167readme\u6559\u7a0b\u6784\u5efa\u6587\u6863\u3002 Whl\u6253\u5305 \u00b6 IFLearner \u4f7f\u7528 setup \u6765\u8fdb\u884c\u6253\u5305\uff1a python setup.py bdist_wheel iflearner- .whl \u548c iflearner- .tar.gz \u5305\u5c06\u5b58\u50a8\u5728 ./dist \u5b50\u76ee\u5f55\u4e2d\u3002 \u53d1\u5e03Pypi \u00b6 \u5982\u679c\u4f60\u6709\u6743\u9650\u53d1\u5e03\u7248\u672c\u5305\uff0c\u4f60\u53ef\u4ee5\u6309\u4e0b\u8ff0\u64cd\u4f5c\u8fdb\u884c\u53d1\u5e03. \u9996\u5148, \u914d\u7f6epypirc\u6587\u4ef6. # Linux ## vim ~/.pypirc # Windows ## C:\\Users\\Username\\.pypirc : <<'COMMENT' [distutils] index-servers=pypi [pypi] repository=https://upload.pypi.org/legacy/ username=<username> password=<password> COMMENT \u7136\u540e, \u8fdb\u884c\u6253\u5305: python setup.py sdist \u6700\u540e, \u8fdb\u884c\u53d1\u5e03: twine pload dist/* -r pypi","title":"Contributor Guide"},{"location":"zh/tutorial/contributor_guide/#_1","text":"","title":"\u8d21\u732e\u8005\u6307\u5357"},{"location":"zh/tutorial/contributor_guide/#_2","text":"Python 3.7 \u6216 \u4ee5\u4e0a","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/tutorial/contributor_guide/#_3","text":"\u9996\u5148\uff0c\u4ece GitHub \u514b\u9686 IFLearner \u5e93\uff1a $ git clone https://github.com/iflytek/iflearner.git $ cd iflearner \u7136\u540e\uff0c\u60a8\u9700\u8981\u4f7f\u7528 conda\u3001pyenv \u7b49\u865a\u62df\u5de5\u5177\u521b\u5efa\u4e00\u4e2a python \u865a\u62df\u73af\u5883\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528 conda \u521b\u5efa\u865a\u62df\u73af\u5883\u7684\u793a\u4f8b\uff1a $ conda create -n iflearner python == 3 .9 $ conda activate iflearner \u6700\u540e\uff0c\u9700\u8981\u5b89\u88c5iflearner\u6240\u9700\u7684\u4f9d\u8d56\uff1a $ pip install -r requirements.txt","title":"\u5f00\u53d1\u73af\u5883\u8bbe\u7f6e"},{"location":"zh/tutorial/contributor_guide/#_4","text":"\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5f00\u53d1\u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u5728 ./dev \u76ee\u5f55\u4e2d\u627e\u5230\u5b83\u4eec\u3002","title":"\u5f00\u53d1\u811a\u672c"},{"location":"zh/tutorial/contributor_guide/#_5","text":"\u9996\u5148\uff0c\u6267\u884c\u811a\u672c\u81ea\u52a8\u683c\u5f0f\u5316\uff1a $ ./dev/format.sh \u5176\u6b21\uff0c\u6267\u884c\u6d4b\u8bd5\u811a\u672c\u3002\u7136\u540e\uff0c\u60a8\u5e94\u8be5\u9075\u5faa\u4ee3\u7801\u89c4\u8303\uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u4ee3\u7801\u8c03\u6574\u3002 $ ./dev/test.sh","title":"\u4ee3\u7801\u81ea\u52a8\u683c\u5f0f\u5316\u548c\u81ea\u52a8\u6d4b\u8bd5"},{"location":"zh/tutorial/contributor_guide/#_6","text":"IFLearner \u4f7f\u7528 mkdocs \u6784\u5efa\u6587\u6863\uff0c\u60a8\u53ef\u4ee5\u8fdb\u5165 ./doc \u76ee\u5f55\u5e76\u6309\u7167readme\u6559\u7a0b\u6784\u5efa\u6587\u6863\u3002","title":"\u6784\u5efa\u6587\u6863"},{"location":"zh/tutorial/contributor_guide/#whl","text":"IFLearner \u4f7f\u7528 setup \u6765\u8fdb\u884c\u6253\u5305\uff1a python setup.py bdist_wheel iflearner- .whl \u548c iflearner- .tar.gz \u5305\u5c06\u5b58\u50a8\u5728 ./dist \u5b50\u76ee\u5f55\u4e2d\u3002","title":"Whl\u6253\u5305"},{"location":"zh/tutorial/contributor_guide/#pypi","text":"\u5982\u679c\u4f60\u6709\u6743\u9650\u53d1\u5e03\u7248\u672c\u5305\uff0c\u4f60\u53ef\u4ee5\u6309\u4e0b\u8ff0\u64cd\u4f5c\u8fdb\u884c\u53d1\u5e03. \u9996\u5148, \u914d\u7f6epypirc\u6587\u4ef6. # Linux ## vim ~/.pypirc # Windows ## C:\\Users\\Username\\.pypirc : <<'COMMENT' [distutils] index-servers=pypi [pypi] repository=https://upload.pypi.org/legacy/ username=<username> password=<password> COMMENT \u7136\u540e, \u8fdb\u884c\u6253\u5305: python setup.py sdist \u6700\u540e, \u8fdb\u884c\u53d1\u5e03: twine pload dist/* -r pypi","title":"\u53d1\u5e03Pypi"},{"location":"zh/tutorial/custom_aggregation_strategy/","text":"\u6784\u5efa\u81ea\u5df1\u7684\u6a2a\u5411\u805a\u5408\u7b56\u7565 \u00b6 1.Build Your Server \u00b6 \u4e3a\u4e86\u6784\u5efa Federate Learning Server\uff0c\u60a8\u5e94\u8be5\u7ee7\u627f if learner.business.homo.strategy.strategy _server.Strategy Server \u5e76\u5b9e\u73b0\u6240\u6709\u62bd\u8c61\u65b9\u6cd5\u3002 handler_register : \u5904\u7406\u6765\u81ea\u5ba2\u6237\u7aef\u7684 MSG_REGISTER \u6d88\u606f\u3002 handler_client_ready : \u5904\u7406\u6765\u81ea\u5ba2\u6237\u7aef\u7684 MSG_CLIENT_READY \u6d88\u606f\u3002 handler_upload_param : \u5904\u7406\u6765\u81ea\u5ba2\u6237\u7aef\u7684 MSG_UPLOAD_PARAM \u6d88\u606f\u3002 \u6b64\u6d88\u606f\u7684\u5185\u5bb9\u7531\u5ba2\u6237\u7aef\u7684 Trainner.get()\u65b9\u6cd5\u51b3\u5b9a\u3002 get_client_notification : \u83b7\u53d6\u6307\u5b9a\u5ba2\u6237\u7aef\u7684\u901a\u77e5\u4fe1\u606f\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u3002 \u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u53c2\u8003fedavg\u5b9e\u73b0\uff0c\u6216\u8005\u8054\u7cfb\u6211\u4eec\u3002 class StrategyServer ( ABC ): \"\"\"Implement the strategy of server.\"\"\" def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers @abstractmethod def handler_register ( self , party_name : str , sample_num : Optional [ int ] = None , step_num : Optional [ int ] = None , ) -> None : \"\"\"Handle the message of MSG_REGISTER from the client.\"\"\" pass @abstractmethod def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" pass @abstractmethod def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client.\"\"\" pass @abstractmethod def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client.\"\"\" pass class MyStrategyServer ( StrategyServer ): '''''' strategy = MyStrategyServer ( ... ) server = AggregateServer ( args . addr , strategy , args . num , params = params ) server . run () 2. Build Your Client \u00b6 \u4e3a\u4e86\u6784\u5efa\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u7aef\uff0c\u60a8\u5e94\u8be5\u7ee7\u627f iflearner.business.homo.strategy.strategy client.Strategy Client \u5e76\u5b9e\u73b0\u6240\u6709\u62bd\u8c61\u65b9\u6cd5\u3002 class StrategyClient ( ABC ): \"\"\"Implement the strategy of client.\"\"\" class Stage ( IntEnum ): \"\"\"Enum the stage of client.\"\"\" Waiting = auto () Training = auto () Setting = auto () def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : self . _trainer_config = config @abstractmethod def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass @abstractmethod def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ]) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM.\"\"\" pass @abstractmethod def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training.\"\"\" pass @abstractmethod def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server.\"\"\" pass @abstractmethod def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" pass class MyStrategyClient ( StrategyClient ): '''''' mnist = Mnist () controller = Controller ( args , mnist , MyStrategyClient ()) controller . run () \u4f8b\u5b50 \u00b6 \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5c06\u9010\u6b65\u5b9e\u73b0 FedDyn \u3002FedDyn\u7b97\u6cd5\u6d41\u7a0b\u8bf7\u53c2\u8003\u539f\u59cb\u8bba\u6587\u3002 \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 FedDyn \u3002 \u5efa\u7acbFedDyn\u670d\u52a1\u7aef \u00b6 \u7ee7\u627f iflearner.business.homo.strategy.strategy_server\u3002StrategyServer \u7c7b\uff0c\u5e76\u6839\u636e\u7b97\u6cd5\u6d41\u7a0b\u8986\u76d6\u548c\u5b9e\u73b0\u4e00\u4e9b\u65b9\u6cd5\u3002\u5728FedDyn\u4e2d\uff0c\u6211\u4eec\u53ea\u9700\u8981\u8986\u76d6 handler_upload_param \u548c __init__ \u3002 from time import sleep from typing import Any , Dict , Optional , Tuple import numpy as np import torch from loguru import logger from torch import dtype , nn from torch.nn import functional as F from iflearner.business.homo.strategy import strategy_server from iflearner.communication.homo import homo_pb2 , message_type from iflearner.communication.homo.homo_exception import HomoException class FedDynServer ( strategy_server . StrategyServer ): \"\"\"Implement the strategy of feddyn on server side.\"\"\" \"\"\" num_clients: client numuber alpha: a static coefficient \"\"\" def __init__ ( self , num_clients : int , learning_rate = 0.1 , alpha = 0.1 , params : Dict [ str , np . ndarray ] = None , ) -> None : super () . __init__ () self . _num_clients = num_clients self . _lr = learning_rate self . _alpha = alpha self . _params = params logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _clients_samples : dict = {} self . _h = { name : np . zeros_like ( p ) . reshape ( - 1 ) for name , p in self . _params . items () } def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _uploaded_num == self . _num_clients : self . _uploaded_num = 0 aggregate_result = dict () grad = dict () logger . info ( f \"Faddyn params, param num: { len ( data . parameters ) } \" ) for param_name , param_info in data . parameters . items (): aggregate_result [ param_name ] = homo_pb2 . Parameter ( shape = param_info . shape ) params = [] for v in self . _training_clients . values (): params . append ( v [ \"param\" ][ param_name ] . values ) avg_param = [ sum ( x ) * ( 1 / self . _num_clients ) for x in zip ( * params )] grad [ param_name ] = np . array ( avg_param , dtype = \"float32\" ) - self . _params [ param_name ] . reshape (( - 1 )) self . _h [ param_name ] = ( self . _h [ param_name ] - self . _alpha * grad [ param_name ] ) self . _params [ param_name ] = ( np . array ( avg_param , dtype = \"float32\" ) - ( 1 / self . _alpha ) * self . _h [ param_name ] ) . reshape ( param_info . shape ) aggregate_result [ param_name ] . values . extend ( self . _params [ param_name ] . reshape ( - 1 ) . tolist () ) self . _server_param = aggregate_result # type: ignore self . _on_aggregating = True Build Client \u00b6 \u6839\u636e FedDyn \u7b97\u6cd5\u6d41\u7a0b\uff0c\u6211\u4eec\u9700\u8981\u8986\u76d6 Trainner.fit \u65b9\u6cd5 def fit ( self , epoch ): self . _old_weights = deepcopy ( self . _model . state_dict ()) batch_time = AverageMeter ( \"Time\" , \":6.3f\" ) data_time = AverageMeter ( \"Data\" , \":6.3f\" ) losses = AverageMeter ( \"Loss\" , \":.4e\" ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , data_time , losses , top1 , top5 ], prefix = \" {} Epoch: [ {} ]\" . format ( self . _args . name , epoch ), ) # switch to train mode self . _model . train () end = time . time () for _ in range ( 1 ): for i , ( images , target ) in enumerate ( self . _train_loader ): # measure data loading time data_time . update ( time . time () - end ) if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) linear_penalty = sum ( [ torch . sum (( p * self . _old_grad [ name ])) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) quad_penalty = sum ( [ F . mse_loss ( p , self . _old_weights [ name ], reduction = \"sum\" ) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) loss += quad_penalty * self . _alpha / 2 loss -= linear_penalty # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # compute gradient and do SGD step self . _optimizer . zero_grad () loss . backward () if self . _scaffold : g = yield self . get ( self . ParameterType . ParameterGradient ) self . set ( homo_pb2 . AggregateResult ( parameters = g . parameters ), self . ParameterType . ParameterGradient , ) self . _optimizer . step () # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) self . _old_grad = { name : p . grad for name , p in self . _model . named_parameters () }","title":"Custom Aggregation Strategy"},{"location":"zh/tutorial/custom_aggregation_strategy/#_1","text":"","title":"\u6784\u5efa\u81ea\u5df1\u7684\u6a2a\u5411\u805a\u5408\u7b56\u7565"},{"location":"zh/tutorial/custom_aggregation_strategy/#1build-your-server","text":"\u4e3a\u4e86\u6784\u5efa Federate Learning Server\uff0c\u60a8\u5e94\u8be5\u7ee7\u627f if learner.business.homo.strategy.strategy _server.Strategy Server \u5e76\u5b9e\u73b0\u6240\u6709\u62bd\u8c61\u65b9\u6cd5\u3002 handler_register : \u5904\u7406\u6765\u81ea\u5ba2\u6237\u7aef\u7684 MSG_REGISTER \u6d88\u606f\u3002 handler_client_ready : \u5904\u7406\u6765\u81ea\u5ba2\u6237\u7aef\u7684 MSG_CLIENT_READY \u6d88\u606f\u3002 handler_upload_param : \u5904\u7406\u6765\u81ea\u5ba2\u6237\u7aef\u7684 MSG_UPLOAD_PARAM \u6d88\u606f\u3002 \u6b64\u6d88\u606f\u7684\u5185\u5bb9\u7531\u5ba2\u6237\u7aef\u7684 Trainner.get()\u65b9\u6cd5\u51b3\u5b9a\u3002 get_client_notification : \u83b7\u53d6\u6307\u5b9a\u5ba2\u6237\u7aef\u7684\u901a\u77e5\u4fe1\u606f\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u3002 \u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u53c2\u8003fedavg\u5b9e\u73b0\uff0c\u6216\u8005\u8054\u7cfb\u6211\u4eec\u3002 class StrategyServer ( ABC ): \"\"\"Implement the strategy of server.\"\"\" def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers @abstractmethod def handler_register ( self , party_name : str , sample_num : Optional [ int ] = None , step_num : Optional [ int ] = None , ) -> None : \"\"\"Handle the message of MSG_REGISTER from the client.\"\"\" pass @abstractmethod def handler_client_ready ( self , party_name : str ) -> None : \"\"\"Handle the message of MSG_CLIENT_READY from the client.\"\"\" pass @abstractmethod def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : \"\"\"Handle the message of MSG_UPLOAD_PARAM from the client.\"\"\" pass @abstractmethod def get_client_notification ( self , party_name : str ) -> Tuple [ str , Any ]: \"\"\"Get the notification information of the specified client.\"\"\" pass class MyStrategyServer ( StrategyServer ): '''''' strategy = MyStrategyServer ( ... ) server = AggregateServer ( args . addr , strategy , args . num , params = params ) server . run ()","title":"1.Build Your Server"},{"location":"zh/tutorial/custom_aggregation_strategy/#2-build-your-client","text":"\u4e3a\u4e86\u6784\u5efa\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u7aef\uff0c\u60a8\u5e94\u8be5\u7ee7\u627f iflearner.business.homo.strategy.strategy client.Strategy Client \u5e76\u5b9e\u73b0\u6240\u6709\u62bd\u8c61\u65b9\u6cd5\u3002 class StrategyClient ( ABC ): \"\"\"Implement the strategy of client.\"\"\" class Stage ( IntEnum ): \"\"\"Enum the stage of client.\"\"\" Waiting = auto () Training = auto () Setting = auto () def __init__ ( self ) -> None : self . _custom_handlers : Dict [ str , Any ] = dict () self . _trainer_config : Dict [ str , Any ] = dict () @property def custom_handlers ( self ) -> Dict [ str , Any ]: return self . _custom_handlers def set_trainer_config ( self , config : Dict [ str , Any ]) -> None : self . _trainer_config = config @abstractmethod def generate_registration_info ( self ) -> None : \"\"\"Generate the message of MSG_REGISTER.\"\"\" pass @abstractmethod def generate_upload_param ( self , epoch : int , data : Dict [ Any , Any ]) -> Any : \"\"\"Generate the message of MSG_UPLOAD_PARAM.\"\"\" pass @abstractmethod def update_param ( self , data : homo_pb2 . AggregateResult ) -> homo_pb2 . AggregateResult : \"\"\"Update the parameter during training.\"\"\" pass @abstractmethod def handler_aggregate_result ( self , data : homo_pb2 . AggregateResult ) -> None : \"\"\"Handle the message of MSG_AGGREGATE_RESULT from the server.\"\"\" pass @abstractmethod def handler_notify_training ( self ) -> None : \"\"\"Handle the message of MSG_NOTIFY_TRAINING from the server.\"\"\" pass class MyStrategyClient ( StrategyClient ): '''''' mnist = Mnist () controller = Controller ( args , mnist , MyStrategyClient ()) controller . run ()","title":"2. Build Your Client"},{"location":"zh/tutorial/custom_aggregation_strategy/#_2","text":"\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5c06\u9010\u6b65\u5b9e\u73b0 FedDyn \u3002FedDyn\u7b97\u6cd5\u6d41\u7a0b\u8bf7\u53c2\u8003\u539f\u59cb\u8bba\u6587\u3002 \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 FedDyn \u3002","title":"\u4f8b\u5b50"},{"location":"zh/tutorial/custom_aggregation_strategy/#feddyn","text":"\u7ee7\u627f iflearner.business.homo.strategy.strategy_server\u3002StrategyServer \u7c7b\uff0c\u5e76\u6839\u636e\u7b97\u6cd5\u6d41\u7a0b\u8986\u76d6\u548c\u5b9e\u73b0\u4e00\u4e9b\u65b9\u6cd5\u3002\u5728FedDyn\u4e2d\uff0c\u6211\u4eec\u53ea\u9700\u8981\u8986\u76d6 handler_upload_param \u548c __init__ \u3002 from time import sleep from typing import Any , Dict , Optional , Tuple import numpy as np import torch from loguru import logger from torch import dtype , nn from torch.nn import functional as F from iflearner.business.homo.strategy import strategy_server from iflearner.communication.homo import homo_pb2 , message_type from iflearner.communication.homo.homo_exception import HomoException class FedDynServer ( strategy_server . StrategyServer ): \"\"\"Implement the strategy of feddyn on server side.\"\"\" \"\"\" num_clients: client numuber alpha: a static coefficient \"\"\" def __init__ ( self , num_clients : int , learning_rate = 0.1 , alpha = 0.1 , params : Dict [ str , np . ndarray ] = None , ) -> None : super () . __init__ () self . _num_clients = num_clients self . _lr = learning_rate self . _alpha = alpha self . _params = params logger . info ( f \"num_clients: { self . _num_clients } \" ) self . _training_clients : dict = {} self . _server_param = None self . _ready_num = 0 self . _uploaded_num = 0 self . _aggregated_num = 0 self . _on_aggregating = False self . _clients_samples : dict = {} self . _h = { name : np . zeros_like ( p ) . reshape ( - 1 ) for name , p in self . _params . items () } def handler_upload_param ( self , party_name : str , data : homo_pb2 . UploadParam ) -> None : logger . info ( f \"Client: { party_name } , epoch: { data . epoch } \" ) if party_name not in self . _training_clients : raise HomoException ( HomoException . HomoResponseCode . Forbidden , \"Client not notified.\" ) self . _training_clients [ party_name ][ \"param\" ] = data . parameters self . _uploaded_num += 1 if self . _uploaded_num == self . _num_clients : self . _uploaded_num = 0 aggregate_result = dict () grad = dict () logger . info ( f \"Faddyn params, param num: { len ( data . parameters ) } \" ) for param_name , param_info in data . parameters . items (): aggregate_result [ param_name ] = homo_pb2 . Parameter ( shape = param_info . shape ) params = [] for v in self . _training_clients . values (): params . append ( v [ \"param\" ][ param_name ] . values ) avg_param = [ sum ( x ) * ( 1 / self . _num_clients ) for x in zip ( * params )] grad [ param_name ] = np . array ( avg_param , dtype = \"float32\" ) - self . _params [ param_name ] . reshape (( - 1 )) self . _h [ param_name ] = ( self . _h [ param_name ] - self . _alpha * grad [ param_name ] ) self . _params [ param_name ] = ( np . array ( avg_param , dtype = \"float32\" ) - ( 1 / self . _alpha ) * self . _h [ param_name ] ) . reshape ( param_info . shape ) aggregate_result [ param_name ] . values . extend ( self . _params [ param_name ] . reshape ( - 1 ) . tolist () ) self . _server_param = aggregate_result # type: ignore self . _on_aggregating = True","title":"\u5efa\u7acbFedDyn\u670d\u52a1\u7aef"},{"location":"zh/tutorial/custom_aggregation_strategy/#build-client","text":"\u6839\u636e FedDyn \u7b97\u6cd5\u6d41\u7a0b\uff0c\u6211\u4eec\u9700\u8981\u8986\u76d6 Trainner.fit \u65b9\u6cd5 def fit ( self , epoch ): self . _old_weights = deepcopy ( self . _model . state_dict ()) batch_time = AverageMeter ( \"Time\" , \":6.3f\" ) data_time = AverageMeter ( \"Data\" , \":6.3f\" ) losses = AverageMeter ( \"Loss\" , \":.4e\" ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , data_time , losses , top1 , top5 ], prefix = \" {} Epoch: [ {} ]\" . format ( self . _args . name , epoch ), ) # switch to train mode self . _model . train () end = time . time () for _ in range ( 1 ): for i , ( images , target ) in enumerate ( self . _train_loader ): # measure data loading time data_time . update ( time . time () - end ) if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) linear_penalty = sum ( [ torch . sum (( p * self . _old_grad [ name ])) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) quad_penalty = sum ( [ F . mse_loss ( p , self . _old_weights [ name ], reduction = \"sum\" ) . cpu () . detach () . numpy () for name , p in self . _model . named_parameters () if p . requires_grad ] ) loss += quad_penalty * self . _alpha / 2 loss -= linear_penalty # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # compute gradient and do SGD step self . _optimizer . zero_grad () loss . backward () if self . _scaffold : g = yield self . get ( self . ParameterType . ParameterGradient ) self . set ( homo_pb2 . AggregateResult ( parameters = g . parameters ), self . ParameterType . ParameterGradient , ) self . _optimizer . step () # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) self . _old_grad = { name : p . grad for name , p in self . _model . named_parameters () }","title":"Build Client"},{"location":"zh/tutorial/metrics_visualization/","text":"\u6307\u6807\u53ef\u89c6\u5316 \u00b6 \u6211\u4eec\u5728iflearner\u4e2d\u96c6\u6210\u4e86 VisualDL \u6765\u53ef\u89c6\u5316\u8bad\u7ec3\u6307\u6807\uff0c\u5f53\u4f60\u5b8c\u6210\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u5728\u4f60\u7684\u8bad\u7ec3\u4ee3\u7801\u76ee\u5f55\u4e0b\u4f1a\u6709\u4e00\u4e2a\u540d\u4e3ametric\u7684\u76ee\u5f55\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u7684\u547d\u4ee4\u6765\u542f\u52a8 VisualDL \uff1a visualdl --logdir ./metric --host 127.0.0.1 --port 8082 \u7136\u540e\uff0c\u4f60\u53ef\u4ee5\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u94fe\u63a5 http://127.0.0.1:8082 \u3002","title":"Metrics Visualization"},{"location":"zh/tutorial/metrics_visualization/#_1","text":"\u6211\u4eec\u5728iflearner\u4e2d\u96c6\u6210\u4e86 VisualDL \u6765\u53ef\u89c6\u5316\u8bad\u7ec3\u6307\u6807\uff0c\u5f53\u4f60\u5b8c\u6210\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u5728\u4f60\u7684\u8bad\u7ec3\u4ee3\u7801\u76ee\u5f55\u4e0b\u4f1a\u6709\u4e00\u4e2a\u540d\u4e3ametric\u7684\u76ee\u5f55\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u7684\u547d\u4ee4\u6765\u542f\u52a8 VisualDL \uff1a visualdl --logdir ./metric --host 127.0.0.1 --port 8082 \u7136\u540e\uff0c\u4f60\u53ef\u4ee5\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u94fe\u63a5 http://127.0.0.1:8082 \u3002","title":"\u6307\u6807\u53ef\u89c6\u5316"},{"location":"zh/tutorial/run_in_container/","text":"\u5728\u5bb9\u5668\u4e2d\u8fd0\u884cIFLearner \u00b6 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5bb9\u5668\u5316\u6280\u672f\uff0c\u5c06IFLearner\u5728Docker\u548cJupyterlab\u4e2d\u8fdb\u884c\u8fd0\u884c\u3002 \u60a8\u53ef\u4ee5\u5728\u9879\u76ee\u7684 ./enviroment \u76ee\u5f55\u4e0b\u627e\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u4e00\u4e9b\u793a\u4f8b\u548c\u6559\u7a0b\u3002 Docker \u00b6 Docker \u5c5e\u4e8e Linux \u5bb9\u5668\u7684\u4e00\u79cd\u5c01\u88c5\uff0c\u63d0\u4f9b\u7b80\u5355\u6613\u7528\u7684\u5bb9\u5668\u4f7f\u7528\u63a5\u53e3\u3002\u5b83\u662f\u76ee\u524d\u6700\u6d41\u884c\u7684 Linux \u5bb9\u5668\u89e3\u51b3\u65b9\u6848\u3002 Docker \u5c06\u5e94\u7528\u7a0b\u5e8f\u4e0e\u8be5\u7a0b\u5e8f\u7684\u4f9d\u8d56\uff0c\u6253\u5305\u5728\u4e00\u4e2a\u6587\u4ef6\u91cc\u9762\u3002\u8fd0\u884c\u8fd9\u4e2a\u6587\u4ef6\uff0c\u5c31\u4f1a\u751f\u6210\u4e00\u4e2a\u865a\u62df\u5bb9\u5668\u3002\u7a0b\u5e8f\u5728\u8fd9\u4e2a\u865a\u62df\u5bb9\u5668\u91cc\u8fd0\u884c\uff0c\u5c31\u597d\u50cf\u5728\u771f\u5b9e\u7684\u7269\u7406\u673a\u4e0a\u8fd0\u884c\u4e00\u6837\u3002\u6709\u4e86 Docker\uff0c\u5c31\u4e0d\u7528\u62c5\u5fc3\u73af\u5883\u95ee\u9898\u3002 \u60a8\u53ef\u4ee5\u5728\u9879\u76ee\u7684 ./enviroment/docker \u76ee\u5f55\u4e0b\u627e\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u4e00\u4e9b\u793a\u4f8b\u548c\u6559\u7a0b\u3002 Jupyterlab \u00b6 JupyterLab\u662fJupyter\u4e3b\u6253\u7684\u6700\u65b0\u6570\u636e\u79d1\u5b66\u751f\u4ea7\u5de5\u5177\uff0c\u67d0\u79cd\u610f\u4e49\u4e0a\uff0c\u5b83\u7684\u51fa\u73b0\u662f\u4e3a\u4e86\u53d6\u4ee3Jupyter Notebook\u3002\u4e0d\u8fc7\u4e0d\u7528\u62c5\u5fc3Jupyter Notebook\u4f1a\u6d88\u5931\uff0cJupyterLab\u5305\u542b\u4e86Jupyter Notebook\u6240\u6709\u529f\u80fd\u3002 JupyterLab\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8eweb\u7684\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u5b83\u7f16\u5199notebook\u3001\u64cd\u4f5c\u7ec8\u7aef\u3001\u7f16\u8f91markdown\u6587\u672c\u3001\u6253\u5f00\u4ea4\u4e92\u6a21\u5f0f\u3001\u67e5\u770bcsv\u6587\u4ef6\u53ca\u56fe\u7247\u7b49\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u5728\u9879\u76ee\u7684 ./enviroment/jupyterlab \u76ee\u5f55\u4e0b\u627e\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u4e00\u4e9b\u793a\u4f8b\u548c\u6559\u7a0b\u3002","title":"Run In Container"},{"location":"zh/tutorial/run_in_container/#iflearner","text":"\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5bb9\u5668\u5316\u6280\u672f\uff0c\u5c06IFLearner\u5728Docker\u548cJupyterlab\u4e2d\u8fdb\u884c\u8fd0\u884c\u3002 \u60a8\u53ef\u4ee5\u5728\u9879\u76ee\u7684 ./enviroment \u76ee\u5f55\u4e0b\u627e\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u4e00\u4e9b\u793a\u4f8b\u548c\u6559\u7a0b\u3002","title":"\u5728\u5bb9\u5668\u4e2d\u8fd0\u884cIFLearner"},{"location":"zh/tutorial/run_in_container/#docker","text":"Docker \u5c5e\u4e8e Linux \u5bb9\u5668\u7684\u4e00\u79cd\u5c01\u88c5\uff0c\u63d0\u4f9b\u7b80\u5355\u6613\u7528\u7684\u5bb9\u5668\u4f7f\u7528\u63a5\u53e3\u3002\u5b83\u662f\u76ee\u524d\u6700\u6d41\u884c\u7684 Linux \u5bb9\u5668\u89e3\u51b3\u65b9\u6848\u3002 Docker \u5c06\u5e94\u7528\u7a0b\u5e8f\u4e0e\u8be5\u7a0b\u5e8f\u7684\u4f9d\u8d56\uff0c\u6253\u5305\u5728\u4e00\u4e2a\u6587\u4ef6\u91cc\u9762\u3002\u8fd0\u884c\u8fd9\u4e2a\u6587\u4ef6\uff0c\u5c31\u4f1a\u751f\u6210\u4e00\u4e2a\u865a\u62df\u5bb9\u5668\u3002\u7a0b\u5e8f\u5728\u8fd9\u4e2a\u865a\u62df\u5bb9\u5668\u91cc\u8fd0\u884c\uff0c\u5c31\u597d\u50cf\u5728\u771f\u5b9e\u7684\u7269\u7406\u673a\u4e0a\u8fd0\u884c\u4e00\u6837\u3002\u6709\u4e86 Docker\uff0c\u5c31\u4e0d\u7528\u62c5\u5fc3\u73af\u5883\u95ee\u9898\u3002 \u60a8\u53ef\u4ee5\u5728\u9879\u76ee\u7684 ./enviroment/docker \u76ee\u5f55\u4e0b\u627e\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u4e00\u4e9b\u793a\u4f8b\u548c\u6559\u7a0b\u3002","title":"Docker"},{"location":"zh/tutorial/run_in_container/#jupyterlab","text":"JupyterLab\u662fJupyter\u4e3b\u6253\u7684\u6700\u65b0\u6570\u636e\u79d1\u5b66\u751f\u4ea7\u5de5\u5177\uff0c\u67d0\u79cd\u610f\u4e49\u4e0a\uff0c\u5b83\u7684\u51fa\u73b0\u662f\u4e3a\u4e86\u53d6\u4ee3Jupyter Notebook\u3002\u4e0d\u8fc7\u4e0d\u7528\u62c5\u5fc3Jupyter Notebook\u4f1a\u6d88\u5931\uff0cJupyterLab\u5305\u542b\u4e86Jupyter Notebook\u6240\u6709\u529f\u80fd\u3002 JupyterLab\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8eweb\u7684\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u5b83\u7f16\u5199notebook\u3001\u64cd\u4f5c\u7ec8\u7aef\u3001\u7f16\u8f91markdown\u6587\u672c\u3001\u6253\u5f00\u4ea4\u4e92\u6a21\u5f0f\u3001\u67e5\u770bcsv\u6587\u4ef6\u53ca\u56fe\u7247\u7b49\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u5728\u9879\u76ee\u7684 ./enviroment/jupyterlab \u76ee\u5f55\u4e0b\u627e\u5230\u6211\u4eec\u63d0\u4f9b\u7684\u4e00\u4e9b\u793a\u4f8b\u548c\u6559\u7a0b\u3002","title":"Jupyterlab"},{"location":"zh/tutorial/strategy/fednova/","text":"FedNova \u00b6 \u6839\u636e\u8bba\u6587 Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization , \u6211\u4eec\u5b9e\u73b0\u4e86FedNova\u805a\u5408\u7b97\u6cd5 \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 FedNova \u3002 To start a FedNova server \u00b6 strategy = message_type . STRATEGY_FEDNOVA # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedNova To start a client \u00b6 See how to use FedNova\u6d89\u53ca\u5ba2\u6237\u7aef\u7684\u6837\u672c\u6570\uff08 sample_num \uff09\u548c\u6bcf\u8f6e\u8bad\u7ec3\u4f18\u5316\u7684\u6b21\u6570\uff08 batch_num \uff09\uff0c\u56e0\u6b64\u9700\u8981\u8986\u76d6 Trainer.config \u65b9\u6cd5\u4ee5\u8fd4\u56de\u8fd9\u4e24\u4e2a\u503c def config ( self ) -> dict (): return { \"batch_num\" : len ( self . _train_loader ), \"sample_num\" : len ( self . _train_loader ) * self . _train_loader . batch_size , } FedNova \u8fd8\u9700\u8981\u91cd\u5199 Trainer.get \u65b9\u6cd5\uff0c\u4ee5\u8fd4\u56de\u5ba2\u6237\u7aef\u5f53\u524d\u6a21\u578b\u548c\u4e0a\u4e00\u8f6e\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 def get ( self , param_type = \"\" ): parameters = dict () for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( \"module.\" , \"\" )] = ( p . cpu () . detach () . numpy () - self . _old_weights [ name ] . cpu () . detach () . numpy () ) return parameters","title":"FedNova Strategy"},{"location":"zh/tutorial/strategy/fednova/#fednova","text":"\u6839\u636e\u8bba\u6587 Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization , \u6211\u4eec\u5b9e\u73b0\u4e86FedNova\u805a\u5408\u7b97\u6cd5 \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 FedNova \u3002","title":"FedNova"},{"location":"zh/tutorial/strategy/fednova/#to-start-a-fednova-server","text":"strategy = message_type . STRATEGY_FEDNOVA # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () or python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedNova","title":"To start a FedNova server"},{"location":"zh/tutorial/strategy/fednova/#to-start-a-client","text":"See how to use FedNova\u6d89\u53ca\u5ba2\u6237\u7aef\u7684\u6837\u672c\u6570\uff08 sample_num \uff09\u548c\u6bcf\u8f6e\u8bad\u7ec3\u4f18\u5316\u7684\u6b21\u6570\uff08 batch_num \uff09\uff0c\u56e0\u6b64\u9700\u8981\u8986\u76d6 Trainer.config \u65b9\u6cd5\u4ee5\u8fd4\u56de\u8fd9\u4e24\u4e2a\u503c def config ( self ) -> dict (): return { \"batch_num\" : len ( self . _train_loader ), \"sample_num\" : len ( self . _train_loader ) * self . _train_loader . batch_size , } FedNova \u8fd8\u9700\u8981\u91cd\u5199 Trainer.get \u65b9\u6cd5\uff0c\u4ee5\u8fd4\u56de\u5ba2\u6237\u7aef\u5f53\u524d\u6a21\u578b\u548c\u4e0a\u4e00\u8f6e\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 def get ( self , param_type = \"\" ): parameters = dict () for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( \"module.\" , \"\" )] = ( p . cpu () . detach () . numpy () - self . _old_weights [ name ] . cpu () . detach () . numpy () ) return parameters","title":"To start a client"},{"location":"zh/tutorial/strategy/fedopt/","text":"FedOpt \u00b6 \u6839\u636e\u6587\u7ae0 Adaptive Federated Optimization \uff0c\u6211\u4eec\u5b9e\u73b0\u4e86fedopt\u805a\u5408\u7b97\u6cd5\uff0c\u5982fedadam\u3001fedyogi\u3001fedadagrad... \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 FedOpt \u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u7ee7\u627f FedOpt \u6765\u5b9e\u73b0\u81ea\u5df1\u7684 fedopt \u805a\u5408\u7b97\u6cd5 class FedOpt : \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _params = params self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' pass \u5982\u679c\u8981\u4f7f\u7528 Pytorch \u7248\u672c\u7684\u4f18\u5316\u5668\uff0c\u4e0b\u9762\u662f\u4e00\u4e2a\u4f8b\u5b50\u3002 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4efb\u4f55\u65b9\u6cd5\u6765\u4f18\u5316\u6a21\u578b\u53c2\u6570\uff08\u786e\u4fdd step() \u8fd4\u56de\u5c55\u5e73\u53c2\u6570\uff09 class PytorchFedAdam ( FedOpt ): \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , model : nn . Module params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( params , learning_rate , betas , t ) self . _model = model self . _opt = torch . optim . Adam ( self . _model . parameters (), lr , betas = betas ) def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' for name , p in self . _model . named_parameters (): p . grad = torch . from_numpy ( np . array ( pseudo_gradient [ name ] )) . reshape ( p . shape ) . type ( p . grad . dtype ) . to ( p . device ) self . _opt . step () params = dict () for name , param in model . named_parameters (): if param . requires_grad : params [ name ] = param . cpu () . detach () . numpy () . reshape (( - 1 )) return params To start a fedopt server \u00b6 strategy = message_type . STRATEGY_FEDOPT # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () \u6216\u8005 python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedOpt --strategy_params { \"learning_rate\" :1, \"betas\" : [ 0 .9,0.99 ] , \"t\" :0.1, \"opt\" : \"FedAdam\" } To start a client \u00b6 \u8bf7\u53c2\u9605 \u5982\u4f55\u4f7f\u7528","title":"Fedopt Strategy"},{"location":"zh/tutorial/strategy/fedopt/#fedopt","text":"\u6839\u636e\u6587\u7ae0 Adaptive Federated Optimization \uff0c\u6211\u4eec\u5b9e\u73b0\u4e86fedopt\u805a\u5408\u7b97\u6cd5\uff0c\u5982fedadam\u3001fedyogi\u3001fedadagrad... \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801 \u53c2\u8003 FedOpt \u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u7ee7\u627f FedOpt \u6765\u5b9e\u73b0\u81ea\u5df1\u7684 fedopt \u805a\u5408\u7b97\u6cd5 class FedOpt : \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : self . _params = params self . _lr = learning_rate self . _beta1 = betas [ 0 ] self . _beta2 = betas [ 1 ] self . _adaptivity = t def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' pass \u5982\u679c\u8981\u4f7f\u7528 Pytorch \u7248\u672c\u7684\u4f18\u5316\u5668\uff0c\u4e0b\u9762\u662f\u4e00\u4e2a\u4f8b\u5b50\u3002 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4efb\u4f55\u65b9\u6cd5\u6765\u4f18\u5316\u6a21\u578b\u53c2\u6570\uff08\u786e\u4fdd step() \u8fd4\u56de\u5c55\u5e73\u53c2\u6570\uff09 class PytorchFedAdam ( FedOpt ): \"\"\"Implementation based on https://arxiv.org/abs/2003.00295.\"\"\" def __init__ ( self , model : nn . Module params : Dict [ str , npt . NDArray [ np . float32 ]], learning_rate : float = 0.1 , betas : Tuple [ float , float ] = ( 0.9 , 0.999 ), t : float = 0.001 , ) -> None : super () . __init__ ( params , learning_rate , betas , t ) self . _model = model self . _opt = torch . optim . Adam ( self . _model . parameters (), lr , betas = betas ) def step ( self , pseudo_gradient : Dict [ str , npt . NDArray [ np . float32 ]] ) -> Dict [ str , npt . NDArray [ np . float32 ]]: '''Update parameters with optimization algorithm according to pseudo gradient''' for name , p in self . _model . named_parameters (): p . grad = torch . from_numpy ( np . array ( pseudo_gradient [ name ] )) . reshape ( p . shape ) . type ( p . grad . dtype ) . to ( p . device ) self . _opt . step () params = dict () for name , param in model . named_parameters (): if param . requires_grad : params [ name ] = param . cpu () . detach () . numpy () . reshape (( - 1 )) return params","title":"FedOpt"},{"location":"zh/tutorial/strategy/fedopt/#to-start-a-fedopt-server","text":"strategy = message_type . STRATEGY_FEDOPT # define server type server = AggregateServer ( args . addr , strategy , args . num ) server . run () \u6216\u8005 python iflearner/business/homo/aggregate_server.py -n 2 --strategy FedOpt --strategy_params { \"learning_rate\" :1, \"betas\" : [ 0 .9,0.99 ] , \"t\" :0.1, \"opt\" : \"FedAdam\" }","title":"To start a fedopt server"},{"location":"zh/tutorial/strategy/fedopt/#to-start-a-client","text":"\u8bf7\u53c2\u9605 \u5982\u4f55\u4f7f\u7528","title":"To start a client"},{"location":"zh/tutorial/strategy/qfedavg/","text":"qFedAvg \u00b6 \u6839\u636e\u6587\u7ae0 FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING \uff0c\u6211\u4eec\u5b9e\u73b0\u4e86qfedavg\u805a\u5408\u7b97\u6cd5\u3002 \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 qFedAvg \u3002 To start a qfedav server \u00b6 strategy = message_type . STRATEGY_qFEDAVG # define server type server = AggregateServer ( args . addr , strategy , args . num , q = .2 , learning_rate = 1 ) server . run () \u6216\u8005 python iflearner/business/homo/aggregate_server.py -n 2 --strategy qFedAvg --strategy_params { \"q\" :.2, \"learning_rate\" :1 } To start a client \u00b6 \u8bf7\u53c2\u9605 \u5982\u4f55\u4f7f\u7528 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5ba2\u6237\u7aef\u5f00\u59cb\u62df\u5408\u4e4b\u524d\uff0c\u9700\u8981\u83b7\u53d6\u5f53\u524d\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u7684\u635f\u5931\u503c loss \u3002 \u7136\u540e\uff0c\u60a8\u5e94\u8be5\u5728\u5ba2\u6237\u7aef\u91cd\u5199 Trainer.get \u65b9\u6cd5\uff0c\u5e76\u5c06 loss \u5173\u952e\u5b57\u6dfb\u52a0\u5230\u4e0a\u4f20\u7684\u53c2\u6570\u4e2d\u3002 def evaluate_traindata ( self ): batch_time = AverageMeter ( \"Time\" , \":6.3f\" , Summary . AVERAGE ) losses = AverageMeter ( \"Loss\" , \":.4e\" , Summary . AVERAGE ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" , Summary . AVERAGE ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" , Summary . AVERAGE ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , losses , top1 , top5 ], prefix = \"Test on training data: \" ) with torch . no_grad (): end = time . time () for i , ( images , target ) in enumerate ( self . _train_loader ): if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) progress . display_summary () self . _fs = losses . avg def get ( self , param_type = '' ): parameters = dict () parameters [ 'loss' ] = np . array ([ self . _fs ]) for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( 'module.' , '' ) ] = p . cpu () . detach () . numpy () return parameters","title":"Qfedavg Strategy"},{"location":"zh/tutorial/strategy/qfedavg/#qfedavg","text":"\u6839\u636e\u6587\u7ae0 FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING \uff0c\u6211\u4eec\u5b9e\u73b0\u4e86qfedavg\u805a\u5408\u7b97\u6cd5\u3002 \u5b8c\u6574\u7684\u8be5\u793a\u4f8b\u7684\u6e90\u4ee3\u7801\u53c2\u8003 qFedAvg \u3002","title":"qFedAvg"},{"location":"zh/tutorial/strategy/qfedavg/#to-start-a-qfedav-server","text":"strategy = message_type . STRATEGY_qFEDAVG # define server type server = AggregateServer ( args . addr , strategy , args . num , q = .2 , learning_rate = 1 ) server . run () \u6216\u8005 python iflearner/business/homo/aggregate_server.py -n 2 --strategy qFedAvg --strategy_params { \"q\" :.2, \"learning_rate\" :1 }","title":"To start a qfedav server"},{"location":"zh/tutorial/strategy/qfedavg/#to-start-a-client","text":"\u8bf7\u53c2\u9605 \u5982\u4f55\u4f7f\u7528 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5ba2\u6237\u7aef\u5f00\u59cb\u62df\u5408\u4e4b\u524d\uff0c\u9700\u8981\u83b7\u53d6\u5f53\u524d\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u7684\u635f\u5931\u503c loss \u3002 \u7136\u540e\uff0c\u60a8\u5e94\u8be5\u5728\u5ba2\u6237\u7aef\u91cd\u5199 Trainer.get \u65b9\u6cd5\uff0c\u5e76\u5c06 loss \u5173\u952e\u5b57\u6dfb\u52a0\u5230\u4e0a\u4f20\u7684\u53c2\u6570\u4e2d\u3002 def evaluate_traindata ( self ): batch_time = AverageMeter ( \"Time\" , \":6.3f\" , Summary . AVERAGE ) losses = AverageMeter ( \"Loss\" , \":.4e\" , Summary . AVERAGE ) top1 = AverageMeter ( \"Acc@1\" , \":6.2f\" , Summary . AVERAGE ) top5 = AverageMeter ( \"Acc@5\" , \":6.2f\" , Summary . AVERAGE ) progress = ProgressMeter ( len ( self . _train_loader ), [ batch_time , losses , top1 , top5 ], prefix = \"Test on training data: \" ) with torch . no_grad (): end = time . time () for i , ( images , target ) in enumerate ( self . _train_loader ): if self . _args . gpu is not None : images = images . cuda ( self . _args . gpu , non_blocking = True ) if torch . cuda . is_available (): target = target . cuda ( self . _args . gpu , non_blocking = True ) # compute output output = self . _model ( images ) loss = self . _criterion ( output , target ) # measure accuracy and record loss acc1 , acc5 = accuracy ( output , target , topk = ( 1 , 5 )) losses . update ( loss . item (), images . size ( 0 )) top1 . update ( acc1 [ 0 ], images . size ( 0 )) top5 . update ( acc5 [ 0 ], images . size ( 0 )) # measure elapsed time batch_time . update ( time . time () - end ) end = time . time () if i % self . _args . print_freq == 0 : progress . display ( i ) progress . display_summary () self . _fs = losses . avg def get ( self , param_type = '' ): parameters = dict () parameters [ 'loss' ] = np . array ([ self . _fs ]) for name , p in self . _model . named_parameters (): if p . requires_grad : parameters [ name . replace ( 'module.' , '' ) ] = p . cpu () . detach () . numpy () return parameters","title":"To start a client"}]}